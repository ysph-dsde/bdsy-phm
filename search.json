[
  {
    "objectID": "Pages/Schedule.html",
    "href": "Pages/Schedule.html",
    "title": "Public Health Modeling Group",
    "section": "",
    "text": "At the end of the program, you will present your work at a symposium, which will include both a presentation with slides and a poster session. Each group will have 25 minutes to present their slide deck, followed by 5 minutes for questions. The poster session will last 45 minutes.\n\n\n\n\n\n\nImportant\n\n\n\nDuring the poster session, at least one member of your group should be stationed at the poster at all times, while the other two can explore the room. We expect that you will switch every 15 minutes so that everyone spends at least 15 minutes at the poster and the remaining 30 minutes exploring other projects.\n\n\nTo help you stay on track with compiling your materials, we have prepared the following scheduled assignments. Over the coming weeks, we will guide you through preparing the materials for this symposium and aim to expose you to the process of developing these materials in a professional environment.\nWhen working on a research team, it is expected that you can clearly articulate your research to people outside your specific area of expertise and provide meaningful, constructive feedback to help teammates improve their research. At some companies, to be considered for a promotion, you will be required to present an annual summary of our research and a literature review to the group/department, as well as regularly provide insightful comments and constructive criticisms to colleagues.\nThis is an ambitious schedule, and we acknowledge the extensive workload you each have during the program. It is equally important for you to experience the research project process as it is to execute the techniques we will teach you to support your project hypothesis/research question. You will not be expected to conduct additional analyses beyond the content we provide and assign.\nIf you encounter insurmountable difficulties implementing any part of the study, please prioritize completing the overall study over finishing any single part that proves too challenging. We are here to answer any questions you might have and help troubleshoot any implementation problems."
  },
  {
    "objectID": "Pages/Schedule.html#expectations-overview",
    "href": "Pages/Schedule.html#expectations-overview",
    "title": "Public Health Modeling Group",
    "section": "",
    "text": "At the end of the program, you will present your work at a symposium, which will include both a presentation with slides and a poster session. Each group will have 25 minutes to present their slide deck, followed by 5 minutes for questions. The poster session will last 45 minutes.\n\n\n\n\n\n\nImportant\n\n\n\nDuring the poster session, at least one member of your group should be stationed at the poster at all times, while the other two can explore the room. We expect that you will switch every 15 minutes so that everyone spends at least 15 minutes at the poster and the remaining 30 minutes exploring other projects.\n\n\nTo help you stay on track with compiling your materials, we have prepared the following scheduled assignments. Over the coming weeks, we will guide you through preparing the materials for this symposium and aim to expose you to the process of developing these materials in a professional environment.\nWhen working on a research team, it is expected that you can clearly articulate your research to people outside your specific area of expertise and provide meaningful, constructive feedback to help teammates improve their research. At some companies, to be considered for a promotion, you will be required to present an annual summary of our research and a literature review to the group/department, as well as regularly provide insightful comments and constructive criticisms to colleagues.\nThis is an ambitious schedule, and we acknowledge the extensive workload you each have during the program. It is equally important for you to experience the research project process as it is to execute the techniques we will teach you to support your project hypothesis/research question. You will not be expected to conduct additional analyses beyond the content we provide and assign.\nIf you encounter insurmountable difficulties implementing any part of the study, please prioritize completing the overall study over finishing any single part that proves too challenging. We are here to answer any questions you might have and help troubleshoot any implementation problems."
  },
  {
    "objectID": "Pages/Schedule.html#format",
    "href": "Pages/Schedule.html#format",
    "title": "Public Health Modeling Group",
    "section": "Format",
    "text": "Format\nEach Thursday, you will be expected to have completed specific aspects of your analysis to allow time to process the upcoming material. Additionally, each group will present summaries of their work. The specifics of these expectations are outlined below for each week.\n\n\n\n\n\n\nImportant\n\n\n\nTime around 1 minute of discussion for each slide. If you need pointers on how to effectively present your work, please check out our tips provided in the Project Materials page.\n\n\nWe expect the audience to provide insightful, constructive feedback to help improve and compliment the presenting groups on their work. Additionally, we expect you to foster a positive, collaborative environment within your group, allowing everyone the opportunity to participate meaningfully.\nIt is critical that each member of your group equally shares the responsibilities of conducting the research and presenting the weekly updates. Complimenting colleagues’ work demonstrates good professionalism and helps the team identify and maintain effective practices."
  },
  {
    "objectID": "Pages/Schedule.html#thursday-july-3rd",
    "href": "Pages/Schedule.html#thursday-july-3rd",
    "title": "Public Health Modeling Group",
    "section": "Thursday July 3rd",
    "text": "Thursday July 3rd\nHave Completed:\n\nConducted a literature search for peer-reviewed references (at least 5 relevant papers), focusing on publications from the last 5-10 years.\nAny exploratory data analysis (EDA) of your dataset, with simple visualization of the variables (i.e. histograms of variables, etc.).\nPrepared a background outline.\nDeveloped a hypothesis/research question.\n\nPresent (12-15 slides):\n\n1 slide describing your hypothesis/research question. Try to explain how you hope the techniques learned thus far will help you justify any conclusion about this hypothesis/research question.\n“Journal Club” like presentation where you will include:\n\n5-6 slides, with each slide providing a high-level overview of one peer-reviewed reference. Cover 5-6 of your references. For each paper, briefly explain what the paper is about, methods, if it applies any of the techniques we will be learning, and how it will help you with your study.\n3 slides giving a deeper dive into one paper.\n\nIn less than 3-5 slides, summarize your EDA findings, like:\n\nTime-span it covers, categories (i.e. age groups or regions), number of unique data points, etc.\nBriefly discuss the variables, especially if the variables imply features that are not readily obvious.\nDiscuss any potential gaps or pitfalls found with the data, such as data collection limitations, missing data, or combined outcomes that could confound the observations."
  },
  {
    "objectID": "Pages/Schedule.html#thursday-july-10th",
    "href": "Pages/Schedule.html#thursday-july-10th",
    "title": "Public Health Modeling Group",
    "section": "Thursday, July 10th",
    "text": "Thursday, July 10th\nHave Completed:\n\nImplemented Time Series and Hierarchical Modeling, focusing on how they support your hypothesis/research questions.\nStarted on methods-specific visualizations and drafting the methods report.\n\nPresent (7-9 slides):\n\n3-5 slides summarizing current results and any conclusions you might have with visualizations and any interesting/unexpected findings.\n3 slides explaining what Time Series and Hierarchical Modeling are addressing in layman’s terms and how they support your hypothesis/study question.\n1 slide to discuss whether working with the data and seeing the results has shaped the research question/hypothesis."
  },
  {
    "objectID": "Pages/Schedule.html#thursday-july-17th",
    "href": "Pages/Schedule.html#thursday-july-17th",
    "title": "Public Health Modeling Group",
    "section": "Thursday, July 17th",
    "text": "Thursday, July 17th\nHave Completed:\n\nImplement Market Basket analysis and visualize the results from the GPS pipeline, focusing on how they support your hypothesis/research questions. The subset of your data that is relevant to your research question will be prerun for you using the GPS analysis pipeline, leaving only the visualization tasks for you to complete.\nFinished the methods section with GPS and Market Basket analysis and relevant visualizations.\nSlide deck first draft for the 25 minute symposium presentation.\nPoster first draft.\n\nPresent (4 slides):\n\n1 slide summarizing current results and any conclusions you might have with visualizations and any interesting/unexpected findings.\n2 slides explaining what GPS and Market Basket are addressing in layman’s terms and how they support your hypothesis/study question.\n1 slide to discuss whether working with the data and seeing the results has shaped the research question/hypothesis.\n\nMock Symposium:\nAfter a brief catch-up on the group’s progress, we aim to spend most of the day practicing the presentations and poster sessions. This time is intended to help you practice with peers who understand the type of study you conducted and give you an opportunity to implement any last-minute changes to improve your content.\nEach group will be given 25 minutes to present their slides followed by 5-10 minutes of peer-feedback. For the audience, provide feedback about:\n\nWhether the presentation was cohesive from start to finish and told a “story” about the study.\nHighlighted interesting findings unique to their data and hypothesis/research question.\nIf presenters clearly explained the takeaway from each slide and equally shared the responsibilities of presenting.\nWhether the group explained any shortcomings in their analysis, such as limitations imposed by the dataset’s original data collection methods.\nWhether the slides were light on text and effectively used visuals to communicate the core takeaways from each slide.\nWhether the presentation formatting was consistent enough to make the progression of information easy to follow. For example, ensuring serotypes are color-coded the same way in each figure.\n\nYou will then be mixed into groups of three people with one representative from each group. Each person will be given time to pull up their groups poster on their computer and give a 30 second to 1 minute elevator speech, or “hook”. Then explain some of the more detailed highlights you might provide someone who stops at your poster to hear more.\nFor poster visiters, practice asking questions from the perspective of someone from another project group. Provide feedback about:\n\nIf the primary 30-second to 1-minute “hook” is engaging and covers relevant points that encompass the study’s purpose and findings.\nHow easy it is to understand the project goals and conclusions if you have not conducted a similar study.\nThe effectiveness of the visualizations.\nWhether the poster is easy for casual browsers to understand, or if it requires a lot of focus to comprehend."
  },
  {
    "objectID": "Pages/Schedule.html#monday-july-21st",
    "href": "Pages/Schedule.html#monday-july-21st",
    "title": "Public Health Modeling Group",
    "section": "Monday July 21st",
    "text": "Monday July 21st\nUse this time to finalize any remaining details and practice the slide presentation or poster with your group.\nHave Completed:\n\nFinalized poster by end of day (EOD) for printing. The instructors will send these to Jackson/Aquielle for printing."
  },
  {
    "objectID": "Pages/Week4/Market Basket Analysis.html",
    "href": "Pages/Week4/Market Basket Analysis.html",
    "title": "Market Basket Analysis",
    "section": "",
    "text": "Market Basket Analysis, aka affinity analysis aka association rules mining is an unsupervised machine learning technique which applies an algorithm (apriori algorithm) to identify association rules in datasets.\nWe’ll be applying this algorithm to identify associations in a dataset containing information about cases of perforative acute otitis media in children.\nWe’ll start with loading in our packages\n# NOTE: you might need to specify the source for the arules package:\n# install.packages(\"arules\", repos='http://cran.rstudio.com/')\n\nsuppressPackageStartupMessages({\n  library(\"tidyverse\")     # Collection of R packages for data science\n  library(\"plyr\")          # For data manipulation\n  library(\"knitr\")         # For dynamic report generation\n  library(\"ggplot2\")       # For creating static visualizations\n  library(\"lubridate\")     # For date and time manipulation\n  library(\"arules\")        # For mining association rules and frequent itemsets\n  library(\"arulesViz\")     # For visualizing association rules\n  library(\"tcltk2\")        # Provides enhanced functionality for Tcl/Tk GUI\n  library(\"RColorBrewer\")  # For color palettes\n  library(\"plotly\")        # For creating interactive web-based graphs\n  library(\"httr\")          # For downloading files from URLs\n})\n\n# Function to select \"Not In\"\n'%!in%' &lt;- function(x,y)!('%in%'(x,y))\nAnd our dataset\n# Read in the cleaned data directly from the instructor's GitHub.\nurl &lt;- \"https://raw.githubusercontent.com/ysph-dsde/bdsy-phm/refs/heads/main/Data/pAOM.csv\"\npAOM &lt;- read_csv(url)\n\nRows: 2137 Columns: 10\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (10): PtID, SmokeNum, PtDaycare, SibNum, Pre_Post_PCV13, PCV13_Serotype_...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nWe have 2137 rows representing 2137 cases of pAOM. Each row contains information about PtID- study ID of patient SmokeNum- number of smokers in household PtDayCare- did patient attend daycare SibNum- number of siblings in household NumSibDaycare- number of siblings in daycare Pre_Post_PCV13- did the case occur before or after the child could have received PCV13? (PreVacc, PostVacc_yr1-5) PCV13_Serotype_AOM- of pneumococcal pAOM cases, were they from PCV13 serotypes? (VT,NVTs) OtoPathogen- Strep_pneumo,Strep_pyogenes,Haem_inf, Morax_cat, Staph_aur, OthBact, PresViral Carriage1- otopathogens (c_Strep_pneumo,c_Strep_pyogenes,c_Haem_inf, c_Morax_cat, c_Staph_aur, c_None)found in NP carriage Carriage2- additional otopathogens found in NP carriage Carriage3- additional otopathogens found in NP carriage\nBefore using our rule mining algorithm, we need to transform data from the data frame format into transactions\n# Create a temporary file\ntemp_file &lt;- tempfile()\n\n# Download the data from GitHub and save it to the temporary file\nGET(url, write_disk(temp_file, overwrite = TRUE))\n\nResponse [https://raw.githubusercontent.com/ysph-dsde/bdsy-phm/refs/heads/main/Data/pAOM.csv]\n  Date: 2025-07-09 03:56\n  Status: 200\n  Content-Type: text/plain; charset=utf-8\n  Size: 144 kB\n&lt;ON DISK&gt;  /var/folders/9f/rwy2b8vj3m90x_s1fvx553cn5v3lr9/T//RtmpOTFFbe/file139af7565765\n\n# Read the transactions from the downloaded file\ntr &lt;- read.transactions(temp_file, format = 'basket', sep = ',')\nprint('Description of the transactions')\n\n[1] \"Description of the transactions\"\n\nsummary(tr)\n\ntransactions as itemMatrix in sparse format with\n 2138 rows (elements/itemsets/transactions) and\n 2178 columns (items) and a density of 0.003250036 \n\nmost frequent items:\n    c_None No Smokers    Daycare    OthBact  1 Sibling    (Other) \n      2008       1732       1265       1082        997       8050 \n\nelement (itemset/transaction) length distribution:\nsizes\n   5    6    7    8    9   10 \n   3   11 1947  170    6    1 \n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  5.000   7.000   7.000   7.079   7.000  10.000 \n\nincludes extended item information - examples:\n  labels\n1   0101\n2   0102\n3   0103\nLet’s see what items occur most frequently:\nitemFrequencyPlot(tr,topN=25,type=\"absolute\",col=brewer.pal(8,'Pastel2'), main=\"pAOM rules\")\na relative frequency plot\nitemFrequencyPlot(tr,topN=20,type=\"relative\",col=brewer.pal(8,'Pastel2'),main=\"Relative frequency, pAOM\")"
  },
  {
    "objectID": "Pages/Week4/Market Basket Analysis.html#create-some-rules",
    "href": "Pages/Week4/Market Basket Analysis.html#create-some-rules",
    "title": "Market Basket Analysis",
    "section": "Create some rules",
    "text": "Create some rules\nWe use the Apriori algorithm from the arules package to look for itemsets and find support for rules\nWe pass supp=0.0001 and conf=0.8 to return all the rules have a support of at least 0.1% and confidence of at least 80%.\nWe sort the rules by decreasing confidence.\nHere are the rules matching these criteria:\n\nrules &lt;- apriori(tr, parameter = list(supp=0.001, conf=0.8))\n\nApriori\n\nParameter specification:\n confidence minval smax arem  aval originalSupport maxtime support minlen\n        0.8    0.1    1 none FALSE            TRUE       5   0.001      1\n maxlen target  ext\n     10  rules TRUE\n\nAlgorithmic control:\n filter tree heap memopt load sort verbose\n    0.1 TRUE TRUE  FALSE TRUE    2    TRUE\n\nAbsolute minimum support count: 2 \n\nset item appearances ...[0 item(s)] done [0.00s].\nset transactions ...[2178 item(s), 2138 transaction(s)] done [0.00s].\nsorting and recoding items ... [31 item(s)] done [0.00s].\ncreating transaction tree ... done [0.00s].\nchecking subsets of size 1 2 3 4 5 6 7 done [0.00s].\nwriting ... [4093 rule(s)] done [0.03s].\ncreating S4 object  ... done [0.00s].\n\nrules &lt;- sort(rules, by='confidence', decreasing = TRUE)\nsummary(rules)\n\nset of 4093 rules\n\nrule length distribution (lhs + rhs):sizes\n   1    2    3    4    5    6    7 \n   2   50  433 1366 1576  621   45 \n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   1.00    4.00    5.00    4.59    5.00    7.00 \n\nsummary of quality measures:\n    support           confidence        coverage             lift         \n Min.   :0.001403   Min.   :0.8000   Min.   :0.001403   Min.   :  0.8518  \n 1st Qu.:0.001871   1st Qu.:0.8913   1st Qu.:0.001871   1st Qu.:  1.0287  \n Median :0.003274   Median :1.0000   Median :0.003274   Median :  1.0647  \n Mean   :0.013357   Mean   :0.9468   Mean   :0.014793   Mean   :  3.3443  \n 3rd Qu.:0.008887   3rd Qu.:1.0000   3rd Qu.:0.009354   3rd Qu.:  1.4648  \n Max.   :0.939195   Max.   :1.0000   Max.   :1.000000   Max.   :106.9000  \n     count        \n Min.   :   3.00  \n 1st Qu.:   4.00  \n Median :   7.00  \n Mean   :  28.56  \n 3rd Qu.:  19.00  \n Max.   :2008.00  \n\nmining info:\n data ntransactions support confidence\n   tr          2138   0.001        0.8\n                                                           call\n apriori(data = tr, parameter = list(supp = 0.001, conf = 0.8))\n\n\nWe have 4093 rules, most are 4 or 5 items long. Let’s inspect the top 10 rules according to these parameters (supp 0.001, conf =0.8).\n\ninspect(rules[1:10])\n\n     lhs                        rhs            support     confidence\n[1]  {3 Smoker}              =&gt; {c_None}       0.001870907 1         \n[2]  {c_Haem_inf}            =&gt; {PreVacc}      0.007015903 1         \n[3]  {c_Strep_pneumo}        =&gt; {PreVacc}      0.009354537 1         \n[4]  {c_Morax_cat}           =&gt; {PreVacc}      0.022450889 1         \n[5]  {NVT}                   =&gt; {Strep_pneumo} 0.027595884 1         \n[6]  {VT}                    =&gt; {Strep_pneumo} 0.047708138 1         \n[7]  {PostVacc_yr1}          =&gt; {c_None}       0.062675398 1         \n[8]  {3 Smoker, 3+ Sibling}  =&gt; {c_None}       0.001403181 1         \n[9]  {Morax_cat, No Sibings} =&gt; {No Smokers}   0.001403181 1         \n[10] {Morax_cat, No Sibings} =&gt; {c_None}       0.001403181 1         \n     coverage    lift      count\n[1]  0.001870907  1.064741   4  \n[2]  0.007015903  2.679198  15  \n[3]  0.009354537  2.679198  20  \n[4]  0.022450889  2.679198  48  \n[5]  0.027595884 13.279503  59  \n[6]  0.047708138 13.279503 102  \n[7]  0.062675398  1.064741 134  \n[8]  0.001403181  1.064741   3  \n[9]  0.001403181  1.234411   3  \n[10] 0.001403181  1.064741   3  \n\n\nAnd plot these top 10 rules, or 20, or 50.\n\ntopRules &lt;- rules[1:10]\nplot(rules)\n\nTo reduce overplotting, jitter is added! Use jitter = 0 to prevent jitter.\n\n\n\n\n\n\n\n\n\nnow with more colors\n\nplot(rules, method = \"two-key plot\")\n\nTo reduce overplotting, jitter is added! Use jitter = 0 to prevent jitter.\n\n\n\n\n\n\n\n\n\nnow how about a network graph?\n\nplot(topRules, method=\"graph\")\n\n\n\n\n\n\n\n\nNow let’s see an interactive map:\n\nplot(topRules, method=\"graph\", engine = 'interactive')\n\n\nplot(topRules, method = \"grouped\")\n\n\n\n\n\n\n\n\n\nplot(topRules, method = \"graph\",  engine = \"htmlwidget\")\n\n\n\n\n\nnow a matrix plot\n\nplot(topRules, method = \"matrix\", engine = \"3d\", measure = \"lift\")\n\nItemsets in Antecedent (LHS)\n[1] \"{NVT}\"                  \"{VT}\"                   \"{c_Haem_inf}\"          \n[4] \"{c_Strep_pneumo}\"       \"{c_Morax_cat}\"          \"{Morax_cat,No Sibings}\"\n[7] \"{3 Smoker}\"             \"{PostVacc_yr1}\"         \"{3 Smoker,3+ Sibling}\" \nItemsets in Consequent (RHS)\n[1] \"{c_None}\"       \"{No Smokers}\"   \"{PreVacc}\"      \"{Strep_pneumo}\"\n\n\n\n\n\n\n\n\n\nmoving back a bit, let’s check a different set of rules:\n\nrules_b &lt;- apriori(tr, parameter = list(supp=0.01, conf=1.0))\n\nApriori\n\nParameter specification:\n confidence minval smax arem  aval originalSupport maxtime support minlen\n          1    0.1    1 none FALSE            TRUE       5    0.01      1\n maxlen target  ext\n     10  rules TRUE\n\nAlgorithmic control:\n filter tree heap memopt load sort verbose\n    0.1 TRUE TRUE  FALSE TRUE    2    TRUE\n\nAbsolute minimum support count: 21 \n\nset item appearances ...[0 item(s)] done [0.00s].\nset transactions ...[2178 item(s), 2138 transaction(s)] done [0.00s].\nsorting and recoding items ... [26 item(s)] done [0.00s].\ncreating transaction tree ... done [0.00s].\nchecking subsets of size 1 2 3 4 5 6 done [0.00s].\nwriting ... [115 rule(s)] done [0.00s].\ncreating S4 object  ... done [0.00s].\n\nrules_b &lt;- sort(rules_b, by='confidence', decreasing = TRUE)\nsummary(rules_b)\n\nset of 115 rules\n\nrule length distribution (lhs + rhs):sizes\n 2  3  4  5  6 \n 4 30 51 26  4 \n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  2.000   3.000   4.000   3.965   5.000   6.000 \n\nsummary of quality measures:\n    support          confidence    coverage            lift       \n Min.   :0.01029   Min.   :1    Min.   :0.01029   Min.   : 1.065  \n 1st Qu.:0.01169   1st Qu.:1    1st Qu.:0.01169   1st Qu.: 1.065  \n Median :0.01403   Median :1    Median :0.01403   Median : 1.065  \n Mean   :0.01759   Mean   :1    Mean   :0.01759   Mean   : 6.355  \n 3rd Qu.:0.02035   3rd Qu.:1    3rd Qu.:0.02035   3rd Qu.:13.280  \n Max.   :0.06268   Max.   :1    Max.   :0.06268   Max.   :13.280  \n     count       \n Min.   : 22.00  \n 1st Qu.: 25.00  \n Median : 30.00  \n Mean   : 37.61  \n 3rd Qu.: 43.50  \n Max.   :134.00  \n\nmining info:\n data ntransactions support confidence\n   tr          2138    0.01          1\n                                                        call\n apriori(data = tr, parameter = list(supp = 0.01, conf = 1))\n\n\nthese more stringent criteria mean that we’re down to 115 rules to sort through.\n\ninspect(rules_b[1:10])\n\n     lhs                          rhs            support    confidence\n[1]  {c_Morax_cat}             =&gt; {PreVacc}      0.02245089 1         \n[2]  {NVT}                     =&gt; {Strep_pneumo} 0.02759588 1         \n[3]  {VT}                      =&gt; {Strep_pneumo} 0.04770814 1         \n[4]  {PostVacc_yr1}            =&gt; {c_None}       0.06267540 1         \n[5]  {c_Morax_cat, No Daycare} =&gt; {PreVacc}      0.01169317 1         \n[6]  {c_Morax_cat, OthBact}    =&gt; {PreVacc}      0.01216090 1         \n[7]  {c_Morax_cat, Daycare}    =&gt; {PreVacc}      0.01028999 1         \n[8]  {c_Morax_cat, No Smokers} =&gt; {PreVacc}      0.01917680 1         \n[9]  {NVT, PreVacc}            =&gt; {Strep_pneumo} 0.01075772 1         \n[10] {No Daycare, NVT}         =&gt; {Strep_pneumo} 0.01169317 1         \n     coverage   lift      count\n[1]  0.02245089  2.679198  48  \n[2]  0.02759588 13.279503  59  \n[3]  0.04770814 13.279503 102  \n[4]  0.06267540  1.064741 134  \n[5]  0.01169317  2.679198  25  \n[6]  0.01216090  2.679198  26  \n[7]  0.01028999  2.679198  22  \n[8]  0.01917680  2.679198  41  \n[9]  0.01075772 13.279503  23  \n[10] 0.01169317 13.279503  25  \n\n\ntry to look for only for rules associated with Strep_pneumo\n\npneumo.rules&lt;-sort(subset(rules_b, subset = rhs %in% \"Strep_pneumo\"))\n\ninspect(pneumo.rules[1:10])\n\n     lhs                          rhs            support    confidence\n[1]  {VT}                      =&gt; {Strep_pneumo} 0.04770814 1         \n[2]  {c_None, VT}              =&gt; {Strep_pneumo} 0.04443405 1         \n[3]  {No Smokers, VT}          =&gt; {Strep_pneumo} 0.03648269 1         \n[4]  {c_None, No Smokers, VT}  =&gt; {Strep_pneumo} 0.03367633 1         \n[5]  {Daycare, VT}             =&gt; {Strep_pneumo} 0.02946679 1         \n[6]  {NVT}                     =&gt; {Strep_pneumo} 0.02759588 1         \n[7]  {c_None, NVT}             =&gt; {Strep_pneumo} 0.02666043 1         \n[8]  {c_None, Daycare, VT}     =&gt; {Strep_pneumo} 0.02666043 1         \n[9]  {PreVacc, VT}             =&gt; {Strep_pneumo} 0.02385407 1         \n[10] {Daycare, No Smokers, VT} =&gt; {Strep_pneumo} 0.02338634 1         \n     coverage   lift    count\n[1]  0.04770814 13.2795 102  \n[2]  0.04443405 13.2795  95  \n[3]  0.03648269 13.2795  78  \n[4]  0.03367633 13.2795  72  \n[5]  0.02946679 13.2795  63  \n[6]  0.02759588 13.2795  59  \n[7]  0.02666043 13.2795  57  \n[8]  0.02666043 13.2795  57  \n[9]  0.02385407 13.2795  51  \n[10] 0.02338634 13.2795  50  \n\n\nnow let’s plot the pneumo rules:\n\nplot(pneumo.rules, measure = c(\"support\", \"confidence\"), shading = \"lift\")\n\nTo reduce overplotting, jitter is added! Use jitter = 0 to prevent jitter.\n\n\n\n\n\n\n\n\n\n\nplot(pneumo.rules[1:10], method=\"graph\", engine = 'interactive')\n\nplot(pneumo.rules[1:10], method= \"paracoord\", control=list(reorder=TRUE))\n\n\n\n\n\n\n\n\nreference: R and Data Mining there are other cool market basket analysis visualizations here:"
  },
  {
    "objectID": "Pages/Materials/Materials.html",
    "href": "Pages/Materials/Materials.html",
    "title": "Public Health Modeling Group",
    "section": "",
    "text": "Important\n\n\n\nPlease note that this page contains numerous links and download buttons, which may take a few minutes to load completely."
  },
  {
    "objectID": "Pages/Materials/Materials.html#course-papers",
    "href": "Pages/Materials/Materials.html#course-papers",
    "title": "Public Health Modeling Group",
    "section": "Course Papers",
    "text": "Course Papers\n\nHierarchical Models Weinberger and Warren et al. 2020 1 (view paper)\nSerotype Replacement Weinberger and Warren et al. 2018 2 (view paper)\nCorrelates of Nonrandom Patterns of Serotype Switching in Pneumococcus Joshi and Al-Mamun et al. 2020 3 (view paper)\n2022 Annual Epidemiological Report for Invasive Pneumococcal Disease (IPD) 4 (view paper)"
  },
  {
    "objectID": "Pages/Materials/Materials.html#lecture-slides",
    "href": "Pages/Materials/Materials.html#lecture-slides",
    "title": "Public Health Modeling Group",
    "section": "Lecture Slides",
    "text": "Lecture Slides\n\nWeek #1\n Day 1 Slide Deck   Day 2 Slide Deck \nDay 2 - Outbreak Investigation\n\nPart 1 (view handout)\nPart 2 (view handout)\nPart 3 (view handout)\n\n\n\nWeek #2\n Day 3 Slide Deck   Day 4 Slide Deck   Day 5 Slide Deck \n\n\nWeek #3\n Day 7 Slide Deck \n\n\nWeek #4\n Day 11 Slide Deck"
  },
  {
    "objectID": "Pages/Materials/Materials.html#codespaces",
    "href": "Pages/Materials/Materials.html#codespaces",
    "title": "Public Health Modeling Group",
    "section": "Codespaces",
    "text": "Codespaces\n\nWeekly Modules\nPlease download the codespaces provided for each weekly module release. Note that you may need to adjust the filepath for some references inside the code. It is recommended that you use the starter codespace package, which includes the following:\n\nAn Rproj file to set your root directory.\nAn renv lockfile.\nA suggested .gitignore.\nA README.md file with directions for using the codespace and guidelines for adding the code to your own GitHub remote repository.\n\nSubsequent weekly releases should be placed inside your root project directory. For instance, if you are using the directory “~/bdsy-phm-individual”, place the weekly releases within that directory.\n Starter for individual project with week 2 code \n Week 3 code \n Week 4 code \n\n\nProject Datasets and Materials\nEach group will be assigned data from one of three countries: Malawi, Nepal, or Peru. The datasets include the incidence of pneumococcal-caused respiratory disease, or in some cases, any causes of pneumonia, along with Global Pneumococcal Sequencing Project (GPS) metadata for genomic analysis using the GPS pipeline 5. Below, you will find these datasets, the source documentation or papers from which they were obtained, and any ancillary documentation, such as a Data Dictionary.\n\n\n\n\n\n\nNote\n\n\n\nNot all sources provided a Data Dictionary, so students will need to explain their variables using the context provided by the source documentation.\n\n\n\n\nComplete GPS Metadata Viewer and Download 5–7 (link to page)\n\n\n GPS Database Dictionary \n Malawi  Find the raw incidents data, GPS data, and literature papers 8.\n Nepal  Find the raw incidents data, GPS data, and literature papers 9,10.\n Peru  Find the raw incidents data, GPS data, and data dictionary 11.\n\n\nPeru Epidemiological surveillance of Acute Respiratory Infections (ARI) Data Page (link to page)"
  },
  {
    "objectID": "Pages/Materials/Materials.html#templates",
    "href": "Pages/Materials/Materials.html#templates",
    "title": "Public Health Modeling Group",
    "section": "Templates",
    "text": "Templates\n BDSY Slides Template 2025 \n Six Poster Template Options"
  },
  {
    "objectID": "Pages/Materials/Materials.html#tips-and-advice",
    "href": "Pages/Materials/Materials.html#tips-and-advice",
    "title": "Public Health Modeling Group",
    "section": "Tips and Advice",
    "text": "Tips and Advice\n\n\nTips for Effective Presentations (view slides)\n\n\n\n\n\n\n\n\nResearch Like a Pro\n\n\n\nIt is vital that you keep track of and disclose all the references you use to support your research. There are many tools available to help you with this, so discuss with your team and decide on a preferred method for reference tracking.\nIn my research, I use Mendeley, which comes with a Web Importer plug-in that makes it easy to compile different types of references. It conveniently formats these entries to various styles, including LaTeX and BibTeX."
  },
  {
    "objectID": "Pages/Materials/Student Codespaces/bdsy-phm-individual/Week2/Time Series Analysis_Hands-On Exercise.html",
    "href": "Pages/Materials/Student Codespaces/bdsy-phm-individual/Week2/Time Series Analysis_Hands-On Exercise.html",
    "title": "Time Series Analysis",
    "section": "",
    "text": "This workshop was developed by Dr. Dan Weinberger from the Yale School of Public Health (YSPH) for the 12^{th} International Symposium on Pneumococci and Pneumococcal Diseases (ISPPD-12) hosted in Toronto, Canada in June 2022. Here, we have adapted his workshop session #1, which compares different methods for fitting a time series dataset 1. Students are encouraged to explore the workshop content that Dr. Weinberger put together to learn more about the subject:\n\nGitHub: Find the code, original data, and slides 1.\nWorkshop Webpage: Find a longer description of the workshop in the format of a mini course with links to video presentations of select sections - Workshop on the evaluation of vaccine impact 2.\n\nThe data used here is a subset of the complete set used in Oliveira, L. H. D. et al. 2020 3. In their paper, national-level mortality data spanning from 2000 to 2016 for children under 5 years of age was compiled and standardized by national mortality registers in 10 Latin American and Caribbean countries. The primary cause of death was classified using the International Classification of Diseases, Tenth Revision (ICD-10) codes (Influenza and pneumonia J09-J18 or Oliveira, L. H. D. et al. 2020 Supplementary Table 2) 4,5. The goal of the paper was to evaluate the efficacy of pneumococcal conjugate vaccines (PCVs) in children who received any part of the standard-of-care vaccination series.\nToday, we are going to examine the efficacy of pneumococcal conjugate vaccines (PCVs) in children aged 2 to 59 months in Ecuador. The standard-of-care vaccination series involves a primary series of either two doses (administered at 2 and 4 months of age) or three doses (administered at 2, 4, and 6 months of age), with the option of an additional booster dose (administered at 12-18 months of age). This dataset does not differentiate between patients who received varying levels of inoculation, and instead reports the data if any PCV was delivered."
  },
  {
    "objectID": "Pages/Materials/Student Codespaces/bdsy-phm-individual/Week2/Time Series Analysis_Hands-On Exercise.html#introduction",
    "href": "Pages/Materials/Student Codespaces/bdsy-phm-individual/Week2/Time Series Analysis_Hands-On Exercise.html#introduction",
    "title": "Time Series Analysis",
    "section": "",
    "text": "This workshop was developed by Dr. Dan Weinberger from the Yale School of Public Health (YSPH) for the 12^{th} International Symposium on Pneumococci and Pneumococcal Diseases (ISPPD-12) hosted in Toronto, Canada in June 2022. Here, we have adapted his workshop session #1, which compares different methods for fitting a time series dataset 1. Students are encouraged to explore the workshop content that Dr. Weinberger put together to learn more about the subject:\n\nGitHub: Find the code, original data, and slides 1.\nWorkshop Webpage: Find a longer description of the workshop in the format of a mini course with links to video presentations of select sections - Workshop on the evaluation of vaccine impact 2.\n\nThe data used here is a subset of the complete set used in Oliveira, L. H. D. et al. 2020 3. In their paper, national-level mortality data spanning from 2000 to 2016 for children under 5 years of age was compiled and standardized by national mortality registers in 10 Latin American and Caribbean countries. The primary cause of death was classified using the International Classification of Diseases, Tenth Revision (ICD-10) codes (Influenza and pneumonia J09-J18 or Oliveira, L. H. D. et al. 2020 Supplementary Table 2) 4,5. The goal of the paper was to evaluate the efficacy of pneumococcal conjugate vaccines (PCVs) in children who received any part of the standard-of-care vaccination series.\nToday, we are going to examine the efficacy of pneumococcal conjugate vaccines (PCVs) in children aged 2 to 59 months in Ecuador. The standard-of-care vaccination series involves a primary series of either two doses (administered at 2 and 4 months of age) or three doses (administered at 2, 4, and 6 months of age), with the option of an additional booster dose (administered at 12-18 months of age). This dataset does not differentiate between patients who received varying levels of inoculation, and instead reports the data if any PCV was delivered."
  },
  {
    "objectID": "Pages/Materials/Student Codespaces/bdsy-phm-individual/Week2/Time Series Analysis_Hands-On Exercise.html#preamble",
    "href": "Pages/Materials/Student Codespaces/bdsy-phm-individual/Week2/Time Series Analysis_Hands-On Exercise.html#preamble",
    "title": "Time Series Analysis",
    "section": "Preamble",
    "text": "Preamble\nThis section is drawn from Dr. Weinberger’s recorded presentation What is ‘vaccine impact’?; and Administrative data: challenges and opportunities for evaluation studies 6.\nStatistical inference can be reduced to quantitatively answering three types of questions:\n\nCausation: Was there a change, and can we identify what caused the change?\nPrediction: What do we expect to see in the future?\nCertainty: How reliable are the answers to the first two questions?\n\nA biotechnologist developing a vaccine is primarily concerned with proving that there was a favorable change, such as mitigating the disease process in an individual. In contrast, a public health analyst aims to determine the overall impact of the vaccine on the population. For example, an effective vaccine not only protects an individual but also attenuates transmission within their immediate social circle.\nThis requires contextualizing the vaccine’s impact as a combination of direct and indirect effects. It is important to identify and control for unexplained linear and non-linear trends unrelated to the introduction of a vaccine to a population. For example, were there changes in overall population health at the same time or changes to diagnostic methods?\nWe can appreciate that this is a Sisyphean task, but one that can be addressed with time series analysis. Keep in mind that the methods discussed here are most applicable to endemic diseases that are consistently present in the population, existing at a relatively stable and predictable level. The core reason is that these methods require us to have an idea of what we would expect to have seen had the vaccine not been introduced, allowing us to posit a realistic counterfactual for model evaluation.\nOur study is framed by PICO. In black text are the general definitions for the acronym and in red text is the application to our example:\n\nPopulation: The target population where the impact of a vaccine is to measured. Children 2-59 months old.\nIntervention: The date and timeframe for a vaccine intervention. Introduction of PCV10 to the Brazilian national immunization program.\nComparator: Diseases or groups of diseases to compare against the intervention, which are not expected to be impacted by the intervention itself. Counterfactual to demonstrate what would have happened if the vaccine had not been introduced, compared to the factual scenario.\nOutcome: The condition representing our expected results from the intervention. Deaths due to pneumonia."
  },
  {
    "objectID": "Pages/Materials/Student Codespaces/bdsy-phm-individual/Week2/Time Series Analysis_Hands-On Exercise.html#set-up-the-environment",
    "href": "Pages/Materials/Student Codespaces/bdsy-phm-individual/Week2/Time Series Analysis_Hands-On Exercise.html#set-up-the-environment",
    "title": "Time Series Analysis",
    "section": "Set Up the Environment",
    "text": "Set Up the Environment\n\nrenv::init()      # Initialize the project     \nrenv::restore()   # Download packages and their version saved in the lockfile\n\n\nsuppressPackageStartupMessages({\n  library(\"readr\")      # For reading in the data\n  library(\"tibble\")     # For handling tidyverse tibble data classes\n  library(\"tidyr\")      # For tidying data \n  library(\"dplyr\")      # For data manipulation \n  library(\"stringr\")    # For string manipulation\n  library(\"MASS\")       # Functions/datasets for statistical analysis\n  library(\"lubridate\")  # For date manipulation\n  library(\"ggplot2\")    # For creating static visualizations\n  library(\"scales\")     # For formatting plots axis\n  library(\"gridExtra\")  # Creates multiple grid-based plots\n})\n\n\n# Function to select \"Not In\"\n'%!in%' &lt;- function(x,y)!('%in%'(x,y))\n\nThe data has been cleaned and standardized for use here, and is imported directly using the GitHub raw URL. You can explore the additional data cleaning steps applied to all of the data in the instructor’s GitHub repository: ysph-dsde/bdsy-phm. The original dataset and prior data cleaning, validation, and standardization can be found in the paper’s GitHub repository and Dr. Weinberger’s workshop GitHub repository 1,3.\n\n# Read in the cleaned data directly from the instructor's GitHub.\ndf &lt;- read_csv(\"https://raw.githubusercontent.com/ysph-dsde/bdsy-phm/refs/heads/main/Data/ec_2to59m.csv\")\n\n# Summarize aspects and dimentions of our dataset.\nglimpse(df)\n\nRows: 144\nColumns: 6\n$ date         &lt;date&gt; 2005-01-01, 2005-02-01, 2005-03-01, 2005-04-01, 2005-05-…\n$ country      &lt;chr&gt; \"Ecuador\", \"Ecuador\", \"Ecuador\", \"Ecuador\", \"Ecuador\", \"E…\n$ age_group    &lt;chr&gt; \"2-59m\", \"2-59m\", \"2-59m\", \"2-59m\", \"2-59m\", \"2-59m\", \"2-…\n$ doses        &lt;chr&gt; \"Any\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\", \"…\n$ J12_J18_prim &lt;dbl&gt; 50, 51, 53, 46, 62, 34, 38, 38, 28, 26, 33, 31, 46, 53, 5…\n$ acm_noj_prim &lt;dbl&gt; 204, 171, 166, 197, 212, 180, 158, 167, 144, 149, 153, 14…\n\n\n\nData Dictionary\nIt is crucial that we understand the meaning of each variable in our dataset. Sometimes, there are surprising aspects embedded within the variables that are not immediately discernible from the table itself. Many sources provide a “Data Dictionary” for this purpose, but at times, you may need to interpret the variable meanings based on context and methods.\nThis paper did not explicitly describe each variable in a “Data Dictionary”; therefore, the following was assembled based on the context and methods provided in the paper and its supplementary materials 3,4.\n\ndate: The month when the events were recorded. This spans from 2005-01-01 to 2016-12-01.\ncountry: Specifies the country where the events were observed. This dataset only represents events recorded in Ecuador.\nage_group: The age of the person who is represented in the counts. This dataset only represents infants aged 2 months to almost 5 years of age (59 months).\ndoses: Specifies the doses of PCV received. As described earlier, all possible combinations of the standard-of-care vaccination series and booster are represented here.\nJ12_J18_prim: Primary cause of death is assigned to the ICD-10 codes J12-J18. We encourage you to read more about what these ICD-10 codes represent 4,5.\nacm_noj_prim: Primary cause of death was assigned any other ICD=10 code, excluding only the J chapter, diseases of the respiratory system."
  },
  {
    "objectID": "Pages/Materials/Student Codespaces/bdsy-phm-individual/Week2/Time Series Analysis_Hands-On Exercise.html#initial-plot-of-the-time-series",
    "href": "Pages/Materials/Student Codespaces/bdsy-phm-individual/Week2/Time Series Analysis_Hands-On Exercise.html#initial-plot-of-the-time-series",
    "title": "Time Series Analysis",
    "section": "Initial Plot of the Time Series",
    "text": "Initial Plot of the Time Series\nWe begin each time series analysis by examining the entire span of data, typically plotted as a line or scatter plot. All methods for modeling the vaccination introduction time series data require the dates when the vaccine was introduced, in this case the PCV10 vaccine.\n\n# Estimation when the vaccine was introduced in YYYY-MM-DD format.\nvax.intro.date &lt;- as.Date(\"2010-08-01\")\n\n# Date when vaccine efficacy evaluations started; at least 12 months\n# following administration.\nvax.eval.date &lt;- as.Date(\"2011-08-01\")\n\nWe are not going to spend time explaining how to plot using the tidyverse package ggplot2() here. Later in the week you will receive a lecture covering this topic. In the meantime, you are welcome to explore the ggplot2 package documentation or the Data Science and Data Equity (DSDE) group’s online Book of Workshops 7,8.\n\np1 &lt;- \n  ggplot(df, aes(x = date, y = J12_J18_prim)) +\n      geom_line() +\n      labs(title = \"Deaths Categorized by\\nICD-10 Codes J12-18\",\n         x = \"Date\", y = \"Counts for &lt;5 yo\") +\n      # Have y-axis for the two plots be the same.\n      ylim(0, 250) +\n      # Reference line.\n      geom_vline(xintercept = vax.intro.date, col = \"red\", lty = 2) +\n      theme_linedraw()\n   \n\np2 &lt;- \n  ggplot(df, aes(x = date, y = acm_noj_prim)) +\n      geom_line() +\n      labs(title = \"Deaths Categorized by\\nNon-J chapter ICD-10 Codes\",\n         x = \"Date\", y = \"Counts for &lt;5 yo\") +\n      # Have y-axis for the two plots be the same.\n      ylim(0, 250) +\n      # Reference line.\n      geom_vline(xintercept = vax.intro.date, col = \"red\", lty = 2) +\n      theme_linedraw()\n\n# Display the plots side-by-side.\ngrid.arrange(p1, p2, nrow = 1)"
  },
  {
    "objectID": "Pages/Materials/Student Codespaces/bdsy-phm-individual/Week2/Time Series Analysis_Hands-On Exercise.html#simple-linear-model",
    "href": "Pages/Materials/Student Codespaces/bdsy-phm-individual/Week2/Time Series Analysis_Hands-On Exercise.html#simple-linear-model",
    "title": "Time Series Analysis",
    "section": "Simple Linear Model",
    "text": "Simple Linear Model\nTo correctly fit a simple linear model to the data, we need to apply a linearization that appropriately reflects its distribution. There are more analytical approaches to achieve this that will not be covered here. We start by visually examining the distribution with a scatter plot.\n\n\n\n\n\n\nNote\n\n\n\nIt is helpful to consider the data generation method, which can provide insights into the likely distribution.\n\n\n\np_base &lt;- \n  ggplot(df, aes(x = date, y = J12_J18_prim)) +\n      geom_point() +\n      labs(title = \"Deaths Scatter Plot\",\n         x = \"Date\", y = \"Counts for &lt;5 yo\") +\n      # Have y-axis for the two plots be the same.\n      ylim(0, NA) +\n      theme_linedraw()\n\np_base\n\n\n\n\n\n\n\n\nVariables that reflect counts (i.e., the number of deaths per month) can be distributed as either Poisson or negative binomial. Note that Poisson regression may artificially narrow confidence intervals when the data is overdispersed. To address this, you can use a negative binomial regression or a quasipoisson model, which accounts for the unexplained variation. In this context, we will apply a negative binomial regression.\n\n# For modeling, we need to use an ordered, discrete variable. Simply, we \n# can use the rownames for this purpose.\ndf &lt;- tibble::rownames_to_column(df, var = \"index\") %&gt;%\n  mutate(index = as.numeric(index))\n\n# Apply the negative binomial regression.\nmod1 &lt;- glm.nb(J12_J18_prim ~ index , data = df)\n\n# Examine the fitting results.\nsummary(mod1)\n\n\nCall:\nglm.nb(formula = J12_J18_prim ~ index, data = df, init.theta = 38.18819754, \n    link = log)\n\nCoefficients:\n              Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  3.8223994  0.0389150   98.22   &lt;2e-16 ***\nindex       -0.0062897  0.0004981  -12.63   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Negative Binomial(38.1882) family taken to be 1)\n\n    Null deviance: 304.02  on 143  degrees of freedom\nResidual deviance: 142.25  on 142  degrees of freedom\nAIC: 976.12\n\nNumber of Fisher Scoring iterations: 1\n\n              Theta:  38.2 \n          Std. Err.:  10.2 \n\n 2 x log-likelihood:  -970.12 \n\n\n\n# Make predictions with confidence intervals.\npred &lt;- predict(mod1, type = \"response\", se.fit = TRUE)\n\n# Add the model predictions and 95% COI to the dataframe.\ndf.pred &lt;- df %&gt;%\n  mutate(se.fit = pred$se.fit, pred = pred$fit) %&gt;%\n  mutate(\n    conf.low = pred - 1.96 * se.fit,\n    conf.high = pred + 1.96 * se.fit\n  )\n\n# Plot the newly created model fitting.\np_sm &lt;- p_base +\n  # Add the fitting line.\n  geom_line(data = df.pred, aes(x = date, y = pred),\n            color = \"#e41a1c\") +\n  # Add the confidence interval.\n  geom_ribbon(data = df.pred, aes(ymin = conf.low, ymax = conf.high), \n              alpha = 0.2, fill = \"blue\") +\n  # Change the title name.\n  labs(title = \"Deaths with a Negative Binomial Fit\")\n\np_sm\n\n\n\n\n\n\n\n\nThis is not a terrible predictor of our trend, but it overlooks many known sources of variance. For instance, we know that infections have seasonal trends, sometimes referred to by its technical term periodicity. Additionally, our baseline population may change from 2005 to 2016, which can consequently shift the overall disease trend in tandem with these baseline changes."
  },
  {
    "objectID": "Pages/Materials/Student Codespaces/bdsy-phm-individual/Week2/Time Series Analysis_Hands-On Exercise.html#building-the-multiple-linear-model",
    "href": "Pages/Materials/Student Codespaces/bdsy-phm-individual/Week2/Time Series Analysis_Hands-On Exercise.html#building-the-multiple-linear-model",
    "title": "Time Series Analysis",
    "section": "Building the Multiple Linear Model",
    "text": "Building the Multiple Linear Model\n\n\n\n\nPrinciples of Seasonality - Generated with Yale’s AI Clarity\n\n\n\n\nAdd Controls for Seasonality\nSeasonality often manifests as periodicity, where recurring patterns repeat at regular, fixed intervals of time. This differs from the related concept of cyclicity, which represents recurring patterns that do not occur at regular intervals or consistently appear in regular seasons 9.\nEach seasonal pattern can be decomposed into three components that can either remain constant (stationary) or change over time (non-stationary): mean, variance, and covariance. The figure on the left illustrates the four possible seasonal trends separately. The top-left panel shows a recurring pattern with time-invariant, stationary parameters, while the other panels demonstrate the effects of non-stationary parameters.\n\n\n\n\n\n\nNote\n\n\n\nMost time series data in the natural sciences exhibit seasonality, periodicity, or cyclicity, though these patterns may be difficult to detect when the data is noisy 10.\n\n\nTo control for temporal variations such as seasonality, we employ a simple form of dynamic linear regression (DLR). Essentially, we add variables to our simple linear regression model that represents different subsections of time as new regressors. Applying a DLR allows for changes in the mean value of the underlying regression relationship 11. For the regression function to recognize each date, we need to factorize the date variable to assign the month the observation occured.\n\ndf$month &lt;- as.factor(month(df$date))\n\n# Inspect the first 36 entries.\ndf$month[1:36]\n\n [1] 1  2  3  4  5  6  7  8  9  10 11 12 1  2  3  4  5  6  7  8  9  10 11 12 1 \n[26] 2  3  4  5  6  7  8  9  10 11 12\nLevels: 1 2 3 4 5 6 7 8 9 10 11 12\n\n# Update the model.\nmod2 &lt;- glm.nb(J12_J18_prim ~ date + month, data = df)\nsummary(mod2)\n\n\nCall:\nglm.nb(formula = J12_J18_prim ~ date + month, data = df, init.theta = 88.28907799, \n    link = log)\n\nCoefficients:\n              Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  6.496e+00  2.156e-01  30.132  &lt; 2e-16 ***\ndate        -1.988e-04  1.426e-05 -13.941  &lt; 2e-16 ***\nmonth2      -2.717e-02  8.166e-02  -0.333 0.739391    \nmonth3       3.446e-02  8.086e-02   0.426 0.669971    \nmonth4      -1.892e-01  8.441e-02  -2.241 0.025011 *  \nmonth5      -4.964e-02  8.228e-02  -0.603 0.546315    \nmonth6      -1.571e-01  8.409e-02  -1.868 0.061714 .  \nmonth7      -2.118e-01  8.513e-02  -2.488 0.012840 *  \nmonth8      -2.076e-01  8.518e-02  -2.437 0.014804 *  \nmonth9      -3.266e-01  8.749e-02  -3.733 0.000189 ***\nmonth10     -2.253e-01  8.574e-02  -2.627 0.008603 ** \nmonth11     -2.467e-01  8.626e-02  -2.859 0.004243 ** \nmonth12     -3.386e-01  8.813e-02  -3.842 0.000122 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Negative Binomial(88.2891) family taken to be 1)\n\n    Null deviance: 404.40  on 143  degrees of freedom\nResidual deviance: 143.84  on 131  degrees of freedom\nAIC: 959.32\n\nNumber of Fisher Scoring iterations: 1\n\n              Theta:  88.3 \n          Std. Err.:  41.2 \n\n 2 x log-likelihood:  -931.319 \n\n\n\n# Make predictions with confidence intervals.\npred2 &lt;- predict(mod2, type = \"response\", se.fit = TRUE)\n\n# Add the model predictions and 95% COI to the dataframe.\ndf.pred2 &lt;- df %&gt;%\n  mutate(se.fit = pred2$se.fit, pred = pred2$fit) %&gt;%\n  mutate(\n    conf.low = pred - 1.96 * se.fit,\n    conf.high = pred + 1.96 * se.fit\n  )\n\n# Plot the newly created model fitting.\np_season &lt;- p_base +\n  # Add the fitting line.\n  geom_line(data = df.pred2, aes(x = date, y = pred),\n            color = \"#e41a1c\") +\n  # Add the confidence interval.\n  geom_ribbon(data = df.pred2, aes(ymin = conf.low, ymax = conf.high), \n              alpha = 0.2, fill = \"blue\") +\n  # Change the title name.\n  labs(title = \"Deaths with a Negative Binomial Fit\\nSeasonality Term Included\")\n\np_season\n\n\n\n\n\n\n\n\n\n\nAdd Controls for Baseline Shifts\n\n\n\n\nFigure by Dr. Dan Weinberger - Generated with Yale’s AI Clarity  6\n\n\n\nWe can expect that the baseline population changes over time, introducing unaccounted-for heterogeneous variance to our model. In epidemiology, it is standard to contextualize raw data by applying a denominator, or offset, that represents the population at risk of the disease.\nThe figure on the right illustrates how the baseline for the number of hospitalizations normalizes when we apply the population offset. Keep in mind that this ratio is often scaled to “per 100,000 persons.” 6.\n\n\\frac{n_{cases}}{n_{population}} \\times 100,000\n\nIn real-world scenarios, the total base population is not always reported, or the coverage might be unreliable for the entire span of the time series. Alternatively, we can apply denominators such as the number of people using the healthcare system, the number of people hospitalized, and similar metrics 6. In this example, we could apply the total population size or all non-respiratory causes of mortality.\n\n# Create the offset using all deaths not coded as J. Transform the values\n# to log before use in the negative binomial fitting.\ndf$log.offset &lt;- log(df$acm_noj_prim)\n\n# Refit the model with an offset.\nmodel3 &lt;- glm.nb(J12_J18_prim ~ index + month + offset(log.offset), data = df)\nsummary(model3)\n\n\nCall:\nglm.nb(formula = J12_J18_prim ~ index + month + offset(log.offset), \n    data = df, init.theta = 59.08482937, link = log)\n\nCoefficients:\n              Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -1.3687586  0.0672076 -20.366  &lt; 2e-16 ***\nindex       -0.0034881  0.0004599  -7.584 3.36e-14 ***\nmonth2       0.1651225  0.0873636   1.890   0.0587 .  \nmonth3       0.1509818  0.0867654   1.740   0.0818 .  \nmonth4      -0.0005318  0.0899497  -0.006   0.9953    \nmonth5       0.0704091  0.0880910   0.799   0.4241    \nmonth6       0.0227796  0.0896446   0.254   0.7994    \nmonth7      -0.0122476  0.0906691  -0.135   0.8925    \nmonth8       0.0077403  0.0907734   0.085   0.9320    \nmonth9      -0.0526956  0.0929670  -0.567   0.5708    \nmonth10     -0.0053806  0.0912550  -0.059   0.9530    \nmonth11      0.0443924  0.0916610   0.484   0.6282    \nmonth12     -0.0283404  0.0935272  -0.303   0.7619    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Negative Binomial(59.0848) family taken to be 1)\n\n    Null deviance: 219.59  on 143  degrees of freedom\nResidual deviance: 145.49  on 131  degrees of freedom\nAIC: 977.59\n\nNumber of Fisher Scoring iterations: 1\n\n              Theta:  59.1 \n          Std. Err.:  21.1 \n\n 2 x log-likelihood:  -949.594 \n\n\n\n# Make predictions with confidence intervals.\npred3 &lt;- predict(model3, type = \"response\", se.fit = TRUE)\n\n# Add the model predictions and 95% COI to the dataframe.\ndf.pred3 &lt;- df %&gt;%\n  mutate(se.fit = pred3$se.fit, pred = pred3$fit) %&gt;%\n  mutate(\n    conf.low = pred - 1.96 * se.fit,\n    conf.high = pred + 1.96 * se.fit\n  )\n\n# Plot the newly created model fitting.\np_offset &lt;- p_base +\n  # Add the fitting line.\n  geom_line(data = df.pred3, aes(x = date, y = pred),\n            color = \"#e41a1c\") +\n  # Add the confidence interval.\n  geom_ribbon(data = df.pred3, aes(ymin = conf.low, ymax = conf.high), \n              alpha = 0.2, fill = \"blue\") +\n  # Change the title name.\n  labs(title = \"Deaths with a Negative Binomial Fit\\nSeasonality Term and Offset\")\n\np_offset\n\n\n\n\n\n\n\n\nNotice that the seasonality trend, which was clear in the previous plot, is disrupted when the population offset is applied. Now that we have our baseline model, we are ready to proceed with examining the effect of the intervention with PCV10 after its introduction on 2010-08-01."
  },
  {
    "objectID": "Pages/Materials/Student Codespaces/bdsy-phm-individual/Week2/Time Series Analysis_Hands-On Exercise.html#does-the-disease-trend-or-level-change",
    "href": "Pages/Materials/Student Codespaces/bdsy-phm-individual/Week2/Time Series Analysis_Hands-On Exercise.html#does-the-disease-trend-or-level-change",
    "title": "Time Series Analysis",
    "section": "Does the Disease Trend or Level Change?",
    "text": "Does the Disease Trend or Level Change?\nIn this section, we will test whether the trend or level of pneumonia deaths due to pneumococcal disease changes after the introduction of the PCV10 vaccine. To accomplish this, we will set up a counterfactual model to represent what our base model predicts the death counts would have been if the vaccine had not been introduced to the population. Similarly, a factual model will be applied to represent what actually happened with the vaccine rollout.\nWe will then compare the factual model against the counterfactual model to determine if a difference is identified in the period following vaccine introduction. This process is generally referred to as detecting vaccine effect or vaccine impact.\n\n\n\n\n\n\nWarning\n\n\n\nYou want to be careful when applying causal inference methods to model building, perturbation, or prediction questions. While our analysis uses a causal framework to detect the possible impact of the vaccine introduction, this does not permit us to make definitive claims of causation.\nAdditionally, it is important to remember that the model we have built may have other flaws influencing its sensitivity to the pre/post vaccination period. It is always best practice to test a model under different conditions and subject it to various tests to robustly justify its accuracy, precision, and generalizability.\n\n\nThe figure below illustrates the three phases of the vaccine rollout: pre-vaccination, post-vaccination, and a latency period between the start of vaccine distribution and efficacy evaluation. Recall that our base negative binomial model accounts for three effector variables: an ordered, discrete variable organizing the outcomes temporally without seasonality, the month to capture seasonal periodicity, and an offset to the baseline accounting for population changes. Our baseline model is expected to, and is therefore assumed to, detect the drop in average cases during the post-vaccination period.\n\n\n\nFigure by Dr. Dan Weinberger - From his recorded lecture Interrupted time series analysis 12.\n\n\nThis model, however, does not explicitly assign outcomes to the three phases represented in the figure. Therefore, we need to set up variables that distinguish outcomes before and after the vaccine introduction, as well as before and after the entry to the vaccine evaluation phase, in order to convert the base model into our factual and counterfactual models.\nWe also need to regenerate the model to produce factually-based and counterfactually-based predictions for comparison. There are three modeling approaches we will evaluate here, each employing a different method to define the pre/post and latency vaccination periods. These differences will impact our inference about vaccine effectiveness.\n\nInterrupted Time Series with Disconnected Segments: This method fits different line segments through the data and tests whether the slope or level of the disease changes. It can sometimes result in abrupt jumps when fitting the model.\nInterrupted Time Series with Connected Segments (Spline Model): This method allows the slope to change in the post-vaccine period but ensures the change is smooth.\nExtrapolation Based on the Pre-Vaccine Period: This method fits the model to data from the pre-vaccine period only and extrapolates the trend to the post-vaccine period.\n\n\nMethod 1. Interrupted Time Series with Disconnected Segments\nWe will add two new binary variables to our dataset: vax_intro and vax_eval. The vax_intro variable will be assigned a value of 0 before 2010-08-01 and 1 afterward; similarly, the vax_eval variable will be assigned a value of 0 before 2011-08-01 and 1 afterward.\n\n# Assign outcomes to the known vaccine phases.\ndf &lt;- df %&gt;%\n  mutate(\n    vax_intro = ifelse(date &gt;= vax.intro.date, 1, 0),\n    vax_eval = ifelse(date &gt;= vax.eval.date, 1, 0)\n  )\n\n# View the changes by randomly selecting dates.\ndf[sort(sample(1:144, 10)), c(\"index\", \"date\", \"vax_intro\", \"vax_eval\")]\n\n# A tibble: 10 × 4\n   index date       vax_intro vax_eval\n   &lt;dbl&gt; &lt;date&gt;         &lt;dbl&gt;    &lt;dbl&gt;\n 1    13 2006-01-01         0        0\n 2    24 2006-12-01         0        0\n 3    35 2007-11-01         0        0\n 4    39 2008-03-01         0        0\n 5    65 2010-05-01         0        0\n 6    68 2010-08-01         1        0\n 7    85 2012-01-01         1        1\n 8    89 2012-05-01         1        1\n 9    99 2013-03-01         1        1\n10   130 2015-10-01         1        1\n\n\nIn linear modeling, it is common to include interaction terms. These terms allow the model to capture interdependencies between variables, showing how the effect of one variable on the outcome changes depending on the level of another variable. Interaction terms also enable the identification of synergistic effects, where the combined effect of two variables is greater (or less) than the sum of their individual effects 13.\nEquation 1 shows a multiple linear regression without an interaction term, and Equation 2 shows the inclusion of the interaction term. Notice that including the interaction term impacts the slope, making it dependent on the value of the other predictor. For example, the impact of X_1, holding X_2 constant, on the slope of Y is a function of (\\beta_1 + \\beta_3)\\ X_2 13.\n\\begin{align}\nY &= \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\epsilon\\\\\nY &= \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 (X_1 \\times X_2) + \\epsilon\n\\end{align}\nWe will not spend more time discussing interaction terms but will briefly examine if they enhance the model’s predictive power. Note that while interaction terms can improve predictive accuracy, they also complicate the model and may result in a trade-off between predictive power and generalizability to other datasets.\nFirst we will generate the two models, then we will evaluate their performance side-by-side.\n\nNo Interaction Terms\nCreate a simple step-change model without an interaction term involving the newly added variables, vax_intro and vax_eval.\n\n# Additional seasonality controls covered in the collapsed box above.\nmod_method1a &lt;- glm.nb(J12_J18_prim ~ index + month + offset(log.offset) +\n                         # Change in disease during administration and \n                         # evaluation period.\n                         vax_intro + vax_eval, data = df)\n\nsummary(mod_method1a)\n\n\nCall:\nglm.nb(formula = J12_J18_prim ~ index + month + offset(log.offset) + \n    vax_intro + vax_eval, data = df, init.theta = 64.37470892, \n    link = log)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -1.382710   0.069290 -19.956   &lt;2e-16 ***\nindex       -0.002510   0.001014  -2.476   0.0133 *  \nmonth2       0.163233   0.086008   1.898   0.0577 .  \nmonth3       0.147780   0.085409   1.730   0.0836 .  \nmonth4      -0.004393   0.088661  -0.050   0.9605    \nmonth5       0.065323   0.086796   0.753   0.4517    \nmonth6       0.017042   0.088446   0.193   0.8472    \nmonth7      -0.018766   0.089510  -0.210   0.8339    \nmonth8       0.006893   0.089471   0.077   0.9386    \nmonth9      -0.054212   0.091693  -0.591   0.5544    \nmonth10     -0.006595   0.089942  -0.073   0.9416    \nmonth11      0.041625   0.090381   0.461   0.6451    \nmonth12     -0.031725   0.092285  -0.344   0.7310    \nvax_intro   -0.167101   0.083391  -2.004   0.0451 *  \nvax_eval     0.076690   0.083253   0.921   0.3570    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Negative Binomial(64.3747) family taken to be 1)\n\n    Null deviance: 225.73  on 143  degrees of freedom\nResidual deviance: 145.44  on 129  degrees of freedom\nAIC: 977.59\n\nNumber of Fisher Scoring iterations: 1\n\n              Theta:  64.4 \n          Std. Err.:  24.3 \n\n 2 x log-likelihood:  -945.595 \n\n\n\n## --------------------\n## Generate the factual model\n\n# Make predictions with confidence intervals.\npred4 &lt;- predict(mod_method1a, type = \"response\", se.fit = TRUE)\n\n\n## --------------------\n## Prepare a plot to visualize the factual model predictions\n\n# Add the model predictions and 95% COI to the dataframe.\ndf.pred.its &lt;- df %&gt;%\n  mutate(se.fit = pred4$se.fit, pred = pred4$fit) %&gt;%\n  mutate(\n    conf.low = pred - 1.96 * se.fit,\n    conf.high = pred + 1.96 * se.fit\n  )\n\n# Plot the newly created model fitting.\np_m1a &lt;- p_base +\n  # Add the fitting line.\n  geom_line(data = df.pred.its, aes(x = date, y = pred),\n            color = \"#377eb8\") +\n  # Add the confidence interval.\n  geom_ribbon(data = df.pred.its, aes(ymin = conf.low, ymax = conf.high), \n              alpha = 0.2, fill = \"blue\") +\n  # Change the title name.\n  labs(title = \"Deaths with a Negative Binomial Fit\\nInterupted Time Series - No Interaction Terms\")\n\n\n## --------------------\n## Generate the counterfactual model\n\n# Initialize the vacccine effect variables by setting them to 0.\ndf.counterfactual.a &lt;- df\ndf.counterfactual.a$vax_intro &lt;- 0\ndf.counterfactual.a$vax_eval &lt;- 0\n\n# Generate the fitted values using the counterfactual setup instead.\ndf.pred.its$pred4.cf.a &lt;- predict(mod_method1a, type = \"response\", \n                                 newdata = df.counterfactual.a)\n\n\n## --------------------\n## Rate ratio to evaluate performance\n\n# Generate the rate ratio between the factual and counterfactual values.\ndf.pred.its$rr.its.a &lt;- df.pred.its$pred/df.pred.its$pred4.cf.a\n\n\n\nWith Interaction Terms\nCreate the more complicated model option by adding an interaction term involving vax_intro and vax_eval, keeping both individually and including with an interaction with index.\n\nmod_method1b &lt;- glm.nb(J12_J18_prim~index + month + offset(log.offset) +\n                         # Add the counterfactuals with an interaction term.\n                         vax_intro + vax_intro*index +\n                         vax_eval + vax_eval*index, data = df)\n\nsummary(mod_method1b)\n\n\nCall:\nglm.nb(formula = J12_J18_prim ~ index + month + offset(log.offset) + \n    vax_intro + vax_intro * index + vax_eval + vax_eval * index, \n    data = df, init.theta = 65.62229438, link = log)\n\nCoefficients:\n                 Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)     -1.410897   0.074005 -19.065   &lt;2e-16 ***\nindex           -0.001660   0.001288  -1.288   0.1976    \nmonth2           0.163960   0.085713   1.913   0.0558 .  \nmonth3           0.148234   0.085153   1.741   0.0817 .  \nmonth4          -0.003138   0.088498  -0.035   0.9717    \nmonth5           0.067516   0.086678   0.779   0.4360    \nmonth6           0.019034   0.088478   0.215   0.8297    \nmonth7          -0.016266   0.089654  -0.181   0.8560    \nmonth8           0.003650   0.089526   0.041   0.9675    \nmonth9          -0.055524   0.091668  -0.606   0.5447    \nmonth10         -0.008492   0.089787  -0.095   0.9247    \nmonth11          0.039981   0.090176   0.443   0.6575    \nmonth12         -0.032091   0.092017  -0.349   0.7273    \nvax_intro        0.267869   1.533398   0.175   0.8613    \nvax_eval        -0.184445   1.541114  -0.120   0.9047    \nindex:vax_intro -0.006384   0.020854  -0.306   0.7595    \nindex:vax_eval   0.004214   0.020870   0.202   0.8400    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Negative Binomial(65.6223) family taken to be 1)\n\n    Null deviance: 227.08  on 143  degrees of freedom\nResidual deviance: 145.13  on 127  degrees of freedom\nAIC: 980.44\n\nNumber of Fisher Scoring iterations: 1\n\n              Theta:  65.6 \n          Std. Err.:  25.0 \n\n 2 x log-likelihood:  -944.439 \n\n\n\n## --------------------\n## Generate the factual model\n\n# Make predictions with confidence intervals.\npred5 &lt;- predict(mod_method1b, type = \"response\", se.fit = TRUE)\n\n\n## --------------------\n## Prepare a plot to visualize the factual model predictions\n\n# Add the model predictions and 95% COI to the dataframe.\ndf.pred.its &lt;- df.pred.its %&gt;%\n  mutate(se.fit = pred5$se.fit, pred = pred4$fit) %&gt;%\n  mutate(\n    conf.low = pred - 1.96 * se.fit,\n    conf.high = pred + 1.96 * se.fit\n  )\n\n# Plot the newly created model fitting.\np_m1b &lt;- p_base +\n  # Add the fitting line.\n  geom_line(data = df.pred.its, aes(x = date, y = pred),\n            color = \"#377eb8\") +\n  # Add the confidence interval.\n  geom_ribbon(data = df.pred.its, aes(ymin = conf.low, ymax = conf.high), \n              alpha = 0.2, fill = \"blue\") +\n  # Change the title name.\n  labs(title = \"Deaths with a Negative Binomial Fit\\nInterupted Time Series - With Interaction Terms\")\n\n\n## --------------------\n## Generate the counterfactual model\n\n# Initialize the vacccine effect variables by setting them to 0.\ndf.counterfactual.b &lt;- df\ndf.counterfactual.b$vax_intro &lt;- 0\ndf.counterfactual.b$vax_eval &lt;- 0\n\n# Generate the fitted values using the counterfactual setup instead.\ndf.pred.its$pred4.cf.b &lt;- predict(mod_method1b, type = \"response\", \n                                  newdata = df.counterfactual.b)\n\n\n## --------------------\n## Rate ratio to evaluate performance\n\n# Generate the rate ratio between the factual and counterfactual values.\ndf.pred.its$rr.its.b &lt;- df.pred.its$pred/df.pred.its$pred4.cf.b\n\n\n\nCompare Model Complexity\nLet’s start by comparing how the fit differs between the models.\n\n# Overlay onto plot.\np_m1a_pred &lt;- p_m1a +\n geom_line(data = df.pred.its, aes(x = date, y = pred4.cf.a),\n           color = '#e41a1c', lty = 2)\n\n# Overlay onto plot.\np_m1b_pred &lt;- p_m1b +\n geom_line(data = df.pred.its, aes(x = date, y = pred4.cf.b),\n           color = '#e41a1c', lty = 2)\n\ngrid.arrange(p_m1a_pred, p_m1b_pred, ncol = 1)\n\n\n\n\n\n\n\n\nWe do see differences with the prediction adherance to the data we trained the model on. By qualitative examination, it appears the model with interaction terms diverge more in the post-vaccination period.\nThe Akaike Information Criterion (AIC) is a one measure used to compare the goodness of fit of different statistical models, while also accounting for model complexity. It helps in model selection by balancing model fit and complexity, otherwise called the bias/variance trade-off.\nThere are different variations of AIC, but the basic definition is\n\n\\text{AIC} = 2k - 2\\ln(\\mathcal{L})\n\nwhere k is the number of parameters in the model and \\mathcal{L} is the maximum likelihood of the model.\n\nAIC(mod_method1a, mod_method1b)\n\n             df      AIC\nmod_method1a 16 977.5948\nmod_method1b 18 980.4387\n\n\nThe results indicate that the AIC score is worse (higher) when interaction terms are included. Since there is no significant gain in performance, we will prefer the simpler model for better generalizability.\nBelow, we also observe that the interaction terms create unexpected trends in the factual/counterfactual ratio. Notably, the ratio is not 1 (indicating that factual and counterfactual predictions are the same) in the pre-vaccination period, which is not ideal.\n\n# Inspect methods sensitivity to vaccine impact.\np_ratio &lt;- \n  ggplot(df.pred.its, aes(x = date, y = rr.its.a)) +\n      geom_line() +\n      labs(title = \"Rate Ratio of the ITS Model\",\n         x = \"Date\", y = \"Rate ratio\") +\n      ylim(0, NA) +\n      geom_vline(xintercept = vax.intro.date, col = \"red\", lty = 2) +\n      annotate(\"text\", x = vax.intro.date + 30, y = 00.1, label = \"Vaccine Introduced\", color = \"red\", hjust = 0) +\n      theme_linedraw()\n\np_ratio +\n  geom_line(data = df.pred.its, aes(x = date, y = rr.its.b),\n            color = \"#4daf4a\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode Like a Pro\n\n\n\nWe won’t try to estimate a confidence interval on the rate ratio here. Students are encouraged to checkout the the InterventionEvaluatR package, which will automatically calculate the confidence intervals for these ratios.\n\n\n\n\n\nMethod 2. Interrupted Time Series with Connected Segments (Spline Model)\nIn this case, we force the changes to be smooth so that we don’t get a drastic jump after vaccination introduction that prematurly shows impact during the expected latency period. Therefore, we will add two new variables to our dataset: spl1 and spl2. The spl1 variable will be assigned a value of 0 before 2010-08-01 and index - intro.index + 1 afterward; similarly, the spl2 variable will be assigned a value of 0 before 2011-08-01 and index - eval.index + 1 afterward.\n\n# Identifies the row index that represents when the vaccine was introduced\n# and when the evaluation period started.\nintro.index &lt;- which(df$date == vax.intro.date)\neval.index  &lt;- which(df$date == vax.eval.date)\n\n# Assign outcomes to the known vaccine phases.\ndf &lt;- df %&gt;%\n  mutate(\n    spl1 = ifelse(index - intro.index + 1 &lt; 0, 0, index - intro.index + 1),\n    spl2 = ifelse(index - eval.index + 1 &lt; 0, 0, index - eval.index + 1)\n  )\n\n# Inspect the changes.\ndf[sort(sample(1:144, 10)), c(\"date\", \"index\", \"spl1\", \"spl2\")]\n\n# A tibble: 10 × 4\n   date       index  spl1  spl2\n   &lt;date&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 2005-04-01     4     0     0\n 2 2008-04-01    40     0     0\n 3 2009-04-01    52     0     0\n 4 2011-09-01    81    14     2\n 5 2011-12-01    84    17     5\n 6 2012-04-01    88    21     9\n 7 2012-09-01    93    26    14\n 8 2015-03-01   123    56    44\n 9 2015-05-01   125    58    46\n10 2015-12-01   132    65    53\n\n\n\nmod_method2 &lt;- glm.nb(J12_J18_prim ~ index + month + offset(log.offset) +\n                        # Post-vaccine changes.\n                        spl1 + spl2, data = df)\n\nsummary(mod_method2)\n\n\nCall:\nglm.nb(formula = J12_J18_prim ~ index + month + offset(log.offset) + \n    spl1 + spl2, data = df, init.theta = 61.04668124, link = log)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -1.397121   0.074586 -18.732   &lt;2e-16 ***\nindex       -0.002349   0.001237  -1.899   0.0576 .  \nmonth2       0.164546   0.086841   1.895   0.0581 .  \nmonth3       0.151023   0.086225   1.752   0.0799 .  \nmonth4      -0.001397   0.089442  -0.016   0.9875    \nmonth5       0.069656   0.087565   0.795   0.4263    \nmonth6       0.020998   0.089150   0.236   0.8138    \nmonth7      -0.013534   0.090170  -0.150   0.8807    \nmonth8       0.006429   0.090270   0.071   0.9432    \nmonth9      -0.055311   0.092521  -0.598   0.5500    \nmonth10     -0.008406   0.090793  -0.093   0.9262    \nmonth11      0.040934   0.091221   0.449   0.6536    \nmonth12     -0.032326   0.093106  -0.347   0.7284    \nspl1        -0.007961   0.007038  -1.131   0.2580    \nspl2         0.007591   0.007271   1.044   0.2965    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Negative Binomial(61.0467) family taken to be 1)\n\n    Null deviance: 221.95  on 143  degrees of freedom\nResidual deviance: 145.72  on 129  degrees of freedom\nAIC: 980.3\n\nNumber of Fisher Scoring iterations: 1\n\n              Theta:  61.0 \n          Std. Err.:  22.3 \n\n 2 x log-likelihood:  -948.296 \n\n\n\n## --------------------\n## Generate the factual model\n\n# Make predictions with confidence intervals.\ndf.pred.spl &lt;- df %&gt;%\n  mutate(pred.spl = predict(mod_method2, type = \"response\"))\n\n\n## --------------------\n## Generate the counterfactual model\n\n# Initialize the vacccine effect variables by setting them to 0.\ndf.counterfactual.spl &lt;- df\ndf.counterfactual.spl$spl1 &lt;- 0\ndf.counterfactual.spl$spl2 &lt;- 0\n\n# Generate the fitted values.\ndf.pred.spl$pred.spl.cf &lt;- predict(mod_method2, type = \"response\", \n                                   newdata = df.counterfactual.spl)\n\n\n## --------------------\n## Rate ratio to evaluate performance\n\n# Generate the rate ratio between the fitted and counterfactual values.\ndf.pred.spl$rr.spline &lt;- df.pred.spl$pred.spl/df.pred.spl$pred.spl.cf\n\n\n## --------------------\n## Prepare a plot to visualize the factual model predictions\n\n# Make predictions with confidence intervals.\npred6 &lt;- predict(mod_method2, type = \"response\", se.fit = TRUE)\n\n# Add the model predictions and 95% COI to the dataframe.\ndf.pred.spl &lt;- df.pred.spl %&gt;%\n  mutate(se.fit = pred6$se.fit, pred = pred6$fit) %&gt;%\n  mutate(\n    conf.low = pred - 1.96 * se.fit,\n    conf.high = pred + 1.96 * se.fit\n  )\n\n\np_m2 &lt;- \n  ggplot(df.pred.spl, aes(x = date, y = J12_J18_prim)) +\n      geom_point() +\n      # Add the fitted line.\n      geom_line(data = df.pred.spl, aes(x = date, y = pred.spl),\n                color = \"#377eb8\") +\n      # Add the confidence interval.\n      geom_ribbon(data = df.pred.spl, aes(ymin = conf.low, ymax = conf.high),\n                  alpha = 0.2, fill = \"blue\") +\n      # Add the counterfactual line.\n      geom_line(data = df.pred.spl, aes(x = date, y = pred.spl.cf),\n                color = \"#e41a1c\", lty = 2) +\n      labs(title = \"Deaths with a Negative Binomial Fit\\nITS with Spline Smoothing\",\n         x = \"Date\", y = \"Counts for &lt;5 yo\") +\n      theme_linedraw()\n\np_m2\n\n\n\n\n\n\n\n\nWe can see here that the decline follows a smoother trajectory, by design. We can check the counterfactual performace by examining the ratio again.\n\n# Inspect methods sensitivity to vaccine impact.\np_ratio &lt;- p_ratio +\n     geom_line(data = df.pred.spl, aes(x = date, y = rr.spline),\n               color = \"#4daf4a\") +\n      # Update the title.\n      labs(title = \"Rate Ratio with the Spline Model\")\n\np_ratio\n\n\n\n\n\n\n\n\n\n\nMethod #3: Extrapolation Based on the Pre-Vaccine Period\nNow we are going to model only the pre-vaccination period and forecast outcomes based on the restricted fitting into the post-vaccination period. We will create a new variable that excludes any records following 2010-08-01, J12_J18_prim.\n\n# Create a new variable where it is J12_J18_prim until the vaccine is introduced,\n# then it is NA afterwards.\ndf$J12_J18_prim_pre &lt;- df$J12_J18_prim\ndf$J12_J18_prim_pre[which(df$date &gt;= vax.intro.date)] &lt;- NA\n\n# Inspect the changes.\ndf[sort(sample(1:144, 10)), c(\"date\", \"J12_J18_prim_pre\")]\n\n# A tibble: 10 × 2\n   date       J12_J18_prim_pre\n   &lt;date&gt;                &lt;dbl&gt;\n 1 2006-06-01               36\n 2 2006-08-01               37\n 3 2007-08-01               28\n 4 2009-07-01               20\n 5 2010-01-01               34\n 6 2010-11-01               NA\n 7 2015-04-01               NA\n 8 2015-06-01               NA\n 9 2015-12-01               NA\n10 2016-07-01               NA\n\n\n\nmod_method3 &lt;- glm.nb(J12_J18_prim_pre ~ index + month + offset(log.offset), data = df)\n\n# Add the prediction using the smoothed model.\ndf.pred.pre &lt;- df %&gt;%\n  mutate(pred.pre = predict(mod_method3, type = \"response\", newdata = df))\n\nUnlike before, the original observed variable J12_J18_prim now represents the factual model, as we regressed on a subset of data assuming nothing was known following the vaccine introduction. The prediction results using the restricted regressand represents the counterfactual model, since it extrapolates data points missing from the model fitting.\n\n## --------------------\n## Rate ratio to evaluate performance\n\n# Generate the rate ratio between the factual and counterfactual values.\ndf.pred.pre$rr.trend &lt;- df$J12_J18_prim/df.pred.pre$pred.pre\n\n\n## --------------------\n## Prepare a plot to visualize the factual model predictions\n\n# Make predictions with confidence intervals.\npred7 &lt;- predict(mod_method3, type = \"response\", se.fit = TRUE, newdata = df)\n\n# Add the model predictions and 95% COI to the dataframe.\ndf.pred.pre &lt;- df.pred.pre %&gt;%\n  mutate(se.fit = pred7$se.fit, pred = pred7$fit) %&gt;%\n  mutate(\n    conf.low = pred - 1.96 * se.fit,\n    conf.high = pred + 1.96 * se.fit\n  )\n\n\np_m3 &lt;- \n  ggplot(df.pred.pre, aes(x = date, y = J12_J18_prim)) +\n      geom_point() +\n      # Add the fitted line.\n      geom_line(data = df.pred.pre, aes(x = date, y = pred.pre),\n                color = \"#377eb8\") +\n      # Add the confidence interval.\n      geom_ribbon(data = df.pred.pre, aes(ymin = conf.low, ymax = conf.high),\n                  alpha = 0.2, fill = \"blue\") +\n      labs(title = \"Deaths with a Negative Binomial Fit\\nExtrapolating Based on the Pre-Vaccine Period\",\n         x = \"Date\", y = \"Counts for &lt;5 yo\") +\n      theme_linedraw()\n\np_m3\n\n\n\n\n\n\n\n\n\n# Inspect methods sensitivity to vaccine impact.\np_ratio &lt;- p_ratio +\n  geom_line(data = df.pred.pre, aes(x = date, y = rr.trend),\n            color = '#377eb8') +\n  # Update the title.\n  labs(title = \"Rate Ratio with the Extrapolation Model\")\n\np_ratio\n\n\n\n\n\n\n\n\n\n\nCompare the Methods\nLet’s compare the rate ratio estimates from the three models. As a rough estimate, we will average the point-by-point estimates of the rate ratio during the evaluation period.\n\n# Create a Boolean variable that defines the vaccine evaluation period.\neval.period &lt;- df$date &gt; vax.eval.date\n\n# Calculate the average outcome during the vaccine evaluation period.\nrr.its.eval    &lt;- mean(df.pred.its$rr.its.a[eval.period])\nrr.spline.eval &lt;- mean(df.pred.spl$rr.spline[eval.period])\nrr.trend.eval  &lt;- sum(df.pred.pre$J12_J18_prim[eval.period])/sum(df.pred.pre$pred.pre[eval.period])\n\n# Average decline detected from 1.\nround(c(\"ITS - Disconnected\" = rr.its.eval, \"ITS - Spline\" = rr.spline.eval,\n        \"Extrapolation\" = rr.trend.eval), digits = 2)\n\nITS - Disconnected       ITS - Spline      Extrapolation \n              0.91               0.90               0.86 \n\n# Percent decline detected.\n100*(1 - round(c(\"ITS - Disconnected\" = rr.its.eval, \"ITS - Spline\" = rr.spline.eval, \n                 \"Extrapolation\" = rr.trend.eval), digits = 2))\n\nITS - Disconnected       ITS - Spline      Extrapolation \n                 9                 10                 14 \n\n\nTwo of the modeling methods identified a decrease of approximately 10% in pneumonia deaths due to pneumococcal disease after the release of the PCV10 vaccine. The extrapolation method indicates a stronger vaccine effect; however, we expect some of this result to be spurious due to the level of random noise introduced by applying the observed values in the counterfactual ratio calculation.\n\n\n\n\n\n\nCaution\n\n\n\nDepending on the dataset, these three methods may or may not agree to a greater extent."
  },
  {
    "objectID": "Pages/Materials/Student Codespaces/bdsy-phm-individual/Week2/Time Series Analysis_Hands-On Exercise.html#references",
    "href": "Pages/Materials/Student Codespaces/bdsy-phm-individual/Week2/Time Series Analysis_Hands-On Exercise.html#references",
    "title": "Time Series Analysis",
    "section": "References",
    "text": "References\n\n\n1. Weinberger, D. M. (2022).\n\n\n2. Weinberger, D. M. et al. Data analysis workshop at ISPPD | workshop on the evaluation of vaccine impact webpage. (2022).\n\n\n3. Oliveira, L. H. D. et al. Declines in pneumonia mortality following the introduction of pneumococcal conjugate vaccines in latin american and caribbean countries. Clinical Infectious Diseases 73, 306–313 (2021).\n\n\n4. Oliveira, L. H. D. et al. Declines in pneumonia mortality following the introduction of pneumococcal conjugate vaccines in latin american and caribbean countries - supplementary table 2. Clinical Infectious Diseases 73, 306–313 (2021).\n\n\n5. icd10data.com. 2025 ICD-10-CM codes J09-J18: Influenza and pneumonia.\n\n\n6. Weinberger, D. M. et al. 1. Vaccine impact; administrative data sources and their pitfalls - YouTube. YouTube (2021).\n\n\n7. Wickham, H. et al. Create Elegant Data Visualisations Using the Grammar of Graphics • Ggplot2. (Springer-Verlag, 2016).\n\n\n8. Golden, S. Data Visualization with Ggplot2 – Book of Workshops. (2025).\n\n\n9. Ninja, N. N. Periodicity: Detecting rhythms in data - let’s data science. (2023).\n\n\n10. Ramanathan, K. et al. Assessing seasonality variation with harmonic regression: Accommodations for sharp peaks. International Journal of Environmental Research and Public Health 17, (2020).\n\n\n11. Young, P. C. Recursive Estimation and Time-Series Analysis. Recursive Estimation and Time-Series Analysis (Springer Berlin Heidelberg, 2011). doi:10.1007/978-3-642-21981-8.\n\n\n12. Weinberger, D. M. et al. 2. Interrupted time series analysis. YouTube (2021).\n\n\n13. C, D. M., A, E. P. & Vining, G. G. Introducing to Linear Regression Analysis (6th Ed.). John Wiley and Sons 642 (Wiley Series, 2012)."
  },
  {
    "objectID": "R/market basket example.html",
    "href": "R/market basket example.html",
    "title": "Market Basket Analysis Walkthrough",
    "section": "",
    "text": "Market Basket Analysis, aka affinity analysis aka association rules mining is an unsupervised machine learning technique which applies an algorithm (apriori algorithm) to identify association rules in datasets.\nWe’ll be applying this algorithm to identify associations in a dataset containing information about cases of perforative acute otitis media in children.\nWe’ll start with loading in our packages\n# NOTE: you might need to specify the source for the arules package:\n# install.packages(\"arules\", repos='http://cran.rstudio.com/')\n\nsuppressPackageStartupMessages({\n  library(\"tidyverse\")     # Collection of R packages for data science\n  library(\"knitr\")         # For dynamic report generation\n  library(\"ggplot2\")       # For creating static visualizations\n  library(\"lubridate\")     # For date and time manipulation\n  library(\"arules\")        # For mining association rules and frequent itemsets\n  library(\"arulesViz\")     # For visualizing association rules\n  library(\"plyr\")          # For data manipulation\n  library(\"RColorBrewer\")  # For color palettes\n  library(\"plotly\")        # For creating interactive web-based graphs\n  library(\"httr\")          # For downloading files from URLs\n})\n\n# Function to select \"Not In\"\n'%!in%' &lt;- function(x,y)!('%in%'(x,y))\nAnd our dataset\n# Read in the cleaned data directly from the instructor's GitHub.\nurl &lt;- \"https://raw.githubusercontent.com/ysph-dsde/bdsy-phm/refs/heads/main/Data/pAOM.csv\"\npAOM &lt;- read_csv(url)\n\nRows: 2137 Columns: 10\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (10): PtID, SmokeNum, PtDaycare, SibNum, Pre_Post_PCV13, PCV13_Serotype_...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nWe have 2137 rows representing 2137 cases of pAOM. Each row contains information about PtID- study ID of patient SmokeNum- number of smokers in household PtDayCare- did patient attend daycare SibNum- number of siblings in household NumSibDaycare- number of siblings in daycare Pre_Post_PCV13- did the case occur before or after the child could have received PCV13? (PreVacc, PostVacc_yr1-5) PCV13_Serotype_AOM- of pneumococcal pAOM cases, were they from PCV13 serotypes? (VT,NVTs) OtoPathogen- Strep_pneumo,Strep_pyogenes,Haem_inf, Morax_cat, Staph_aur, OthBact, PresViral Carriage1- otopathogens (c_Strep_pneumo,c_Strep_pyogenes,c_Haem_inf, c_Morax_cat, c_Staph_aur, c_None)found in NP carriage Carriage2- additional otopathogens found in NP carriage Carriage3- additional otopathogens found in NP carriage\nBefore using our rule mining algorithm, we need to transform data from the data frame format into transactions\n# Create a temporary file\ntemp_file &lt;- tempfile()\n\n# Download the data from GitHub and save it to the temporary file\nGET(url, write_disk(temp_file, overwrite = TRUE))\n\nResponse [https://raw.githubusercontent.com/ysph-dsde/bdsy-phm/refs/heads/main/Data/pAOM.csv]\n  Date: 2025-07-09 03:55\n  Status: 200\n  Content-Type: text/plain; charset=utf-8\n  Size: 144 kB\n&lt;ON DISK&gt;  /var/folders/9f/rwy2b8vj3m90x_s1fvx553cn5v3lr9/T//RtmphY6oAi/file1372f978d325\n\n# Read the transactions from the downloaded file\ntr &lt;- read.transactions(temp_file, format = 'basket', sep = ',')\nprint('Description of the transactions')\n\n[1] \"Description of the transactions\"\n\nsummary(tr)\n\ntransactions as itemMatrix in sparse format with\n 2138 rows (elements/itemsets/transactions) and\n 2178 columns (items) and a density of 0.003250036 \n\nmost frequent items:\n    c_None No Smokers    Daycare    OthBact  1 Sibling    (Other) \n      2008       1732       1265       1082        997       8050 \n\nelement (itemset/transaction) length distribution:\nsizes\n   5    6    7    8    9   10 \n   3   11 1947  170    6    1 \n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  5.000   7.000   7.000   7.079   7.000  10.000 \n\nincludes extended item information - examples:\n  labels\n1   0101\n2   0102\n3   0103\nLet’s see what items occur most frequently:\nitemFrequencyPlot(tr,topN=25,type=\"absolute\",col=brewer.pal(8,'Pastel2'), main=\"pAOM rules\")\na relative frequency plot\nitemFrequencyPlot(tr,topN=20,type=\"relative\",col=brewer.pal(8,'Pastel2'),main=\"Relative frequency, pAOM\")"
  },
  {
    "objectID": "R/market basket example.html#create-some-rules",
    "href": "R/market basket example.html#create-some-rules",
    "title": "Market Basket Analysis Walkthrough",
    "section": "Create some rules",
    "text": "Create some rules\nWe use the Apriori algorithm from the arules package to look for itemsets and find support for rules\nWe pass supp=0.0001 and conf=0.8 to return all the rules have a support of at least 0.1% and confidence of at least 80%.\nWe sort the rules by decreasing confidence.\nHere are the rules matching these criteria:\n\nrules &lt;- apriori(tr, parameter = list(supp=0.001, conf=0.8))\n\nApriori\n\nParameter specification:\n confidence minval smax arem  aval originalSupport maxtime support minlen\n        0.8    0.1    1 none FALSE            TRUE       5   0.001      1\n maxlen target  ext\n     10  rules TRUE\n\nAlgorithmic control:\n filter tree heap memopt load sort verbose\n    0.1 TRUE TRUE  FALSE TRUE    2    TRUE\n\nAbsolute minimum support count: 2 \n\nset item appearances ...[0 item(s)] done [0.00s].\nset transactions ...[2178 item(s), 2138 transaction(s)] done [0.00s].\nsorting and recoding items ... [31 item(s)] done [0.00s].\ncreating transaction tree ... done [0.00s].\nchecking subsets of size 1 2 3 4 5 6 7 done [0.00s].\nwriting ... [4093 rule(s)] done [0.00s].\ncreating S4 object  ... done [0.00s].\n\nrules &lt;- sort(rules, by='confidence', decreasing = TRUE)\nsummary(rules)\n\nset of 4093 rules\n\nrule length distribution (lhs + rhs):sizes\n   1    2    3    4    5    6    7 \n   2   50  433 1366 1576  621   45 \n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   1.00    4.00    5.00    4.59    5.00    7.00 \n\nsummary of quality measures:\n    support           confidence        coverage             lift         \n Min.   :0.001403   Min.   :0.8000   Min.   :0.001403   Min.   :  0.8518  \n 1st Qu.:0.001871   1st Qu.:0.8913   1st Qu.:0.001871   1st Qu.:  1.0287  \n Median :0.003274   Median :1.0000   Median :0.003274   Median :  1.0647  \n Mean   :0.013357   Mean   :0.9468   Mean   :0.014793   Mean   :  3.3443  \n 3rd Qu.:0.008887   3rd Qu.:1.0000   3rd Qu.:0.009354   3rd Qu.:  1.4648  \n Max.   :0.939195   Max.   :1.0000   Max.   :1.000000   Max.   :106.9000  \n     count        \n Min.   :   3.00  \n 1st Qu.:   4.00  \n Median :   7.00  \n Mean   :  28.56  \n 3rd Qu.:  19.00  \n Max.   :2008.00  \n\nmining info:\n data ntransactions support confidence\n   tr          2138   0.001        0.8\n                                                           call\n apriori(data = tr, parameter = list(supp = 0.001, conf = 0.8))\n\n\nWe have 4093 rules, most are 4 or 5 items long. Let’s inspect the top 10 rules according to these parameters (supp 0.001, conf =0.8).\n\ninspect(rules[1:10])\n\n     lhs                        rhs            support     confidence\n[1]  {3 Smoker}              =&gt; {c_None}       0.001870907 1         \n[2]  {c_Haem_inf}            =&gt; {PreVacc}      0.007015903 1         \n[3]  {c_Strep_pneumo}        =&gt; {PreVacc}      0.009354537 1         \n[4]  {c_Morax_cat}           =&gt; {PreVacc}      0.022450889 1         \n[5]  {NVT}                   =&gt; {Strep_pneumo} 0.027595884 1         \n[6]  {VT}                    =&gt; {Strep_pneumo} 0.047708138 1         \n[7]  {PostVacc_yr1}          =&gt; {c_None}       0.062675398 1         \n[8]  {3 Smoker, 3+ Sibling}  =&gt; {c_None}       0.001403181 1         \n[9]  {Morax_cat, No Sibings} =&gt; {No Smokers}   0.001403181 1         \n[10] {Morax_cat, No Sibings} =&gt; {c_None}       0.001403181 1         \n     coverage    lift      count\n[1]  0.001870907  1.064741   4  \n[2]  0.007015903  2.679198  15  \n[3]  0.009354537  2.679198  20  \n[4]  0.022450889  2.679198  48  \n[5]  0.027595884 13.279503  59  \n[6]  0.047708138 13.279503 102  \n[7]  0.062675398  1.064741 134  \n[8]  0.001403181  1.064741   3  \n[9]  0.001403181  1.234411   3  \n[10] 0.001403181  1.064741   3  \n\n\nAnd plot these top 10 rules, or 20, or 50.\n\ntopRules &lt;- rules[1:10]\nplot(rules)\n\nTo reduce overplotting, jitter is added! Use jitter = 0 to prevent jitter.\n\n\n\n\n\n\n\n\n\nnow with more colors\n\nplot(rules, method = \"two-key plot\")\n\nTo reduce overplotting, jitter is added! Use jitter = 0 to prevent jitter.\n\n\n\n\n\n\n\n\n\nnow how about a network graph?\n\nplot(topRules, method=\"graph\")\n\n\n\n\n\n\n\n\nNow let’s see an interactive map:\n\nplot(topRules, method=\"graph\", engine = 'interactive')\n\n\nplot(topRules, method = \"grouped\")\n\n\n\n\n\n\n\n\n\nplot(topRules, method = \"graph\",  engine = \"htmlwidget\")\n\n\n\n\n\nnow a matrix plot\n\nplot(topRules, method = \"matrix\", engine = \"3d\", measure = \"lift\")\n\nItemsets in Antecedent (LHS)\n[1] \"{NVT}\"                  \"{VT}\"                   \"{c_Haem_inf}\"          \n[4] \"{c_Strep_pneumo}\"       \"{c_Morax_cat}\"          \"{Morax_cat,No Sibings}\"\n[7] \"{3 Smoker}\"             \"{PostVacc_yr1}\"         \"{3 Smoker,3+ Sibling}\" \nItemsets in Consequent (RHS)\n[1] \"{c_None}\"       \"{No Smokers}\"   \"{PreVacc}\"      \"{Strep_pneumo}\"\n\n\n\n\n\n\n\n\n\nmoving back a bit, let’s check a different set of rules:\n\nrules_b &lt;- apriori(tr, parameter = list(supp=0.01, conf=1.0))\n\nApriori\n\nParameter specification:\n confidence minval smax arem  aval originalSupport maxtime support minlen\n          1    0.1    1 none FALSE            TRUE       5    0.01      1\n maxlen target  ext\n     10  rules TRUE\n\nAlgorithmic control:\n filter tree heap memopt load sort verbose\n    0.1 TRUE TRUE  FALSE TRUE    2    TRUE\n\nAbsolute minimum support count: 21 \n\nset item appearances ...[0 item(s)] done [0.00s].\nset transactions ...[2178 item(s), 2138 transaction(s)] done [0.00s].\nsorting and recoding items ... [26 item(s)] done [0.00s].\ncreating transaction tree ... done [0.00s].\nchecking subsets of size 1 2 3 4 5 6 done [0.00s].\nwriting ... [115 rule(s)] done [0.00s].\ncreating S4 object  ... done [0.00s].\n\nrules_b &lt;- sort(rules_b, by='confidence', decreasing = TRUE)\nsummary(rules_b)\n\nset of 115 rules\n\nrule length distribution (lhs + rhs):sizes\n 2  3  4  5  6 \n 4 30 51 26  4 \n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  2.000   3.000   4.000   3.965   5.000   6.000 \n\nsummary of quality measures:\n    support          confidence    coverage            lift       \n Min.   :0.01029   Min.   :1    Min.   :0.01029   Min.   : 1.065  \n 1st Qu.:0.01169   1st Qu.:1    1st Qu.:0.01169   1st Qu.: 1.065  \n Median :0.01403   Median :1    Median :0.01403   Median : 1.065  \n Mean   :0.01759   Mean   :1    Mean   :0.01759   Mean   : 6.355  \n 3rd Qu.:0.02035   3rd Qu.:1    3rd Qu.:0.02035   3rd Qu.:13.280  \n Max.   :0.06268   Max.   :1    Max.   :0.06268   Max.   :13.280  \n     count       \n Min.   : 22.00  \n 1st Qu.: 25.00  \n Median : 30.00  \n Mean   : 37.61  \n 3rd Qu.: 43.50  \n Max.   :134.00  \n\nmining info:\n data ntransactions support confidence\n   tr          2138    0.01          1\n                                                        call\n apriori(data = tr, parameter = list(supp = 0.01, conf = 1))\n\n\nthese more stringent criteria mean that we’re down to 115 rules to sort through.\n\ninspect(rules_b[1:10])\n\n     lhs                          rhs            support    confidence\n[1]  {c_Morax_cat}             =&gt; {PreVacc}      0.02245089 1         \n[2]  {NVT}                     =&gt; {Strep_pneumo} 0.02759588 1         \n[3]  {VT}                      =&gt; {Strep_pneumo} 0.04770814 1         \n[4]  {PostVacc_yr1}            =&gt; {c_None}       0.06267540 1         \n[5]  {c_Morax_cat, No Daycare} =&gt; {PreVacc}      0.01169317 1         \n[6]  {c_Morax_cat, OthBact}    =&gt; {PreVacc}      0.01216090 1         \n[7]  {c_Morax_cat, Daycare}    =&gt; {PreVacc}      0.01028999 1         \n[8]  {c_Morax_cat, No Smokers} =&gt; {PreVacc}      0.01917680 1         \n[9]  {NVT, PreVacc}            =&gt; {Strep_pneumo} 0.01075772 1         \n[10] {No Daycare, NVT}         =&gt; {Strep_pneumo} 0.01169317 1         \n     coverage   lift      count\n[1]  0.02245089  2.679198  48  \n[2]  0.02759588 13.279503  59  \n[3]  0.04770814 13.279503 102  \n[4]  0.06267540  1.064741 134  \n[5]  0.01169317  2.679198  25  \n[6]  0.01216090  2.679198  26  \n[7]  0.01028999  2.679198  22  \n[8]  0.01917680  2.679198  41  \n[9]  0.01075772 13.279503  23  \n[10] 0.01169317 13.279503  25  \n\n\ntry to look for only for rules associated with Strep_pneumo\n\npneumo.rules&lt;-sort(subset(rules_b, subset = rhs %in% \"Strep_pneumo\"))\n\ninspect(pneumo.rules[1:10])\n\n     lhs                          rhs            support    confidence\n[1]  {VT}                      =&gt; {Strep_pneumo} 0.04770814 1         \n[2]  {c_None, VT}              =&gt; {Strep_pneumo} 0.04443405 1         \n[3]  {No Smokers, VT}          =&gt; {Strep_pneumo} 0.03648269 1         \n[4]  {c_None, No Smokers, VT}  =&gt; {Strep_pneumo} 0.03367633 1         \n[5]  {Daycare, VT}             =&gt; {Strep_pneumo} 0.02946679 1         \n[6]  {NVT}                     =&gt; {Strep_pneumo} 0.02759588 1         \n[7]  {c_None, NVT}             =&gt; {Strep_pneumo} 0.02666043 1         \n[8]  {c_None, Daycare, VT}     =&gt; {Strep_pneumo} 0.02666043 1         \n[9]  {PreVacc, VT}             =&gt; {Strep_pneumo} 0.02385407 1         \n[10] {Daycare, No Smokers, VT} =&gt; {Strep_pneumo} 0.02338634 1         \n     coverage   lift    count\n[1]  0.04770814 13.2795 102  \n[2]  0.04443405 13.2795  95  \n[3]  0.03648269 13.2795  78  \n[4]  0.03367633 13.2795  72  \n[5]  0.02946679 13.2795  63  \n[6]  0.02759588 13.2795  59  \n[7]  0.02666043 13.2795  57  \n[8]  0.02666043 13.2795  57  \n[9]  0.02385407 13.2795  51  \n[10] 0.02338634 13.2795  50  \n\n\nnow let’s plot the pneumo rules:\n\nplot(pneumo.rules, measure = c(\"support\", \"confidence\"), shading = \"lift\")\n\nTo reduce overplotting, jitter is added! Use jitter = 0 to prevent jitter.\n\n\n\n\n\n\n\n\n\n\nplot(pneumo.rules[1:10], method=\"graph\", engine = 'interactive')\n\nplot(pneumo.rules[1:10], method= \"paracoord\", control=list(reorder=TRUE))\n\n\n\n\n\n\n\n\nreference: R and Data Mining there are other cool market basket analysis visualizations here:"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome!",
    "section": "",
    "text": "Welcome to our project on modeling pneumococcal disease, a leading cause of death in children under 5 worldwide. Public health is an interdisciplinary field that measures and tracks disease trends and promotes health through policy and treatment interventions. We will investigate the impact of pneumococcal vaccines and the phenomenon of serotype replacement, where uncommon serotypes variations not targeted by the therapy flourish and take over as the predominant causes of disease after vaccination. Using data from infectious disease surveillance systems and statistical methods such as time series analysis, hierarchical modeling, and serotype clustering and association, we aim to understand serotype replacement and its implications. Our analyses will be conducted in R.\nOur objectives for this course will be to:\nWe will accomplish these objectives using data from national and multinational infectious disease surveillance systems, and the following statistical methods:\nFull project statement"
  },
  {
    "objectID": "index.html#purpose-of-this-site",
    "href": "index.html#purpose-of-this-site",
    "title": "Welcome!",
    "section": "Purpose of This Site",
    "text": "Purpose of This Site\nSharing code-based materials poses a challenging technical problem that Canvas is not well suited to handle. Therefore, we have created an instructor GitHub webpage where we can effectively create, communicate, and distribute course materials. This platform provides students with the most current information in an easily accessible format and allows us, the instructors, to distribute materials more efficiently.\nOn this site, you will find:\n\nA schedule of important project goals and assignment dates.\nWebpages with full weekly hands-on demonstrations of concepts.\nWeekly codebases for students to apply in their own projects, reflecting that week’s concepts.\nLinks to the subgroup dataset zip files, the associated paper, and brief descriptions of them.\nLinks to other course materials, including blank slide decks, poster layouts, slides, and more.\n\n\n\n    About the Instructors\n\n\n\n\n\n  \n  People Tiles\n  \n  \n\n\n  \n    \n      \n        \n          \n            \n          \n          \n            Stephanie Perniciaro, PhD, MPH\n            Associate Research Scientist in Epidemiology (Microbial Diseases)\n\n            \n              \n                \n                  \n                \n                \n                \n                  \n                \n                \n                  \n                \n                \n                  \n                \n              \n            \n            The surveillance and epidemiology of vaccine-preventable respiratory diseases, especially pneumococcal disease, is my passion. I want to determine how to efficiently and effectively protect populations from pneumococcal disease, both invasive (bacteremia, meningitis) and non-invasive (otitis media, non-bacteremic pneumonia). I'm interested in respiratory disease surveillance, serotype replacement, antibiotic resistance, vulnerable populations, vaccine schedules, vaccine advocacy, and overcoming vaccine hesitancy.\n\n            \n              \n                Email: stephanie.perniciaro@yale.edu\n              \n              \n            \n            \n          \n        \n      \n        \n          \n            \n          \n          \n            Shelby Golden, MS\n            Data Scientist I\n\n            \n              \n                \n                  \n                \n                \n                  \n                \n                \n                  \n                \n                \n                  \n                \n                \n                  \n                \n              \n            \n            With my work I aim to contribute to impactful projects that advance scientific understanding by utilizing strong analytical skills and a deep commitment to reliable data-driven decision-making. \n\n\nMy background is in computational mathematics, molecular biology, and biochemistry. I hold a Master of Science in Applied Computational Mathematics from Johns Hopkins University and dual Bachelor of Science degrees in Molecular, Cellular, Developmental Biology and Biochemistry, alongside a minor in Engineering in Applied Mathematics from the University of Colorado at Boulder.\n\n            \n              \n                Email: shelby.golden@yale.edu\n              \n              \n            \n            \n          \n        \n      \n    \n  \n\n\n\nNo matching items"
  },
  {
    "objectID": "Pages/Materials/Student Codespaces/Week 4 Code.html",
    "href": "Pages/Materials/Student Codespaces/Week 4 Code.html",
    "title": "Market Basket Analysis Walkthrough",
    "section": "",
    "text": "Market Basket Analysis, aka affinity analysis aka association rules mining is an unsupervised machine learning technique which applies an algorithm (apriori algorithm) to identify association rules in datasets.\nWe’ll be applying this algorithm to identify associations in a dataset containing information about cases of perforative acute otitis media in children.\nWe’ll start with loading in our packages\n# NOTE: you might need to specify the source for the arules package:\n# install.packages(\"arules\", repos='http://cran.rstudio.com/')\n\nsuppressPackageStartupMessages({\n  library(\"tidyverse\")     # Collection of R packages for data science\n  library(\"knitr\")         # For dynamic report generation\n  library(\"ggplot2\")       # For creating static visualizations\n  library(\"lubridate\")     # For date and time manipulation\n  library(\"arules\")        # For mining association rules and frequent itemsets\n  library(\"arulesViz\")     # For visualizing association rules\n  library(\"plyr\")          # For data manipulation\n  library(\"RColorBrewer\")  # For color palettes\n  library(\"plotly\")        # For creating interactive web-based graphs\n  library(\"httr\")          # For downloading files from URLs\n})\n\n# Function to select \"Not In\"\n'%!in%' &lt;- function(x,y)!('%in%'(x,y))\nAnd our dataset\n# Read in the cleaned data directly from the instructor's GitHub.\nurl &lt;- \"https://raw.githubusercontent.com/ysph-dsde/bdsy-phm/refs/heads/main/Data/pAOM.csv\"\npAOM &lt;- read_csv(url)\n\nRows: 2137 Columns: 10\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (10): PtID, SmokeNum, PtDaycare, SibNum, Pre_Post_PCV13, PCV13_Serotype_...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nWe have 2137 rows representing 2137 cases of pAOM. Each row contains information about PtID- study ID of patient SmokeNum- number of smokers in household PtDayCare- did patient attend daycare SibNum- number of siblings in household NumSibDaycare- number of siblings in daycare Pre_Post_PCV13- did the case occur before or after the child could have received PCV13? (PreVacc, PostVacc_yr1-5) PCV13_Serotype_AOM- of pneumococcal pAOM cases, were they from PCV13 serotypes? (VT,NVTs) OtoPathogen- Strep_pneumo,Strep_pyogenes,Haem_inf, Morax_cat, Staph_aur, OthBact, PresViral Carriage1- otopathogens (c_Strep_pneumo,c_Strep_pyogenes,c_Haem_inf, c_Morax_cat, c_Staph_aur, c_None)found in NP carriage Carriage2- additional otopathogens found in NP carriage Carriage3- additional otopathogens found in NP carriage\nBefore using our rule mining algorithm, we need to transform data from the data frame format into transactions\n# Create a temporary file\ntemp_file &lt;- tempfile()\n\n# Download the data from GitHub and save it to the temporary file\nGET(url, write_disk(temp_file, overwrite = TRUE))\n\nResponse [https://raw.githubusercontent.com/ysph-dsde/bdsy-phm/refs/heads/main/Data/pAOM.csv]\n  Date: 2025-07-09 03:56\n  Status: 200\n  Content-Type: text/plain; charset=utf-8\n  Size: 144 kB\n&lt;ON DISK&gt;  /var/folders/9f/rwy2b8vj3m90x_s1fvx553cn5v3lr9/T//RtmpFKV489/file138ee99f5c5\n\n# Read the transactions from the downloaded file\ntr &lt;- read.transactions(temp_file, format = 'basket', sep = ',')\nprint('Description of the transactions')\n\n[1] \"Description of the transactions\"\n\nsummary(tr)\n\ntransactions as itemMatrix in sparse format with\n 2138 rows (elements/itemsets/transactions) and\n 2178 columns (items) and a density of 0.003250036 \n\nmost frequent items:\n    c_None No Smokers    Daycare    OthBact  1 Sibling    (Other) \n      2008       1732       1265       1082        997       8050 \n\nelement (itemset/transaction) length distribution:\nsizes\n   5    6    7    8    9   10 \n   3   11 1947  170    6    1 \n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  5.000   7.000   7.000   7.079   7.000  10.000 \n\nincludes extended item information - examples:\n  labels\n1   0101\n2   0102\n3   0103\nLet’s see what items occur most frequently:\nitemFrequencyPlot(tr,topN=25,type=\"absolute\",col=brewer.pal(8,'Pastel2'), main=\"pAOM rules\")\na relative frequency plot\nitemFrequencyPlot(tr,topN=20,type=\"relative\",col=brewer.pal(8,'Pastel2'),main=\"Relative frequency, pAOM\")"
  },
  {
    "objectID": "Pages/Materials/Student Codespaces/Week 4 Code.html#create-some-rules",
    "href": "Pages/Materials/Student Codespaces/Week 4 Code.html#create-some-rules",
    "title": "Market Basket Analysis Walkthrough",
    "section": "Create some rules",
    "text": "Create some rules\nWe use the Apriori algorithm from the arules package to look for itemsets and find support for rules\nWe pass supp=0.0001 and conf=0.8 to return all the rules have a support of at least 0.1% and confidence of at least 80%.\nWe sort the rules by decreasing confidence.\nHere are the rules matching these criteria:\n\nrules &lt;- apriori(tr, parameter = list(supp=0.001, conf=0.8))\n\nApriori\n\nParameter specification:\n confidence minval smax arem  aval originalSupport maxtime support minlen\n        0.8    0.1    1 none FALSE            TRUE       5   0.001      1\n maxlen target  ext\n     10  rules TRUE\n\nAlgorithmic control:\n filter tree heap memopt load sort verbose\n    0.1 TRUE TRUE  FALSE TRUE    2    TRUE\n\nAbsolute minimum support count: 2 \n\nset item appearances ...[0 item(s)] done [0.00s].\nset transactions ...[2178 item(s), 2138 transaction(s)] done [0.00s].\nsorting and recoding items ... [31 item(s)] done [0.00s].\ncreating transaction tree ... done [0.00s].\nchecking subsets of size 1 2 3 4 5 6 7 done [0.00s].\nwriting ... [4093 rule(s)] done [0.00s].\ncreating S4 object  ... done [0.00s].\n\nrules &lt;- sort(rules, by='confidence', decreasing = TRUE)\nsummary(rules)\n\nset of 4093 rules\n\nrule length distribution (lhs + rhs):sizes\n   1    2    3    4    5    6    7 \n   2   50  433 1366 1576  621   45 \n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   1.00    4.00    5.00    4.59    5.00    7.00 \n\nsummary of quality measures:\n    support           confidence        coverage             lift         \n Min.   :0.001403   Min.   :0.8000   Min.   :0.001403   Min.   :  0.8518  \n 1st Qu.:0.001871   1st Qu.:0.8913   1st Qu.:0.001871   1st Qu.:  1.0287  \n Median :0.003274   Median :1.0000   Median :0.003274   Median :  1.0647  \n Mean   :0.013357   Mean   :0.9468   Mean   :0.014793   Mean   :  3.3443  \n 3rd Qu.:0.008887   3rd Qu.:1.0000   3rd Qu.:0.009354   3rd Qu.:  1.4648  \n Max.   :0.939195   Max.   :1.0000   Max.   :1.000000   Max.   :106.9000  \n     count        \n Min.   :   3.00  \n 1st Qu.:   4.00  \n Median :   7.00  \n Mean   :  28.56  \n 3rd Qu.:  19.00  \n Max.   :2008.00  \n\nmining info:\n data ntransactions support confidence\n   tr          2138   0.001        0.8\n                                                           call\n apriori(data = tr, parameter = list(supp = 0.001, conf = 0.8))\n\n\nWe have 4093 rules, most are 4 or 5 items long. Let’s inspect the top 10 rules according to these parameters (supp 0.001, conf =0.8).\n\ninspect(rules[1:10])\n\n     lhs                        rhs            support     confidence\n[1]  {3 Smoker}              =&gt; {c_None}       0.001870907 1         \n[2]  {c_Haem_inf}            =&gt; {PreVacc}      0.007015903 1         \n[3]  {c_Strep_pneumo}        =&gt; {PreVacc}      0.009354537 1         \n[4]  {c_Morax_cat}           =&gt; {PreVacc}      0.022450889 1         \n[5]  {NVT}                   =&gt; {Strep_pneumo} 0.027595884 1         \n[6]  {VT}                    =&gt; {Strep_pneumo} 0.047708138 1         \n[7]  {PostVacc_yr1}          =&gt; {c_None}       0.062675398 1         \n[8]  {3 Smoker, 3+ Sibling}  =&gt; {c_None}       0.001403181 1         \n[9]  {Morax_cat, No Sibings} =&gt; {No Smokers}   0.001403181 1         \n[10] {Morax_cat, No Sibings} =&gt; {c_None}       0.001403181 1         \n     coverage    lift      count\n[1]  0.001870907  1.064741   4  \n[2]  0.007015903  2.679198  15  \n[3]  0.009354537  2.679198  20  \n[4]  0.022450889  2.679198  48  \n[5]  0.027595884 13.279503  59  \n[6]  0.047708138 13.279503 102  \n[7]  0.062675398  1.064741 134  \n[8]  0.001403181  1.064741   3  \n[9]  0.001403181  1.234411   3  \n[10] 0.001403181  1.064741   3  \n\n\nAnd plot these top 10 rules, or 20, or 50.\n\ntopRules &lt;- rules[1:10]\nplot(rules)\n\nTo reduce overplotting, jitter is added! Use jitter = 0 to prevent jitter.\n\n\n\n\n\n\n\n\n\nnow with more colors\n\nplot(rules, method = \"two-key plot\")\n\nTo reduce overplotting, jitter is added! Use jitter = 0 to prevent jitter.\n\n\n\n\n\n\n\n\n\nnow how about a network graph?\n\nplot(topRules, method=\"graph\")\n\n\n\n\n\n\n\n\nNow let’s see an interactive map:\n\nplot(topRules, method=\"graph\", engine = 'interactive')\n\n\nplot(topRules, method = \"grouped\")\n\n\n\n\n\n\n\n\n\nplot(topRules, method = \"graph\",  engine = \"htmlwidget\")\n\n\n\n\n\nnow a matrix plot\n\nplot(topRules, method = \"matrix\", engine = \"3d\", measure = \"lift\")\n\nItemsets in Antecedent (LHS)\n[1] \"{NVT}\"                  \"{VT}\"                   \"{c_Haem_inf}\"          \n[4] \"{c_Strep_pneumo}\"       \"{c_Morax_cat}\"          \"{Morax_cat,No Sibings}\"\n[7] \"{3 Smoker}\"             \"{PostVacc_yr1}\"         \"{3 Smoker,3+ Sibling}\" \nItemsets in Consequent (RHS)\n[1] \"{c_None}\"       \"{No Smokers}\"   \"{PreVacc}\"      \"{Strep_pneumo}\"\n\n\n\n\n\n\n\n\n\nmoving back a bit, let’s check a different set of rules:\n\nrules_b &lt;- apriori(tr, parameter = list(supp=0.01, conf=1.0))\n\nApriori\n\nParameter specification:\n confidence minval smax arem  aval originalSupport maxtime support minlen\n          1    0.1    1 none FALSE            TRUE       5    0.01      1\n maxlen target  ext\n     10  rules TRUE\n\nAlgorithmic control:\n filter tree heap memopt load sort verbose\n    0.1 TRUE TRUE  FALSE TRUE    2    TRUE\n\nAbsolute minimum support count: 21 \n\nset item appearances ...[0 item(s)] done [0.00s].\nset transactions ...[2178 item(s), 2138 transaction(s)] done [0.00s].\nsorting and recoding items ... [26 item(s)] done [0.00s].\ncreating transaction tree ... done [0.00s].\nchecking subsets of size 1 2 3 4 5 6 done [0.00s].\nwriting ... [115 rule(s)] done [0.00s].\ncreating S4 object  ... done [0.00s].\n\nrules_b &lt;- sort(rules_b, by='confidence', decreasing = TRUE)\nsummary(rules_b)\n\nset of 115 rules\n\nrule length distribution (lhs + rhs):sizes\n 2  3  4  5  6 \n 4 30 51 26  4 \n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  2.000   3.000   4.000   3.965   5.000   6.000 \n\nsummary of quality measures:\n    support          confidence    coverage            lift       \n Min.   :0.01029   Min.   :1    Min.   :0.01029   Min.   : 1.065  \n 1st Qu.:0.01169   1st Qu.:1    1st Qu.:0.01169   1st Qu.: 1.065  \n Median :0.01403   Median :1    Median :0.01403   Median : 1.065  \n Mean   :0.01759   Mean   :1    Mean   :0.01759   Mean   : 6.355  \n 3rd Qu.:0.02035   3rd Qu.:1    3rd Qu.:0.02035   3rd Qu.:13.280  \n Max.   :0.06268   Max.   :1    Max.   :0.06268   Max.   :13.280  \n     count       \n Min.   : 22.00  \n 1st Qu.: 25.00  \n Median : 30.00  \n Mean   : 37.61  \n 3rd Qu.: 43.50  \n Max.   :134.00  \n\nmining info:\n data ntransactions support confidence\n   tr          2138    0.01          1\n                                                        call\n apriori(data = tr, parameter = list(supp = 0.01, conf = 1))\n\n\nthese more stringent criteria mean that we’re down to 115 rules to sort through.\n\ninspect(rules_b[1:10])\n\n     lhs                          rhs            support    confidence\n[1]  {c_Morax_cat}             =&gt; {PreVacc}      0.02245089 1         \n[2]  {NVT}                     =&gt; {Strep_pneumo} 0.02759588 1         \n[3]  {VT}                      =&gt; {Strep_pneumo} 0.04770814 1         \n[4]  {PostVacc_yr1}            =&gt; {c_None}       0.06267540 1         \n[5]  {c_Morax_cat, No Daycare} =&gt; {PreVacc}      0.01169317 1         \n[6]  {c_Morax_cat, OthBact}    =&gt; {PreVacc}      0.01216090 1         \n[7]  {c_Morax_cat, Daycare}    =&gt; {PreVacc}      0.01028999 1         \n[8]  {c_Morax_cat, No Smokers} =&gt; {PreVacc}      0.01917680 1         \n[9]  {NVT, PreVacc}            =&gt; {Strep_pneumo} 0.01075772 1         \n[10] {No Daycare, NVT}         =&gt; {Strep_pneumo} 0.01169317 1         \n     coverage   lift      count\n[1]  0.02245089  2.679198  48  \n[2]  0.02759588 13.279503  59  \n[3]  0.04770814 13.279503 102  \n[4]  0.06267540  1.064741 134  \n[5]  0.01169317  2.679198  25  \n[6]  0.01216090  2.679198  26  \n[7]  0.01028999  2.679198  22  \n[8]  0.01917680  2.679198  41  \n[9]  0.01075772 13.279503  23  \n[10] 0.01169317 13.279503  25  \n\n\ntry to look for only for rules associated with Strep_pneumo\n\npneumo.rules&lt;-sort(subset(rules_b, subset = rhs %in% \"Strep_pneumo\"))\n\ninspect(pneumo.rules[1:10])\n\n     lhs                          rhs            support    confidence\n[1]  {VT}                      =&gt; {Strep_pneumo} 0.04770814 1         \n[2]  {c_None, VT}              =&gt; {Strep_pneumo} 0.04443405 1         \n[3]  {No Smokers, VT}          =&gt; {Strep_pneumo} 0.03648269 1         \n[4]  {c_None, No Smokers, VT}  =&gt; {Strep_pneumo} 0.03367633 1         \n[5]  {Daycare, VT}             =&gt; {Strep_pneumo} 0.02946679 1         \n[6]  {NVT}                     =&gt; {Strep_pneumo} 0.02759588 1         \n[7]  {c_None, NVT}             =&gt; {Strep_pneumo} 0.02666043 1         \n[8]  {c_None, Daycare, VT}     =&gt; {Strep_pneumo} 0.02666043 1         \n[9]  {PreVacc, VT}             =&gt; {Strep_pneumo} 0.02385407 1         \n[10] {Daycare, No Smokers, VT} =&gt; {Strep_pneumo} 0.02338634 1         \n     coverage   lift    count\n[1]  0.04770814 13.2795 102  \n[2]  0.04443405 13.2795  95  \n[3]  0.03648269 13.2795  78  \n[4]  0.03367633 13.2795  72  \n[5]  0.02946679 13.2795  63  \n[6]  0.02759588 13.2795  59  \n[7]  0.02666043 13.2795  57  \n[8]  0.02666043 13.2795  57  \n[9]  0.02385407 13.2795  51  \n[10] 0.02338634 13.2795  50  \n\n\nnow let’s plot the pneumo rules:\n\nplot(pneumo.rules, measure = c(\"support\", \"confidence\"), shading = \"lift\")\n\nTo reduce overplotting, jitter is added! Use jitter = 0 to prevent jitter.\n\n\n\n\n\n\n\n\n\n\nplot(pneumo.rules[1:10], method=\"graph\", engine = 'interactive')\n\nplot(pneumo.rules[1:10], method= \"paracoord\", control=list(reorder=TRUE))\n\n\n\n\n\n\n\n\nreference: R and Data Mining there are other cool market basket analysis visualizations here:"
  },
  {
    "objectID": "Pages/Week2/Time Series Analysis_Hands-On Exercise.html",
    "href": "Pages/Week2/Time Series Analysis_Hands-On Exercise.html",
    "title": "Time Series Analysis",
    "section": "",
    "text": "This workshop was developed by Dr. Dan Weinberger from the Yale School of Public Health (YSPH) for the 12^{th} International Symposium on Pneumococci and Pneumococcal Diseases (ISPPD-12) hosted in Toronto, Canada in June 2022. Here, we have adapted his workshop session #1, which compares different methods for fitting a time series dataset 1. Students are encouraged to explore the workshop content that Dr. Weinberger put together to learn more about the subject:\n\nGitHub: Find the code, original data, and slides 1.\nWorkshop Webpage: Find a longer description of the workshop in the format of a mini course with links to video presentations of select sections - Workshop on the evaluation of vaccine impact 2.\n\nThe data used here is a subset of the complete set used in Oliveira, L. H. D. et al. 2020 3. In their paper, national-level mortality data spanning from 2000 to 2016 for children under 5 years of age was compiled and standardized by national mortality registers in 10 Latin American and Caribbean countries. The primary cause of death was classified using the International Classification of Diseases, Tenth Revision (ICD-10) codes (Influenza and pneumonia J09-J18 or Oliveira, L. H. D. et al. 2020 Supplementary Table 2) 4,5. The goal of the paper was to evaluate the efficacy of pneumococcal conjugate vaccines (PCVs) in children who received any part of the standard-of-care vaccination series.\nToday, we are going to examine the efficacy of pneumococcal conjugate vaccines (PCVs) in children aged 2 to 59 months in Ecuador. The standard-of-care vaccination series involves a primary series of either two doses (administered at 2 and 4 months of age) or three doses (administered at 2, 4, and 6 months of age), with the option of an additional booster dose (administered at 12-18 months of age). This dataset does not differentiate between patients who received varying levels of inoculation, and instead reports the data if any PCV was delivered."
  },
  {
    "objectID": "Pages/Week2/Time Series Analysis_Hands-On Exercise.html#introduction",
    "href": "Pages/Week2/Time Series Analysis_Hands-On Exercise.html#introduction",
    "title": "Time Series Analysis",
    "section": "",
    "text": "This workshop was developed by Dr. Dan Weinberger from the Yale School of Public Health (YSPH) for the 12^{th} International Symposium on Pneumococci and Pneumococcal Diseases (ISPPD-12) hosted in Toronto, Canada in June 2022. Here, we have adapted his workshop session #1, which compares different methods for fitting a time series dataset 1. Students are encouraged to explore the workshop content that Dr. Weinberger put together to learn more about the subject:\n\nGitHub: Find the code, original data, and slides 1.\nWorkshop Webpage: Find a longer description of the workshop in the format of a mini course with links to video presentations of select sections - Workshop on the evaluation of vaccine impact 2.\n\nThe data used here is a subset of the complete set used in Oliveira, L. H. D. et al. 2020 3. In their paper, national-level mortality data spanning from 2000 to 2016 for children under 5 years of age was compiled and standardized by national mortality registers in 10 Latin American and Caribbean countries. The primary cause of death was classified using the International Classification of Diseases, Tenth Revision (ICD-10) codes (Influenza and pneumonia J09-J18 or Oliveira, L. H. D. et al. 2020 Supplementary Table 2) 4,5. The goal of the paper was to evaluate the efficacy of pneumococcal conjugate vaccines (PCVs) in children who received any part of the standard-of-care vaccination series.\nToday, we are going to examine the efficacy of pneumococcal conjugate vaccines (PCVs) in children aged 2 to 59 months in Ecuador. The standard-of-care vaccination series involves a primary series of either two doses (administered at 2 and 4 months of age) or three doses (administered at 2, 4, and 6 months of age), with the option of an additional booster dose (administered at 12-18 months of age). This dataset does not differentiate between patients who received varying levels of inoculation, and instead reports the data if any PCV was delivered."
  },
  {
    "objectID": "Pages/Week2/Time Series Analysis_Hands-On Exercise.html#preamble",
    "href": "Pages/Week2/Time Series Analysis_Hands-On Exercise.html#preamble",
    "title": "Time Series Analysis",
    "section": "Preamble",
    "text": "Preamble\nThis section is drawn from Dr. Weinberger’s recorded presentation What is ‘vaccine impact’?; and Administrative data: challenges and opportunities for evaluation studies 6.\nStatistical inference can be reduced to quantitatively answering three types of questions:\n\nCausation: Was there a change, and can we identify what caused the change?\nPrediction: What do we expect to see in the future?\nCertainty: How reliable are the answers to the first two questions?\n\nA biotechnologist developing a vaccine is primarily concerned with proving that there was a favorable change, such as mitigating the disease process in an individual. In contrast, a public health analyst aims to determine the overall impact of the vaccine on the population. For example, an effective vaccine not only protects an individual but also attenuates transmission within their immediate social circle.\nThis requires contextualizing the vaccine’s impact as a combination of direct and indirect effects. It is important to identify and control for unexplained linear and non-linear trends unrelated to the introduction of a vaccine to a population. For example, were there changes in overall population health at the same time or changes to diagnostic methods?\nWe can appreciate that this is a Sisyphean task, but one that can be addressed with time series analysis. Keep in mind that the methods discussed here are most applicable to endemic diseases that are consistently present in the population, existing at a relatively stable and predictable level. The core reason is that these methods require us to have an idea of what we would expect to have seen had the vaccine not been introduced, allowing us to posit a realistic counterfactual for model evaluation.\nOur study is framed by PICO. In black text are the general definitions for the acronym and in red text is the application to our example:\n\nPopulation: The target population where the impact of a vaccine is to measured. Children 2-59 months old.\nIntervention: The date and timeframe for a vaccine intervention. Introduction of PCV10 to the Brazilian national immunization program.\nComparator: Diseases or groups of diseases to compare against the intervention, which are not expected to be impacted by the intervention itself. Counterfactual to demonstrate what would have happened if the vaccine had not been introduced, compared to the factual scenario.\nOutcome: The condition representing our expected results from the intervention. Deaths due to pneumonia."
  },
  {
    "objectID": "Pages/Week2/Time Series Analysis_Hands-On Exercise.html#set-up-the-environment",
    "href": "Pages/Week2/Time Series Analysis_Hands-On Exercise.html#set-up-the-environment",
    "title": "Time Series Analysis",
    "section": "Set Up the Environment",
    "text": "Set Up the Environment\n\nrenv::init()      # Initialize the project     \nrenv::restore()   # Download packages and their version saved in the lockfile\n\n\nsuppressPackageStartupMessages({\n  library(\"readr\")      # For reading in the data\n  library(\"tibble\")     # For handling tidyverse tibble data classes\n  library(\"tidyr\")      # For tidying data \n  library(\"dplyr\")      # For data manipulation \n  library(\"stringr\")    # For string manipulation\n  library(\"MASS\")       # Functions/datasets for statistical analysis\n  library(\"lubridate\")  # For date manipulation\n  library(\"ggplot2\")    # For creating static visualizations\n  library(\"scales\")     # For formatting plots axis\n  library(\"gridExtra\")  # Creates multiple grid-based plots\n})\n\n\n# Function to select \"Not In\"\n'%!in%' &lt;- function(x,y)!('%in%'(x,y))\n\nThe data has been cleaned and standardized for use here, and is imported directly using the GitHub raw URL. You can explore the additional data cleaning steps applied to all of the data in the instructor’s GitHub repository: ysph-dsde/bdsy-phm. The original dataset and prior data cleaning, validation, and standardization can be found in the paper’s GitHub repository and Dr. Weinberger’s workshop GitHub repository 1,3.\n\n# Read in the cleaned data directly from the instructor's GitHub.\ndf &lt;- read_csv(\"https://raw.githubusercontent.com/ysph-dsde/bdsy-phm/refs/heads/main/Data/ec_2to59m.csv\")\n\n# Summarize aspects and dimentions of our dataset.\nglimpse(df)\n\nRows: 144\nColumns: 6\n$ date         &lt;date&gt; 2005-01-01, 2005-02-01, 2005-03-01, 2005-04-01, 2005-05-…\n$ country      &lt;chr&gt; \"Ecuador\", \"Ecuador\", \"Ecuador\", \"Ecuador\", \"Ecuador\", \"E…\n$ age_group    &lt;chr&gt; \"2-59m\", \"2-59m\", \"2-59m\", \"2-59m\", \"2-59m\", \"2-59m\", \"2-…\n$ doses        &lt;chr&gt; \"Any\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\", \"…\n$ J12_J18_prim &lt;dbl&gt; 50, 51, 53, 46, 62, 34, 38, 38, 28, 26, 33, 31, 46, 53, 5…\n$ acm_noj_prim &lt;dbl&gt; 204, 171, 166, 197, 212, 180, 158, 167, 144, 149, 153, 14…\n\n\n\nData Dictionary\nIt is crucial that we understand the meaning of each variable in our dataset. Sometimes, there are surprising aspects embedded within the variables that are not immediately discernible from the table itself. Many sources provide a “Data Dictionary” for this purpose, but at times, you may need to interpret the variable meanings based on context and methods.\nThis paper did not explicitly describe each variable in a “Data Dictionary”; therefore, the following was assembled based on the context and methods provided in the paper and its supplementary materials 3,4.\n\ndate: The month when the events were recorded. This spans from 2005-01-01 to 2016-12-01.\ncountry: Specifies the country where the events were observed. This dataset only represents events recorded in Ecuador.\nage_group: The age of the person who is represented in the counts. This dataset only represents infants aged 2 months to almost 5 years of age (59 months).\ndoses: Specifies the doses of PCV received. As described earlier, all possible combinations of the standard-of-care vaccination series and booster are represented here.\nJ12_J18_prim: Primary cause of death is assigned to the ICD-10 codes J12-J18. We encourage you to read more about what these ICD-10 codes represent 4,5.\nacm_noj_prim: Primary cause of death was assigned any other ICD=10 code, excluding only the J chapter, diseases of the respiratory system."
  },
  {
    "objectID": "Pages/Week2/Time Series Analysis_Hands-On Exercise.html#initial-plot-of-the-time-series",
    "href": "Pages/Week2/Time Series Analysis_Hands-On Exercise.html#initial-plot-of-the-time-series",
    "title": "Time Series Analysis",
    "section": "Initial Plot of the Time Series",
    "text": "Initial Plot of the Time Series\nWe begin each time series analysis by examining the entire span of data, typically plotted as a line or scatter plot. All methods for modeling the vaccination introduction time series data require the dates when the vaccine was introduced, in this case the PCV10 vaccine.\n\n# Estimation when the vaccine was introduced in YYYY-MM-DD format.\nvax.intro.date &lt;- as.Date(\"2010-08-01\")\n\n# Date when vaccine efficacy evaluations started; at least 12 months\n# following administration.\nvax.eval.date &lt;- as.Date(\"2011-08-01\")\n\nWe are not going to spend time explaining how to plot using the tidyverse package ggplot2() here. Later in the week you will receive a lecture covering this topic. In the meantime, you are welcome to explore the ggplot2 package documentation or the Data Science and Data Equity (DSDE) group’s online Book of Workshops 7,8.\n\np1 &lt;- \n  ggplot(df, aes(x = date, y = J12_J18_prim)) +\n      geom_line() +\n      labs(title = \"Deaths Categorized by\\nICD-10 Codes J12-18\",\n         x = \"Date\", y = \"Counts for &lt;5 yo\") +\n      # Have y-axis for the two plots be the same.\n      ylim(0, 250) +\n      # Reference line.\n      geom_vline(xintercept = vax.intro.date, col = \"red\", lty = 2) +\n      theme_linedraw()\n   \n\np2 &lt;- \n  ggplot(df, aes(x = date, y = acm_noj_prim)) +\n      geom_line() +\n      labs(title = \"Deaths Categorized by\\nNon-J chapter ICD-10 Codes\",\n         x = \"Date\", y = \"Counts for &lt;5 yo\") +\n      # Have y-axis for the two plots be the same.\n      ylim(0, 250) +\n      # Reference line.\n      geom_vline(xintercept = vax.intro.date, col = \"red\", lty = 2) +\n      theme_linedraw()\n\n# Display the plots side-by-side.\ngrid.arrange(p1, p2, nrow = 1)"
  },
  {
    "objectID": "Pages/Week2/Time Series Analysis_Hands-On Exercise.html#simple-linear-model",
    "href": "Pages/Week2/Time Series Analysis_Hands-On Exercise.html#simple-linear-model",
    "title": "Time Series Analysis",
    "section": "Simple Linear Model",
    "text": "Simple Linear Model\nTo correctly fit a simple linear model to the data, we need to apply a linearization that appropriately reflects its distribution. There are more analytical approaches to achieve this that will not be covered here. We start by visually examining the distribution with a scatter plot.\n\n\n\n\n\n\nNote\n\n\n\nIt is helpful to consider the data generation method, which can provide insights into the likely distribution.\n\n\n\np_base &lt;- \n  ggplot(df, aes(x = date, y = J12_J18_prim)) +\n      geom_point() +\n      labs(title = \"Deaths Scatter Plot\",\n         x = \"Date\", y = \"Counts for &lt;5 yo\") +\n      # Have y-axis for the two plots be the same.\n      ylim(0, NA) +\n      theme_linedraw()\n\np_base\n\n\n\n\n\n\n\n\nVariables that reflect counts (i.e., the number of deaths per month) can be distributed as either Poisson or negative binomial. Note that Poisson regression may artificially narrow confidence intervals when the data is overdispersed. To address this, you can use a negative binomial regression or a quasipoisson model, which accounts for the unexplained variation. In this context, we will apply a negative binomial regression.\n\n# For modeling, we need to use an ordered, discrete variable. Simply, we \n# can use the rownames for this purpose.\ndf &lt;- tibble::rownames_to_column(df, var = \"index\") %&gt;%\n  mutate(index = as.numeric(index))\n\n# Apply the negative binomial regression.\nmod1 &lt;- glm.nb(J12_J18_prim ~ index , data = df)\n\n# Examine the fitting results.\nsummary(mod1)\n\n\nCall:\nglm.nb(formula = J12_J18_prim ~ index, data = df, init.theta = 38.18819754, \n    link = log)\n\nCoefficients:\n              Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  3.8223994  0.0389150   98.22   &lt;2e-16 ***\nindex       -0.0062897  0.0004981  -12.63   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Negative Binomial(38.1882) family taken to be 1)\n\n    Null deviance: 304.02  on 143  degrees of freedom\nResidual deviance: 142.25  on 142  degrees of freedom\nAIC: 976.12\n\nNumber of Fisher Scoring iterations: 1\n\n              Theta:  38.2 \n          Std. Err.:  10.2 \n\n 2 x log-likelihood:  -970.12 \n\n\n\n# Make predictions with confidence intervals.\npred &lt;- predict(mod1, type = \"response\", se.fit = TRUE)\n\n# Add the model predictions and 95% COI to the dataframe.\ndf.pred &lt;- df %&gt;%\n  mutate(se.fit = pred$se.fit, pred = pred$fit) %&gt;%\n  mutate(\n    conf.low = pred - 1.96 * se.fit,\n    conf.high = pred + 1.96 * se.fit\n  )\n\n# Plot the newly created model fitting.\np_sm &lt;- p_base +\n  # Add the fitting line.\n  geom_line(data = df.pred, aes(x = date, y = pred),\n            color = \"#e41a1c\") +\n  # Add the confidence interval.\n  geom_ribbon(data = df.pred, aes(ymin = conf.low, ymax = conf.high), \n              alpha = 0.2, fill = \"blue\") +\n  # Change the title name.\n  labs(title = \"Deaths with a Negative Binomial Fit\")\n\np_sm\n\n\n\n\n\n\n\n\nThis is not a terrible predictor of our trend, but it overlooks many known sources of variance. For instance, we know that infections have seasonal trends, sometimes referred to by its technical term periodicity. Additionally, our baseline population may change from 2005 to 2016, which can consequently shift the overall disease trend in tandem with these baseline changes."
  },
  {
    "objectID": "Pages/Week2/Time Series Analysis_Hands-On Exercise.html#building-the-multiple-linear-model",
    "href": "Pages/Week2/Time Series Analysis_Hands-On Exercise.html#building-the-multiple-linear-model",
    "title": "Time Series Analysis",
    "section": "Building the Multiple Linear Model",
    "text": "Building the Multiple Linear Model\n\n\n\n\nPrinciples of Seasonality - Generated with Yale’s AI Clarity\n\n\n\n\nAdd Controls for Seasonality\nSeasonality often manifests as periodicity, where recurring patterns repeat at regular, fixed intervals of time. This differs from the related concept of cyclicity, which represents recurring patterns that do not occur at regular intervals or consistently appear in regular seasons 9.\nEach seasonal pattern can be decomposed into three components that can either remain constant (stationary) or change over time (non-stationary): mean, variance, and covariance. The figure on the left illustrates the four possible seasonal trends separately. The top-left panel shows a recurring pattern with time-invariant, stationary parameters, while the other panels demonstrate the effects of non-stationary parameters.\n\n\n\n\n\n\nNote\n\n\n\nMost time series data in the natural sciences exhibit seasonality, periodicity, or cyclicity, though these patterns may be difficult to detect when the data is noisy 10.\n\n\nTo control for temporal variations such as seasonality, we employ a simple form of dynamic linear regression (DLR). Essentially, we add variables to our simple linear regression model that represents different subsections of time as new regressors. Applying a DLR allows for changes in the mean value of the underlying regression relationship 11. For the regression function to recognize each date, we need to factorize the date variable to assign the month the observation occured.\n\ndf$month &lt;- as.factor(month(df$date))\n\n# Inspect the first 36 entries.\ndf$month[1:36]\n\n [1] 1  2  3  4  5  6  7  8  9  10 11 12 1  2  3  4  5  6  7  8  9  10 11 12 1 \n[26] 2  3  4  5  6  7  8  9  10 11 12\nLevels: 1 2 3 4 5 6 7 8 9 10 11 12\n\n# Update the model.\nmod2 &lt;- glm.nb(J12_J18_prim ~ date + month, data = df)\nsummary(mod2)\n\n\nCall:\nglm.nb(formula = J12_J18_prim ~ date + month, data = df, init.theta = 88.28907799, \n    link = log)\n\nCoefficients:\n              Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  6.496e+00  2.156e-01  30.132  &lt; 2e-16 ***\ndate        -1.988e-04  1.426e-05 -13.941  &lt; 2e-16 ***\nmonth2      -2.717e-02  8.166e-02  -0.333 0.739391    \nmonth3       3.446e-02  8.086e-02   0.426 0.669971    \nmonth4      -1.892e-01  8.441e-02  -2.241 0.025011 *  \nmonth5      -4.964e-02  8.228e-02  -0.603 0.546315    \nmonth6      -1.571e-01  8.409e-02  -1.868 0.061714 .  \nmonth7      -2.118e-01  8.513e-02  -2.488 0.012840 *  \nmonth8      -2.076e-01  8.518e-02  -2.437 0.014804 *  \nmonth9      -3.266e-01  8.749e-02  -3.733 0.000189 ***\nmonth10     -2.253e-01  8.574e-02  -2.627 0.008603 ** \nmonth11     -2.467e-01  8.626e-02  -2.859 0.004243 ** \nmonth12     -3.386e-01  8.813e-02  -3.842 0.000122 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Negative Binomial(88.2891) family taken to be 1)\n\n    Null deviance: 404.40  on 143  degrees of freedom\nResidual deviance: 143.84  on 131  degrees of freedom\nAIC: 959.32\n\nNumber of Fisher Scoring iterations: 1\n\n              Theta:  88.3 \n          Std. Err.:  41.2 \n\n 2 x log-likelihood:  -931.319 \n\n\n\n# Make predictions with confidence intervals.\npred2 &lt;- predict(mod2, type = \"response\", se.fit = TRUE)\n\n# Add the model predictions and 95% COI to the dataframe.\ndf.pred2 &lt;- df %&gt;%\n  mutate(se.fit = pred2$se.fit, pred = pred2$fit) %&gt;%\n  mutate(\n    conf.low = pred - 1.96 * se.fit,\n    conf.high = pred + 1.96 * se.fit\n  )\n\n# Plot the newly created model fitting.\np_season &lt;- p_base +\n  # Add the fitting line.\n  geom_line(data = df.pred2, aes(x = date, y = pred),\n            color = \"#e41a1c\") +\n  # Add the confidence interval.\n  geom_ribbon(data = df.pred2, aes(ymin = conf.low, ymax = conf.high), \n              alpha = 0.2, fill = \"blue\") +\n  # Change the title name.\n  labs(title = \"Deaths with a Negative Binomial Fit\\nSeasonality Term Included\")\n\np_season\n\n\n\n\n\n\n\n\n\n\nAdd Controls for Baseline Shifts\n\n\n\n\nFigure by Dr. Dan Weinberger - Generated with Yale’s AI Clarity  6\n\n\n\nWe can expect that the baseline population changes over time, introducing unaccounted-for heterogeneous variance to our model. In epidemiology, it is standard to contextualize raw data by applying a denominator, or offset, that represents the population at risk of the disease.\nThe figure on the right illustrates how the baseline for the number of hospitalizations normalizes when we apply the population offset. Keep in mind that this ratio is often scaled to “per 100,000 persons.” 6.\n\n\\frac{n_{cases}}{n_{population}} \\times 100,000\n\nIn real-world scenarios, the total base population is not always reported, or the coverage might be unreliable for the entire span of the time series. Alternatively, we can apply denominators such as the number of people using the healthcare system, the number of people hospitalized, and similar metrics 6. In this example, we could apply the total population size or all non-respiratory causes of mortality.\n\n# Create the offset using all deaths not coded as J. Transform the values\n# to log before use in the negative binomial fitting.\ndf$log.offset &lt;- log(df$acm_noj_prim)\n\n# Refit the model with an offset.\nmodel3 &lt;- glm.nb(J12_J18_prim ~ index + month + offset(log.offset), data = df)\nsummary(model3)\n\n\nCall:\nglm.nb(formula = J12_J18_prim ~ index + month + offset(log.offset), \n    data = df, init.theta = 59.08482937, link = log)\n\nCoefficients:\n              Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -1.3687586  0.0672076 -20.366  &lt; 2e-16 ***\nindex       -0.0034881  0.0004599  -7.584 3.36e-14 ***\nmonth2       0.1651225  0.0873636   1.890   0.0587 .  \nmonth3       0.1509818  0.0867654   1.740   0.0818 .  \nmonth4      -0.0005318  0.0899497  -0.006   0.9953    \nmonth5       0.0704091  0.0880910   0.799   0.4241    \nmonth6       0.0227796  0.0896446   0.254   0.7994    \nmonth7      -0.0122476  0.0906691  -0.135   0.8925    \nmonth8       0.0077403  0.0907734   0.085   0.9320    \nmonth9      -0.0526956  0.0929670  -0.567   0.5708    \nmonth10     -0.0053806  0.0912550  -0.059   0.9530    \nmonth11      0.0443924  0.0916610   0.484   0.6282    \nmonth12     -0.0283404  0.0935272  -0.303   0.7619    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Negative Binomial(59.0848) family taken to be 1)\n\n    Null deviance: 219.59  on 143  degrees of freedom\nResidual deviance: 145.49  on 131  degrees of freedom\nAIC: 977.59\n\nNumber of Fisher Scoring iterations: 1\n\n              Theta:  59.1 \n          Std. Err.:  21.1 \n\n 2 x log-likelihood:  -949.594 \n\n\n\n# Make predictions with confidence intervals.\npred3 &lt;- predict(model3, type = \"response\", se.fit = TRUE)\n\n# Add the model predictions and 95% COI to the dataframe.\ndf.pred3 &lt;- df %&gt;%\n  mutate(se.fit = pred3$se.fit, pred = pred3$fit) %&gt;%\n  mutate(\n    conf.low = pred - 1.96 * se.fit,\n    conf.high = pred + 1.96 * se.fit\n  )\n\n# Plot the newly created model fitting.\np_offset &lt;- p_base +\n  # Add the fitting line.\n  geom_line(data = df.pred3, aes(x = date, y = pred),\n            color = \"#e41a1c\") +\n  # Add the confidence interval.\n  geom_ribbon(data = df.pred3, aes(ymin = conf.low, ymax = conf.high), \n              alpha = 0.2, fill = \"blue\") +\n  # Change the title name.\n  labs(title = \"Deaths with a Negative Binomial Fit\\nSeasonality Term and Offset\")\n\np_offset\n\n\n\n\n\n\n\n\nNotice that the seasonality trend, which was clear in the previous plot, is disrupted when the population offset is applied. Now that we have our baseline model, we are ready to proceed with examining the effect of the intervention with PCV10 after its introduction on 2010-08-01."
  },
  {
    "objectID": "Pages/Week2/Time Series Analysis_Hands-On Exercise.html#does-the-disease-trend-or-level-change",
    "href": "Pages/Week2/Time Series Analysis_Hands-On Exercise.html#does-the-disease-trend-or-level-change",
    "title": "Time Series Analysis",
    "section": "Does the Disease Trend or Level Change?",
    "text": "Does the Disease Trend or Level Change?\nIn this section, we will test whether the trend or level of pneumonia deaths due to pneumococcal disease changes after the introduction of the PCV10 vaccine. To accomplish this, we will set up a counterfactual model to represent what our base model predicts the death counts would have been if the vaccine had not been introduced to the population. Similarly, a factual model will be applied to represent what actually happened with the vaccine rollout.\nWe will then compare the factual model against the counterfactual model to determine if a difference is identified in the period following vaccine introduction. This process is generally referred to as detecting vaccine effect or vaccine impact.\n\n\n\n\n\n\nWarning\n\n\n\nYou want to be careful when applying causal inference methods to model building, perturbation, or prediction questions. While our analysis uses a causal framework to detect the possible impact of the vaccine introduction, this does not permit us to make definitive claims of causation.\nAdditionally, it is important to remember that the model we have built may have other flaws influencing its sensitivity to the pre/post vaccination period. It is always best practice to test a model under different conditions and subject it to various tests to robustly justify its accuracy, precision, and generalizability.\n\n\nThe figure below illustrates the three phases of the vaccine rollout: pre-vaccination, post-vaccination, and a latency period between the start of vaccine distribution and efficacy evaluation. Recall that our base negative binomial model accounts for three effector variables: an ordered, discrete variable organizing the outcomes temporally without seasonality, the month to capture seasonal periodicity, and an offset to the baseline accounting for population changes. Our baseline model is expected to, and is therefore assumed to, detect the drop in average cases during the post-vaccination period.\n\n\n\nFigure by Dr. Dan Weinberger - From his recorded lecture Interrupted time series analysis 12.\n\n\nThis model, however, does not explicitly assign outcomes to the three phases represented in the figure. Therefore, we need to set up variables that distinguish outcomes before and after the vaccine introduction, as well as before and after the entry to the vaccine evaluation phase, in order to convert the base model into our factual and counterfactual models.\nWe also need to regenerate the model to produce factually-based and counterfactually-based predictions for comparison. There are three modeling approaches we will evaluate here, each employing a different method to define the pre/post and latency vaccination periods. These differences will impact our inference about vaccine effectiveness.\n\nInterrupted Time Series with Disconnected Segments: This method fits different line segments through the data and tests whether the slope or level of the disease changes. It can sometimes result in abrupt jumps when fitting the model.\nInterrupted Time Series with Connected Segments (Spline Model): This method allows the slope to change in the post-vaccine period but ensures the change is smooth.\nExtrapolation Based on the Pre-Vaccine Period: This method fits the model to data from the pre-vaccine period only and extrapolates the trend to the post-vaccine period.\n\n\nMethod 1. Interrupted Time Series with Disconnected Segments\nWe will add two new binary variables to our dataset: vax_intro and vax_eval. The vax_intro variable will be assigned a value of 0 before 2010-08-01 and 1 afterward; similarly, the vax_eval variable will be assigned a value of 0 before 2011-08-01 and 1 afterward.\n\n# Assign outcomes to the known vaccine phases.\ndf &lt;- df %&gt;%\n  mutate(\n    vax_intro = ifelse(date &gt;= vax.intro.date, 1, 0),\n    vax_eval = ifelse(date &gt;= vax.eval.date, 1, 0)\n  )\n\n# View the changes by randomly selecting dates.\ndf[sort(sample(1:144, 10)), c(\"index\", \"date\", \"vax_intro\", \"vax_eval\")]\n\n# A tibble: 10 × 4\n   index date       vax_intro vax_eval\n   &lt;dbl&gt; &lt;date&gt;         &lt;dbl&gt;    &lt;dbl&gt;\n 1    37 2008-01-01         0        0\n 2    39 2008-03-01         0        0\n 3    41 2008-05-01         0        0\n 4    45 2008-09-01         0        0\n 5    59 2009-11-01         0        0\n 6    60 2009-12-01         0        0\n 7    61 2010-01-01         0        0\n 8    64 2010-04-01         0        0\n 9    85 2012-01-01         1        1\n10   135 2016-03-01         1        1\n\n\nIn linear modeling, it is common to include interaction terms. These terms allow the model to capture interdependencies between variables, showing how the effect of one variable on the outcome changes depending on the level of another variable. Interaction terms also enable the identification of synergistic effects, where the combined effect of two variables is greater (or less) than the sum of their individual effects 13.\nEquation 1 shows a multiple linear regression without an interaction term, and Equation 2 shows the inclusion of the interaction term. Notice that including the interaction term impacts the slope, making it dependent on the value of the other predictor. For example, the impact of X_1, holding X_2 constant, on the slope of Y is a function of (\\beta_1 + \\beta_3)\\ X_2 13.\n\\begin{align}\nY &= \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\epsilon\\\\\nY &= \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 (X_1 \\times X_2) + \\epsilon\n\\end{align}\nWe will not spend more time discussing interaction terms but will briefly examine if they enhance the model’s predictive power. Note that while interaction terms can improve predictive accuracy, they also complicate the model and may result in a trade-off between predictive power and generalizability to other datasets.\nFirst we will generate the two models, then we will evaluate their performance side-by-side.\n\nNo Interaction Terms\nCreate a simple step-change model without an interaction term involving the newly added variables, vax_intro and vax_eval.\n\n# Additional seasonality controls covered in the collapsed box above.\nmod_method1a &lt;- glm.nb(J12_J18_prim ~ index + month + offset(log.offset) +\n                         # Change in disease during administration and \n                         # evaluation period.\n                         vax_intro + vax_eval, data = df)\n\nsummary(mod_method1a)\n\n\nCall:\nglm.nb(formula = J12_J18_prim ~ index + month + offset(log.offset) + \n    vax_intro + vax_eval, data = df, init.theta = 64.37470892, \n    link = log)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -1.382710   0.069290 -19.956   &lt;2e-16 ***\nindex       -0.002510   0.001014  -2.476   0.0133 *  \nmonth2       0.163233   0.086008   1.898   0.0577 .  \nmonth3       0.147780   0.085409   1.730   0.0836 .  \nmonth4      -0.004393   0.088661  -0.050   0.9605    \nmonth5       0.065323   0.086796   0.753   0.4517    \nmonth6       0.017042   0.088446   0.193   0.8472    \nmonth7      -0.018766   0.089510  -0.210   0.8339    \nmonth8       0.006893   0.089471   0.077   0.9386    \nmonth9      -0.054212   0.091693  -0.591   0.5544    \nmonth10     -0.006595   0.089942  -0.073   0.9416    \nmonth11      0.041625   0.090381   0.461   0.6451    \nmonth12     -0.031725   0.092285  -0.344   0.7310    \nvax_intro   -0.167101   0.083391  -2.004   0.0451 *  \nvax_eval     0.076690   0.083253   0.921   0.3570    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Negative Binomial(64.3747) family taken to be 1)\n\n    Null deviance: 225.73  on 143  degrees of freedom\nResidual deviance: 145.44  on 129  degrees of freedom\nAIC: 977.59\n\nNumber of Fisher Scoring iterations: 1\n\n              Theta:  64.4 \n          Std. Err.:  24.3 \n\n 2 x log-likelihood:  -945.595 \n\n\n\n## --------------------\n## Generate the factual model\n\n# Make predictions with confidence intervals.\npred4 &lt;- predict(mod_method1a, type = \"response\", se.fit = TRUE)\n\n\n## --------------------\n## Prepare a plot to visualize the factual model predictions\n\n# Add the model predictions and 95% COI to the dataframe.\ndf.pred.its &lt;- df %&gt;%\n  mutate(se.fit = pred4$se.fit, pred = pred4$fit) %&gt;%\n  mutate(\n    conf.low = pred - 1.96 * se.fit,\n    conf.high = pred + 1.96 * se.fit\n  )\n\n# Plot the newly created model fitting.\np_m1a &lt;- p_base +\n  # Add the fitting line.\n  geom_line(data = df.pred.its, aes(x = date, y = pred),\n            color = \"#377eb8\") +\n  # Add the confidence interval.\n  geom_ribbon(data = df.pred.its, aes(ymin = conf.low, ymax = conf.high), \n              alpha = 0.2, fill = \"blue\") +\n  # Change the title name.\n  labs(title = \"Deaths with a Negative Binomial Fit\\nInterupted Time Series - No Interaction Terms\")\n\n\n## --------------------\n## Generate the counterfactual model\n\n# Initialize the vacccine effect variables by setting them to 0.\ndf.counterfactual.a &lt;- df\ndf.counterfactual.a$vax_intro &lt;- 0\ndf.counterfactual.a$vax_eval &lt;- 0\n\n# Generate the fitted values using the counterfactual setup instead.\ndf.pred.its$pred4.cf.a &lt;- predict(mod_method1a, type = \"response\", \n                                 newdata = df.counterfactual.a)\n\n\n## --------------------\n## Rate ratio to evaluate performance\n\n# Generate the rate ratio between the factual and counterfactual values.\ndf.pred.its$rr.its.a &lt;- df.pred.its$pred/df.pred.its$pred4.cf.a\n\n\n\nWith Interaction Terms\nCreate the more complicated model option by adding an interaction term involving vax_intro and vax_eval, keeping both individually and including with an interaction with index.\n\nmod_method1b &lt;- glm.nb(J12_J18_prim ~ index + month + offset(log.offset) +\n                         # Add the counterfactuals with an interaction term.\n                         vax_intro + vax_intro*index +\n                         vax_eval + vax_eval*index, data = df)\n\nsummary(mod_method1b)\n\n\nCall:\nglm.nb(formula = J12_J18_prim ~ index + month + offset(log.offset) + \n    vax_intro + vax_intro * index + vax_eval + vax_eval * index, \n    data = df, init.theta = 65.62229438, link = log)\n\nCoefficients:\n                 Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)     -1.410897   0.074005 -19.065   &lt;2e-16 ***\nindex           -0.001660   0.001288  -1.288   0.1976    \nmonth2           0.163960   0.085713   1.913   0.0558 .  \nmonth3           0.148234   0.085153   1.741   0.0817 .  \nmonth4          -0.003138   0.088498  -0.035   0.9717    \nmonth5           0.067516   0.086678   0.779   0.4360    \nmonth6           0.019034   0.088478   0.215   0.8297    \nmonth7          -0.016266   0.089654  -0.181   0.8560    \nmonth8           0.003650   0.089526   0.041   0.9675    \nmonth9          -0.055524   0.091668  -0.606   0.5447    \nmonth10         -0.008492   0.089787  -0.095   0.9247    \nmonth11          0.039981   0.090176   0.443   0.6575    \nmonth12         -0.032091   0.092017  -0.349   0.7273    \nvax_intro        0.267869   1.533398   0.175   0.8613    \nvax_eval        -0.184445   1.541114  -0.120   0.9047    \nindex:vax_intro -0.006384   0.020854  -0.306   0.7595    \nindex:vax_eval   0.004214   0.020870   0.202   0.8400    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Negative Binomial(65.6223) family taken to be 1)\n\n    Null deviance: 227.08  on 143  degrees of freedom\nResidual deviance: 145.13  on 127  degrees of freedom\nAIC: 980.44\n\nNumber of Fisher Scoring iterations: 1\n\n              Theta:  65.6 \n          Std. Err.:  25.0 \n\n 2 x log-likelihood:  -944.439 \n\n\n\n## --------------------\n## Generate the factual model\n\n# Make predictions with confidence intervals.\npred5 &lt;- predict(mod_method1b, type = \"response\", se.fit = TRUE)\n\n\n## --------------------\n## Prepare a plot to visualize the factual model predictions\n\n# Add the model predictions and 95% COI to the dataframe.\ndf.pred.its &lt;- df.pred.its %&gt;%\n  mutate(se.fit = pred5$se.fit, pred = pred4$fit) %&gt;%\n  mutate(\n    conf.low = pred - 1.96 * se.fit,\n    conf.high = pred + 1.96 * se.fit\n  )\n\n# Plot the newly created model fitting.\np_m1b &lt;- p_base +\n  # Add the fitting line.\n  geom_line(data = df.pred.its, aes(x = date, y = pred),\n            color = \"#377eb8\") +\n  # Add the confidence interval.\n  geom_ribbon(data = df.pred.its, aes(ymin = conf.low, ymax = conf.high), \n              alpha = 0.2, fill = \"blue\") +\n  # Change the title name.\n  labs(title = \"Deaths with a Negative Binomial Fit\\nInterupted Time Series - With Interaction Terms\")\n\n\n## --------------------\n## Generate the counterfactual model\n\n# Initialize the vacccine effect variables by setting them to 0.\ndf.counterfactual.b &lt;- df\ndf.counterfactual.b$vax_intro &lt;- 0\ndf.counterfactual.b$vax_eval &lt;- 0\n\n# Generate the fitted values using the counterfactual setup instead.\ndf.pred.its$pred4.cf.b &lt;- predict(mod_method1b, type = \"response\", \n                                  newdata = df.counterfactual.b)\n\n\n## --------------------\n## Rate ratio to evaluate performance\n\n# Generate the rate ratio between the factual and counterfactual values.\ndf.pred.its$rr.its.b &lt;- df.pred.its$pred/df.pred.its$pred4.cf.b\n\n\n\nCompare Model Complexity\nLet’s start by comparing how the fit differs between the models.\n\n# Overlay onto plot.\np_m1a_pred &lt;- p_m1a +\n geom_line(data = df.pred.its, aes(x = date, y = pred4.cf.a),\n           color = '#e41a1c', lty = 2)\n\n# Overlay onto plot.\np_m1b_pred &lt;- p_m1b +\n geom_line(data = df.pred.its, aes(x = date, y = pred4.cf.b),\n           color = '#e41a1c', lty = 2)\n\ngrid.arrange(p_m1a_pred, p_m1b_pred, ncol = 1)\n\n\n\n\n\n\n\n\nWe do see differences with the prediction adherance to the data we trained the model on. By qualitative examination, it appears the model with interaction terms diverge more in the post-vaccination period.\nThe Akaike Information Criterion (AIC) is a one measure used to compare the goodness of fit of different statistical models, while also accounting for model complexity. It helps in model selection by balancing model fit and complexity, otherwise called the bias/variance trade-off.\nThere are different variations of AIC, but the basic definition is\n\n\\text{AIC} = 2k - 2\\ln(\\mathcal{L})\n\nwhere k is the number of parameters in the model and \\mathcal{L} is the maximum likelihood of the model.\n\nAIC(mod_method1a, mod_method1b)\n\n             df      AIC\nmod_method1a 16 977.5948\nmod_method1b 18 980.4387\n\n\nThe results indicate that the AIC score is worse (higher) when interaction terms are included. Since there is no significant gain in performance, we will prefer the simpler model for better generalizability.\nBelow, we also observe that the interaction terms create unexpected trends in the factual/counterfactual ratio. Notably, the ratio is not 1 (indicating that factual and counterfactual predictions are the same) in the pre-vaccination period, which is not ideal.\n\n# Inspect methods sensitivity to vaccine impact.\np_ratio &lt;- \n  ggplot(df.pred.its, aes(x = date, y = rr.its.a)) +\n      geom_line() +\n      labs(title = \"Rate Ratio of the ITS Model\",\n         x = \"Date\", y = \"Rate ratio\") +\n      ylim(0, NA) +\n      geom_vline(xintercept = vax.intro.date, col = \"red\", lty = 2) +\n      annotate(\"text\", x = vax.intro.date + 30, y = 00.1, label = \"Vaccine Introduced\", color = \"red\", hjust = 0) +\n      theme_linedraw()\n\np_ratio +\n  geom_line(data = df.pred.its, aes(x = date, y = rr.its.b),\n            color = \"#4daf4a\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode Like a Pro\n\n\n\nWe won’t try to estimate a confidence interval on the rate ratio here. Students are encouraged to checkout the the InterventionEvaluatR package, which will automatically calculate the confidence intervals for these ratios.\n\n\n\n\n\nMethod 2. Interrupted Time Series with Connected Segments (Spline Model)\nIn this case, we force the changes to be smooth so that we don’t get a drastic jump after vaccination introduction that prematurly shows impact during the expected latency period. Therefore, we will add two new variables to our dataset: spl1 and spl2. The spl1 variable will be assigned a value of 0 before 2010-08-01 and index - intro.index + 1 afterward; similarly, the spl2 variable will be assigned a value of 0 before 2011-08-01 and index - eval.index + 1 afterward.\n\n# Identifies the row index that represents when the vaccine was introduced\n# and when the evaluation period started.\nintro.index &lt;- which(df$date == vax.intro.date)\neval.index  &lt;- which(df$date == vax.eval.date)\n\n# Assign outcomes to the known vaccine phases.\ndf &lt;- df %&gt;%\n  mutate(\n    spl1 = ifelse(index - intro.index + 1 &lt; 0, 0, index - intro.index + 1),\n    spl2 = ifelse(index - eval.index + 1 &lt; 0, 0, index - eval.index + 1)\n  )\n\n# Inspect the changes.\ndf[sort(sample(1:144, 10)), c(\"date\", \"index\", \"spl1\", \"spl2\")]\n\n# A tibble: 10 × 4\n   date       index  spl1  spl2\n   &lt;date&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 2005-11-01    11     0     0\n 2 2006-02-01    14     0     0\n 3 2006-03-01    15     0     0\n 4 2007-12-01    36     0     0\n 5 2008-04-01    40     0     0\n 6 2010-04-01    64     0     0\n 7 2011-11-01    83    16     4\n 8 2013-06-01   102    35    23\n 9 2014-03-01   111    44    32\n10 2016-04-01   136    69    57\n\n\n\nmod_method2 &lt;- glm.nb(J12_J18_prim ~ index + month + offset(log.offset) +\n                        # Post-vaccine changes.\n                        spl1 + spl2, data = df)\n\nsummary(mod_method2)\n\n\nCall:\nglm.nb(formula = J12_J18_prim ~ index + month + offset(log.offset) + \n    spl1 + spl2, data = df, init.theta = 61.04668124, link = log)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -1.397121   0.074586 -18.732   &lt;2e-16 ***\nindex       -0.002349   0.001237  -1.899   0.0576 .  \nmonth2       0.164546   0.086841   1.895   0.0581 .  \nmonth3       0.151023   0.086225   1.752   0.0799 .  \nmonth4      -0.001397   0.089442  -0.016   0.9875    \nmonth5       0.069656   0.087565   0.795   0.4263    \nmonth6       0.020998   0.089150   0.236   0.8138    \nmonth7      -0.013534   0.090170  -0.150   0.8807    \nmonth8       0.006429   0.090270   0.071   0.9432    \nmonth9      -0.055311   0.092521  -0.598   0.5500    \nmonth10     -0.008406   0.090793  -0.093   0.9262    \nmonth11      0.040934   0.091221   0.449   0.6536    \nmonth12     -0.032326   0.093106  -0.347   0.7284    \nspl1        -0.007961   0.007038  -1.131   0.2580    \nspl2         0.007591   0.007271   1.044   0.2965    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Negative Binomial(61.0467) family taken to be 1)\n\n    Null deviance: 221.95  on 143  degrees of freedom\nResidual deviance: 145.72  on 129  degrees of freedom\nAIC: 980.3\n\nNumber of Fisher Scoring iterations: 1\n\n              Theta:  61.0 \n          Std. Err.:  22.3 \n\n 2 x log-likelihood:  -948.296 \n\n\n\n## --------------------\n## Generate the factual model\n\n# Make predictions with confidence intervals.\ndf.pred.spl &lt;- df %&gt;%\n  mutate(pred.spl = predict(mod_method2, type = \"response\"))\n\n\n## --------------------\n## Generate the counterfactual model\n\n# Initialize the vacccine effect variables by setting them to 0.\ndf.counterfactual.spl &lt;- df\ndf.counterfactual.spl$spl1 &lt;- 0\ndf.counterfactual.spl$spl2 &lt;- 0\n\n# Generate the fitted values.\ndf.pred.spl$pred.spl.cf &lt;- predict(mod_method2, type = \"response\", \n                                   newdata = df.counterfactual.spl)\n\n\n## --------------------\n## Rate ratio to evaluate performance\n\n# Generate the rate ratio between the fitted and counterfactual values.\ndf.pred.spl$rr.spline &lt;- df.pred.spl$pred.spl/df.pred.spl$pred.spl.cf\n\n\n## --------------------\n## Prepare a plot to visualize the factual model predictions\n\n# Make predictions with confidence intervals.\npred6 &lt;- predict(mod_method2, type = \"response\", se.fit = TRUE)\n\n# Add the model predictions and 95% COI to the dataframe.\ndf.pred.spl &lt;- df.pred.spl %&gt;%\n  mutate(se.fit = pred6$se.fit, pred = pred6$fit) %&gt;%\n  mutate(\n    conf.low = pred - 1.96 * se.fit,\n    conf.high = pred + 1.96 * se.fit\n  )\n\n\np_m2 &lt;- \n  ggplot(df.pred.spl, aes(x = date, y = J12_J18_prim)) +\n      geom_point() +\n      # Add the fitted line.\n      geom_line(data = df.pred.spl, aes(x = date, y = pred.spl),\n                color = \"#377eb8\") +\n      # Add the confidence interval.\n      geom_ribbon(data = df.pred.spl, aes(ymin = conf.low, ymax = conf.high),\n                  alpha = 0.2, fill = \"blue\") +\n      # Add the counterfactual line.\n      geom_line(data = df.pred.spl, aes(x = date, y = pred.spl.cf),\n                color = \"#e41a1c\", lty = 2) +\n      labs(title = \"Deaths with a Negative Binomial Fit\\nITS with Spline Smoothing\",\n         x = \"Date\", y = \"Counts for &lt;5 yo\") +\n      theme_linedraw()\n\np_m2\n\n\n\n\n\n\n\n\nWe can see here that the decline follows a smoother trajectory, by design. We can check the counterfactual performace by examining the ratio again.\n\n# Inspect methods sensitivity to vaccine impact.\np_ratio &lt;- p_ratio +\n     geom_line(data = df.pred.spl, aes(x = date, y = rr.spline),\n               color = \"#4daf4a\") +\n      # Update the title.\n      labs(title = \"Rate Ratio with the Spline Model\")\n\np_ratio\n\n\n\n\n\n\n\n\n\n\nMethod #3: Extrapolation Based on the Pre-Vaccine Period\nNow we are going to model only the pre-vaccination period and forecast outcomes based on the restricted fitting into the post-vaccination period. We will create a new variable that excludes any records following 2010-08-01, J12_J18_prim_pre.\n\n# Create a new variable where it is J12_J18_prim until the vaccine is introduced,\n# then it is NA afterwards.\ndf$J12_J18_prim_pre &lt;- df$J12_J18_prim\ndf$J12_J18_prim_pre[which(df$date &gt;= vax.intro.date)] &lt;- NA\n\n# Inspect the changes.\ndf[sort(sample(1:144, 10)), c(\"date\", \"J12_J18_prim_pre\")]\n\n# A tibble: 10 × 2\n   date       J12_J18_prim_pre\n   &lt;date&gt;                &lt;dbl&gt;\n 1 2005-12-01               31\n 2 2006-05-01               49\n 3 2007-01-01               45\n 4 2008-11-01               51\n 5 2010-03-01               36\n 6 2010-05-01               33\n 7 2011-12-01               NA\n 8 2012-06-01               NA\n 9 2013-10-01               NA\n10 2014-11-01               NA\n\n\n\nmod_method3 &lt;- glm.nb(J12_J18_prim_pre ~ index + month + offset(log.offset), data = df)\n\n# Add the prediction using the smoothed model.\ndf.pred.pre &lt;- df %&gt;%\n  mutate(pred.pre = predict(mod_method3, type = \"response\", newdata = df))\n\nUnlike before, the original observed variable J12_J18_prim now represents the factual model, as we regressed on a subset of data assuming nothing was known following the vaccine introduction. The prediction results using the restricted regressand represents the counterfactual model, since it extrapolates data points missing from the model fitting.\n\n## --------------------\n## Rate ratio to evaluate performance\n\n# Generate the rate ratio between the factual and counterfactual values.\ndf.pred.pre$rr.trend &lt;- df$J12_J18_prim/df.pred.pre$pred.pre\n\n\n## --------------------\n## Prepare a plot to visualize the factual model predictions\n\n# Make predictions with confidence intervals.\npred7 &lt;- predict(mod_method3, type = \"response\", se.fit = TRUE, newdata = df)\n\n# Add the model predictions and 95% COI to the dataframe.\ndf.pred.pre &lt;- df.pred.pre %&gt;%\n  mutate(se.fit = pred7$se.fit, pred = pred7$fit) %&gt;%\n  mutate(\n    conf.low = pred - 1.96 * se.fit,\n    conf.high = pred + 1.96 * se.fit\n  )\n\n\np_m3 &lt;- \n  ggplot(df.pred.pre, aes(x = date, y = J12_J18_prim)) +\n      geom_point() +\n      # Add the fitted line.\n      geom_line(data = df.pred.pre, aes(x = date, y = pred.pre),\n                color = \"#377eb8\") +\n      # Add the confidence interval.\n      geom_ribbon(data = df.pred.pre, aes(ymin = conf.low, ymax = conf.high),\n                  alpha = 0.2, fill = \"blue\") +\n      labs(title = \"Deaths with a Negative Binomial Fit\\nExtrapolating Based on the Pre-Vaccine Period\",\n         x = \"Date\", y = \"Counts for &lt;5 yo\") +\n      theme_linedraw()\n\np_m3\n\n\n\n\n\n\n\n\n\n# Inspect methods sensitivity to vaccine impact.\np_ratio &lt;- p_ratio +\n  geom_line(data = df.pred.pre, aes(x = date, y = rr.trend),\n            color = '#377eb8') +\n  # Update the title.\n  labs(title = \"Rate Ratio with the Extrapolation Model\")\n\np_ratio\n\n\n\n\n\n\n\n\n\n\nCompare the Methods\nLet’s compare the rate ratio estimates from the three models. As a rough estimate, we will average the point-by-point estimates of the rate ratio during the evaluation period.\n\n# Create a Boolean variable that defines the vaccine evaluation period.\neval.period &lt;- df$date &gt; vax.eval.date\n\n# Calculate the average outcome during the vaccine evaluation period.\nrr.its.eval    &lt;- mean(df.pred.its$rr.its.a[eval.period])\nrr.spline.eval &lt;- mean(df.pred.spl$rr.spline[eval.period])\nrr.trend.eval  &lt;- sum(df.pred.pre$J12_J18_prim[eval.period])/sum(df.pred.pre$pred.pre[eval.period])\n\n# Average decline detected from 1.\nround(c(\"ITS - Disconnected\" = rr.its.eval, \"ITS - Spline\" = rr.spline.eval,\n        \"Extrapolation\" = rr.trend.eval), digits = 2)\n\nITS - Disconnected       ITS - Spline      Extrapolation \n              0.91               0.90               0.86 \n\n# Percent decline detected.\n100*(1 - round(c(\"ITS - Disconnected\" = rr.its.eval, \"ITS - Spline\" = rr.spline.eval, \n                 \"Extrapolation\" = rr.trend.eval), digits = 2))\n\nITS - Disconnected       ITS - Spline      Extrapolation \n                 9                 10                 14 \n\n\nTwo of the modeling methods identified a decrease of approximately 10% in pneumonia deaths due to pneumococcal disease after the release of the PCV10 vaccine. The extrapolation method indicates a stronger vaccine effect; however, we expect some of this result to be spurious due to the level of random noise introduced by applying the observed values in the counterfactual ratio calculation.\n\n\n\n\n\n\nCaution\n\n\n\nDepending on the dataset, these three methods may or may not agree to a greater extent."
  },
  {
    "objectID": "Pages/Week3/Hierarchical Modeling.html",
    "href": "Pages/Week3/Hierarchical Modeling.html",
    "title": "Hierarchical Modeling",
    "section": "",
    "text": "Page still in progress!!"
  },
  {
    "objectID": "Pages/Week3/Hierarchical Modeling.html#introduction",
    "href": "Pages/Week3/Hierarchical Modeling.html#introduction",
    "title": "Hierarchical Modeling",
    "section": "Introduction",
    "text": "Introduction\nReal-world analytical problems are often complex, making it difficult to define or derive deterministic solutions. Two common approaches to support complex analytical projects are:\n\nApproximating with a known solvable function.\nUsing stochastic search and optimization, such as Monte Carlo methods.\n\nWhile the first approach has many advantages, there are circumstances where approximating a problem with a known distribution is computationally prohibitive, overly reductive, or infeasible. In such cases, we leverage abstract probabilistic and statistical concepts to transform randomness into inferential statistics. When applied correctly, these concepts can surprisingly guarantee robust solutions to otherwise unsolvable problems.\nOne of these powerful techniques is Markov chain Monte Carlo (MCMC). Random samples generated with MCMC can be used for various purposes, including computing statistical estimates, numerical integrals, and estimating the marginal or joint probabilities of multivariate distributions and densities 1. Notably, MCMC provides a means to generate samples from joint distributions by utilizing conditional, factorized distributions 1,2.\nTwo common MCMC implementations are Metropolis-Hastings (M-H) and Gibbs sampling, with the latter being more accurately considered a special case of the former. Students are encouraged to explore M-H further on their own by reading the relevant sections of the supporting textbooks listed below. Today, we will focus on Gibbs sampling and only discuss M-H where it supports this learning, as Gibbs sampling is particularly well-suited for reducing high-dimensional problems into several one-dimensional ones 1,3.\n\n\n\n\n\n\nSolid theory supports effective and accurate application!\n\n\n\nThe concepts discussed here have been compiled and summarized from various resources listed in the bibliography at the end of this page. We would like to specifically highlight the following textbooks, with suggested readings, to help students deepen their theoretical understanding of this week’s material. This foundation will aid in generating better analysis in practice.\n\nAll of Statistics: A Concise Course in Statistical Inference by Larry Wasserman 3\n\nChapter 23 Probability Redux: Stochastic Processes Section 23.2 Markov chains.\nChapter 24 Simulation Methods. Section 24.4 MCMC Part I: The Metropolis-Hastings Algorithm and Section 24.5 MCMC Part II: Different Flavors.\n\nIntroduction to Stochastic Search and Optimization: Estimation, Simulation, and Control by James C. Spall 1\n\nChapter 16 Markov chain Monte Carlo Section 16.1 Background, Section 16.2 Metropolis-Hastings Algorithm, and Section 16.3 Gibbs Sampling.\n\nProbabilistic Graphical Models - Principles and Techniques by Koller and Friedman 2\n\nChapter 3 The Bayesian Network Representation Section 3.1 Exploiting Independence Properties, Section 3.2 Bayesian Networks, and Section 3.3 Independencies in Graphs.\nChapter 4 Unirected Graphical Models.\nChapter 12 Particle-Based Approximate Inference Section 12.3 Markov Chain Monte Carlo Methods.\nChapter 17 Parameter Estimation Section 17.5 Learning Models with Shared Parameters."
  },
  {
    "objectID": "Pages/Week3/Hierarchical Modeling.html#distribution-estimation-by-particle-sampling",
    "href": "Pages/Week3/Hierarchical Modeling.html#distribution-estimation-by-particle-sampling",
    "title": "Hierarchical Modeling",
    "section": "Distribution Estimation by Particle Sampling",
    "text": "Distribution Estimation by Particle Sampling\n\n\n\n\nAbstraction of the introduction to Chapter 12 Particle-BasedApproximate Inference found in Probabilistic Graphical Models byKoller and Friedman. Points on the graph are only illustrativeof the variation in techniques 2.\n\n\n\nParticle-based methods approximate a multivariate distribution or network with a set of instantiations, referred to as particles, of some or all of the random variables in that distribution2. There are different flavors of this type of method illustrated on the right. Some particles are generated directly from an equation, while others are sampled from a distribution. Some particles represent all the variables in a distribution or network individually, while others assign a proxy variable that groups them 2.\nToday we are discussing one type of particle estimation: Gibbs sampling. In most cases, random variables are represented individually. However, readers should be aware that there are variations of Gibbs sampling that facilitate grouping the full set of random variables into “blocks”.\n\nRecall Markov Chains - A Stochastic Process\nThis section summarizes relevant concepts presented in Wasserman’s All of Statistics, specifically Chapter 23 3. Unless otherwise noted, the content comes from Wasserman’s All of Statistics.\nA stochastic process \\{X_t : t \\in T\\} is a collection of random variables taken from a state space, \\mathcal{X}, and indexed by an ordered set T, which can be interpreted as time. The index set T can be discrete, such as T = \\{0, 1, 2, \\dots\\}, or continuous, such as T = [0, \\infty). To denote the stochastic nature of X, it can be written as X_t or X(t).\n\n\n\n\nIllustration of state transition diagram for the Markov chain 4.\n\n\n\nA Markov chain is a type of stochastic process where the distribution of future states depends only on the current state and not on the sequence of events that preceded it. For example, the outcome for X_t depends only on what the outcome was for X_{t-1}. Notice that sometimes, authors with specify that X is a Markov chain stochastic process by writing X_n instead of X_t.\nThe figure on the left shows a state transition diagram for a discrete-time Markov chain with transition probabilities p_{ij} = \\mathbb{P}(X_{n+1}= j\\ \\lvert\\ X_n = i). Below the diagram is the matrix representation of these interactions, known as the transition matrix, \\mathbf{P}, where each row is a probability mass function. A generalized definition for a Markov chain is given by\n\n\\mathbb{P}(X_n = x\\ \\lvert\\ X_0, \\dots, X_{n-1}) = \\mathbb{P}(X_n = x\\ \\lvert\\ X_{n-1})\n\nfor all n and for all x \\in \\mathcal{X}. Markov chains have many aspects, but for this application, it is essential to consider the properties of a specific transitional stage denoted by, \\pi = (\\pi_i : i \\in \\mathcal{X}). The vector \\pi consists of non-negative numbers that sum to one and is a stationary (or invariant) distribution if \\pi = \\pi\\mathbf{P}.\nBy applying the Chapman-Kolmogorov equations for n-step probabilities, where it is shown that p_{ij}(m+n) = \\sum_k p_{ik}(m) p_{kj}(n), we derive that a key characteristic of a stationary distribution \\pi for a Markov chain is that it is limiting. This means that, after any number of steps n, a distribution that reaches \\pi remains as \\pi:\n\nDraw X_0 from the distribution \\pi gives \\mu_0 = \\pi.\nDraw X_1 from the distribution \\pi gives \\mu_1 = \\mu_0\\mathbf{P} = \\pi\\mathbf{P} = \\pi.\nDraw X_2 from the distribution \\pi gives \\pi\\mathbf{P}^2 = (\\pi\\mathbf{P})\\mathbf{P} = \\pi\\mathbf{P} = \\pi.\n\\dots etc.\n\nHere, \\mu_n(i) = \\mathbb{P}(X_n = i) denotes the marginal probability that the chain is in state i at time n, while \\mu_0 represents the initial distribution. Once the chain limits to the distribution \\pi, it will remain in this distribution indefinitely. However, ensuring the process is stationary is not sufficient; it is also important to guarantee that \\pi is unique. This uniqueness is guaranteed if the Markov chain is ergodic, which must satisfy the following two characteristics 3,5:\n\nAperiodicity: The states do not get trapped in cycles of fixed length. This means revisiting any state does not happen at regular intervals, ensuring more general behavior. For d = \\text{gcd}\\{n\\ :\\ p_{ii}(n) &gt; 0\\} then d(i) = 1, where gcd means “greatest common divisor”.\nPositive Recurrence: The expected number of steps to return to that state is finite. A Markov chain is positive recurrent if every state is positive recurrent.\n\nIf the Markov chain is also irreducible, then all states communicate, meaning it is possible to get from any state to any other state, denoted by i \\leftrightarrow j. Having an irreducible, ergodic Markov chain guarantees that it has a unique stationary distribution, \\pi.\n\n\n\n\n\n\nWarning\n\n\n\nA Markov chain with a stationary distribution does not mean that it converges.\n\n\nThere is much more to learn about Markov chains beyond what was discussed here, but these fundamental concepts provide a foundation for understanding their use in Monte Carlo methods.\n\n\nMotivation\nTraditionally, Monte Carlo methods rely on independent sampling. However, MCMC methods rely on a dependent Markov sequence with a limiting distribution that corresponds to the distribution we aim to model 1. MCMC has wide-ranging applications and is frequently used in Bayesian inference. However, it is also generalizable to other problems that are not Bayesian in nature 1.\nTo help you understand the connection between Monte Carlo sampling methods and estimating complex analytical solutions using Markov chains, let’s first explore the motivation: initially within the commonly exemplified Bayesian framework, and then in a general sense. Consider the following scenario provided by Wasserman in the context of Bayesian inference where we have a process that generates the random vector X 3. We have the prior distribution f(\\theta) with respect to parameter \\theta and data \\mathbf{X} = (X_1, \\dots, X_n) with a posterior density\n\nf(\\theta\\ \\lvert\\ \\mathbf{X}) = \\frac{\\mathcal{L}(\\theta)\\ f(\\theta)}{\\int \\mathcal{L}(\\theta)\\ f(\\theta)\\ d\\theta}\n\nwhere the denominator is the normalizing constant, here denoted by c. The mean of the posterior is \\begin{align*}\n\\bar{\\theta} &= \\int \\theta\\ f(\\theta\\ \\lvert\\ \\mathbf{X})\\ d\\theta \\\\\n&= \\frac{1}{c} \\int \\theta\\ \\mathcal{L}(\\theta)\\ f(\\theta)\\ d\\theta\n\\end{align*}\nIf the parameter space is multidimensional, i.e., \\boldsymbol{\\theta} = (\\theta_1, \\dots, \\theta_k), but we are only interested in one component, say \\theta_1, then the marginal posterior density of \\theta_1 is defined as \nf(\\theta_1\\ \\lvert\\ \\mathbf{X}) = \\int\\int\\cdots\\int f(\\theta_1, \\dots, \\theta_k\\ \\lvert\\ \\mathbf{X})\\ d\\theta_2 \\cdots\\ d\\theta_k\n\nAs can easily be seen, it might not always be feasible to calculate this outcome directly. Generally speaking, we have a random vector \\mathbf{X} and we wish to compute E[f(\\mathbf{X})] 1. If the random vector is continuous and associated with the probability density p(\\boldsymbol{x}), then we want to calculate \nE[f(\\mathbf{X})] = \\int f(\\boldsymbol{x})\\ p(\\boldsymbol{x})\\ d\\boldsymbol{x}\n\nThe density p(\\boldsymbol{x}) is often referred to as the target density, representing the distribution of the random variables that are of interest in our study 1.\n\n\n\n\n\n\nNote\n\n\n\nMarkov chains are typically reserved for discrete outcomes. However, general applications of MCMC can be used for discrete, continuous, or hybrid outcomes. In this context, Markov chains can be applied to any type of distribution 1.\n\n\nStandard Monte Carlo methods aim to draw n independent, identically distributed (i.i.d.) samples of \\mathbf{X} directly from p(\\boldsymbol{x}), but this is also not always feasible 1. Simple random generation methods require specific restrictions to ensure a solution or might be computationally inefficient in high-dimensional problems. For example, the adaptation of the accept-reject method from Monte Carlo Statistical Methods by Robert and Casella, called adaptive rejection sampling, is restricted to densities that are log-concave and has computational power requirements that are proportional to the fifth power of \\text{dim}(\\mathbf{X}) 1,6.\nCanonical Monte Carlo approaches are more restrictive than is necessary to estimate E[f(\\mathbf{X})]. Leveraging Markov chain-based schemes avoids the necessity of sampling directly from p(\\boldsymbol{x}); instead, dependent sequences with a limiting distribution of p(\\boldsymbol{x}) are used 1.\nRecall that in the previous section, we showed that a Markov chain that is irreducible and ergodic is guaranteed to have a unique stationary distribution, \\pi 3. This is a consequence of the ergodic theorem, which guarantees that the normalized sum below will approach the mean of f(\\mathbf{X}), with convergence in mean square (m.s.) or almost surely (a.s.) as n \\rightarrow \\infty for any fixed M 1.\n\n\\frac{1}{n-M} \\sum_{k = M+1}^n\\ f(\\mathbf{X}_k)\n\nHere, M is referred to as the “burn-in” period. We know that the dependence of \\mathbf{X}_k on early states, say \\mathbf{X}_0, \\mathbf{X}_1, \\dots, \\mathbf{X}_M, for bounded M &lt; \\infty, disappears as k \\rightarrow \\infty 1. With sufficient burn-in, we achieve the ergodic average that stabilizes at \\pi, and when executed correctly, \\pi equals the target density to within a scalar value 1,3.\n\n\n\n\n\n\nTypes of Convergence\n\n\n\n\n\nThis excerpt summarizes Spall’s Introduction to Stochastic Search and Optimization: Estimation, Simulation, and Control Appendix C.2 Convergence Theory 1.\nFinite-sampled stochastic algorithms are often intractable, justifying the focus on the behavior of stochastic processes in their limit. The type of convergence properties a process has informs us about the scenarios where it is appropriate to apply the theory in practice and provides insight into non-limiting behaviors. Convergence for stochastic algorithms is probabilistic and characterized by measure-theoretic concepts, often classified into the following four types:\n\n\\gdef\\norm#1{\\left\\lVert#1\\right\\rVert}\n\nConvergence almost surely: The sequence of random vectors \\mathbf{X}_k \\rightarrow \\mathbf{X} a.s. (or with probability 1)\n\nP\\left(\\omega \\in \\Omega \\ : \\ \\lim_{k \\rightarrow \\infty}\\ \\norm{\\mathbf{X}_k(\\omega) - \\mathbf{X}(\\omega)} = 0\\right) = 1\n\nConvergence in probability: The sequence of random vectors \\mathbf{X}_k \\rightarrow \\mathbf{X} in pr.\n\n\\lim_{k \\rightarrow \\infty} P\\left(\\omega \\in \\Omega \\ : \\ \\norm{\\mathbf{X}_k(\\omega) - \\mathbf{X}(\\omega)} \\geq c\\right) = 0\\ \\ \\ \\text{ for any } c &gt; 0\n\nConvergence in mean square: The sequence of random vectors \\mathbf{X}_k \\rightarrow \\mathbf{X} in m.s.\n\n\\lim_{k \\rightarrow \\infty} E\\left(\\norm{\\mathbf{X}_k(\\omega) - \\mathbf{X}(\\omega)}^2\\right) = 0\n\nConvergence in distribution: The sequence of random vectors \\mathbf{X}_k \\rightarrow \\mathbf{X} in m.s.\n\n\\lim_{k \\rightarrow \\infty} F_{\\mathbf{X}_k}(\\boldsymbol{x}) = F_{\\mathbf{X}}(\\boldsymbol{x})\\ \\ \\ \\text{ at every point } \\boldsymbol{x} \\text{ where } F_{\\mathbf{X}}(\\boldsymbol{x}) \\text{ is continuous.}\n\nwhere \\norm{\\cdot} denotes any valid norm, i.e., the Euclidean norm \\norm{\\boldsymbol{x}} = \\sqrt{\\sum_i x_i^2}, and x_i is the i^{th} component of the vector \\boldsymbol{x}.\n\n\n\nIn the next section, we will show how Gibbs sampling produces the correct mean, computed with respect to our target density p(\\boldsymbol{x}). In summary, we have thus far introduced the basic concepts around Markov chains that simplify random sampling from conditional distributions, which are guaranteed to converge if irreducible and ergodic. With the appropriate burn-in, M, we can ensure the ergodic average stabilizes at \\pi. We now want to show that Gibbs sampling is capable of producing a stabilized limiting distribution, \\pi, that is proportional to our target density p(\\boldsymbol{x}).\n\n\nA Brief Interlude in Probabilistic Graphical Modeling\nThe concept of Gibbs sampling was introduced by Geman and Geman in their paper Stochastic Relaxation, Gibbs Distributions, and the Bayesian Restoration of Images 1,5,7. In their work, they identified the equivalence between Markov random fields and the Gibbs distribution, also known as the Boltzmann distribution, which had been previously used in physical chemistry to relate the probability of a system’s state to its energy and temperature 7,8.\n\n\n\n\n\n\nJosiah Gibbs: Yale’s First Engineering Ph.D. and Chair of Mathematical Physics\n\n\n\nGibbs sampling is named after the American mechanical engineer and scientist, Josiah Willard Gibbs, who received the first Ph.D. in engineering in the United States in 1863 from Yale University. He was also appointed the first chair of mathematical physics in the United States at Yale, and contributed to many areas in physics, chemistry, and mathematics. Gibbs is credited with laying the foundation for the field of physical chemistry 5,9.\n\n\nFundamentally, this is why Gibbs sampling has a natural intuition tied to graphical modeling, a broader subject that we will only lightly touch upon in this week’s concept. Some sources attempt to explain Gibbs sampling outside of the graphical model framework, but the more comprehensive ones will acknowledge this connection. It is beyond our scope to bring students entirely up to speed with graphical theory, so additional reading has been provided earlier to help bridge this gap if readers find any of the content too advanced.\nIn practice, especially in medical diagnosis problems, there may be hundreds of attributes that contribute to the joint probability distribution for a given disease 2. Furthermore, some of these distributions may overlap or have similarities with those of other diseases, making them challenging to separate using standard probabilistic approaches. Probabilistic graphical models are graph-based representations of variable interactions, which can help to succinctly represent these interactions in digestible pieces despite their high dimensionality 2.\nMore impressively, graphical modeling encodes interaction attributes such as independencies, factorization, pathways, and message propagation. This not only provides an intuitive understanding of complex interactions but also allows for the exploitation of these features for inference and efficient computation 2,5. Shown below are two fundamental ways in which variables and their relationships with one another can be represented: a Bayesian network, also referred to as a Directed Graph, and a Markov network, also known as an Undirected Graph.\n\n\n\nModified version of Figure 1.1 from Probabilistic Graphical Models by Koller and Friedman 2.\n\n\nThe key concept exemplified by the figure above is independence properties. These can differ for the same graph depending on whether the relationships between variables are defined as directed or undirected, but ultimately allow for factorization representing these independence assertions. The notation differences for factorized joint distributions, shown at the bottom of the figure, are used to differentiate the types of dependency structures implied by the graph itself.\nBy definition, the Markov network factorization is a Gibbs distribution parameterized by the set of factors \\Phi = {\\phi(A, B), \\phi(B, C), \\phi(C, D), \\phi(A, D)} with the normalizing constant called the partition function\n\nZ = \\sum_{X_1, \\dots, X_n}\\ \\tilde{P}_{\\Phi}(X_1, \\dots, X_n)\n where \\tilde{P}_{\\Phi} represents the joint distribution factorized into conditional probabilities based on its independencies 2.\n\n\n\n\n\n\nFactor \\neq Marginal\n\n\n\nA factor is one contribution to the overall joint distribution, while a marginal represents a distribution where at least one variable from the joint has been summed or integrated out 2.\n\n\nAlthough we have discussed Markov chains earlier and provided the explicit definition of a Gibbs distribution in the context of probabilistic graphical modeling, this does not imply that Gibbs sampling is exclusively applicable to undirected graphs 5. How the differences affect Gibbs sampling is discussed in the next section.\n\n\nGibbs Sampling\nIn the M-H algorithm, we start with a proposed distribution, q(\\cdot\\ \\lvert x), from which we can sample, but which is not directly the target distribution, p(\\boldsymbol{x}). In many cases, it may be chosen arbitrarily, as long as its limiting behaviors converge to the target distribution 1. For MCMC conducted using the M-H algorithm to work, we need to ensure \\pi satisfies detailed balance, where the probability of transitioning from state i to j must be equal to the probability of the reverse transition when multiplied by their respective stationary distribution probabilities 3.\n\n\\pi_i p_{ij} = \\pi_j p_{ji}\n\nAt each step in the algorithm, a candidate point is drawn from the proposal distribution, denoted using the random variable \\mathbf{W}. Assuming that this proposal distribution satisfies detailed balance and is a superset of the points for the target distribution (ensuring they are fully supported), then candidate draws are kept or discarded with probability \\rho(\\mathbf{X}_k, \\mathbf{W}) 1\n\n\\rho(\\boldsymbol{x}, \\boldsymbol{w}) = \\min\\left\\{\\frac{p(\\boldsymbol{w})}{p(\\boldsymbol{x})} \\frac{q(\\boldsymbol{x}\\ \\lvert\\ \\boldsymbol{w})}{q(\\boldsymbol{w}\\ \\lvert\\ \\boldsymbol{x})},\\ 1\\right\\}\n\nIf a candidate point is accepted, then \\mathbf{X}_{k+1} = \\mathbf{W}; otherwise, \\mathbf{X}_{k+1} = \\mathbf{X}_k 1.\n\n\n\n\n\n\nNote\n\n\n\nWhen samples from the Markov chain start to resemble the target distribution, it is said to be “mixing well” 3. Tweaking the parameters used in the proposed distribution to support good mixing is not always straightforward and may require some trial-and-error troubleshooting; refer to example 24.10 from Wasserman’s All of Statistics for an illustration of this.\n\n\nGibbs sampling is much like M-H, but it is a special case where the proposal distribution comes directly from the conditional factorizations of p(\\boldsymbol{x}) itself. In a sense, it can be thought of as m M-H algorithms working together, where each proposed distribution is one conditional factorization of p(\\boldsymbol{x}) 1. In this context, m represents the number of random variables, each having its own conditional distribution for sampling. Consider the following scenario from Spall where we have a distribution p_{X,Y,Z}(\\cdot) that we wish to sample from:\n\nStart with an initial guess for Y and Z and sample from p_{X \\mid Y_0, Z_0}(x \\mid Y_0, Z_0) to generate X_1.\nAccept the outcome from the previous step and sample from p_{Y \\mid X_1, Z_0}(y \\mid X_1, Z_0) to generate Y_1.\nAccept the outcome from the previous step and sample from p_{Z \\mid X_1, Y_1}(z \\mid X_1, Y_1) to generate Z_1.\nRepeat the process until you have n-1 values for X and n values for Y and Z.\n\nAt the end of this process, you will have produced the Gibbs sequence. For the example above, we produce:\n\nY_0, Z_0;\\ X_1, Y_1, Z_1;\\ X_2, Y_2, Z_2;\\ \\dots; X_n, Y_n, Z_n\n\nThe process described above is iterated over all variables and conditional statements until it is possible to compute the ergodic average. According to the ergodic theorem, this occurs when a sufficient burn-in period has been allowed to run. Using the example above, if we set our burn-in period to M &lt; n, then the appended Gibbs sequence {X_{M+1}, Y_{M+1}, Z_{M+1}; \\dots; X_n, Y_n, Z_n} represents measurements that converge in distribution as n \\rightarrow \\infty to the unique, stationary distribution \\pi, that is p_{X,Y,Z}(\\cdot) 1.\nFinally, the Gibbs sequence results considering the burn-in are substituded into the following equation to give E[f(X,Y,Z)]\n\n\\frac{1}{n-M} \\sum_{k = M+1}^n\\ f(\\mathbf{X}_k)\n\nNotice that samplings for X, Y, and Z will converge to their respective marginal distributions, p_X(x), p_Y(y), and p_Z(z); see Section 7.1.3 from Monte Carlo Statistical Methods by Robert and Casella 1,6.\n\n\n\n\n\n\nVariations that make the Gibbs sequence resemble i.i.d.\n\n\n\nThe procedure we have detailed here to estimate the distribution using the collected Gibbs sequence is one of several methods. As might be expected, successive samples drawn from the Markov chain are highly correlated with one another 5. Different approaches are applied if it is desirable to make the values more i.i.d. You can learn more about some of these techniques in Introduction to Stochastic Search and Optimization: Estimation, Simulation, and Control, Section 16.3 Gibbs Sampling by Spall 1.\n\n\nBecause we are drawing from the conditioned factors of the target distribution, this makes Gibbs sampling less generalizable than M-H, but it comes with advantageous trade-offs. For instance, drawing directly from the target distribution itself ensures that detailed balance is already satisfied, and the algorithm is expected to be more efficient because the new point is always accepted 1,2,5.\nIt is helpful to see the process illustrated for a simpler case, but for completeness, it’s important to know how this extends to a generalized representation. The following is the generalized definition provided by Spall 1. Say that \\mathbf{X} is a collection of m univariate components, then the k^{th} sample from X using the previous algorithm is\n\n\\mathbf{X}_k = \\left[ \\begin{array}{c}\nX_{k1} \\\\\nX_{k2} \\\\\n\\vdots \\\\\nX_{km}\n\\end{array} \\right]\n\nwhere X_{ki} is the i^{th} component for the k^{th} replicate of \\mathbf{X} that was generated using Gibbs sampling. Denote the step-wise updating results that have been conditioned on the previous samples by \\{X_{k+1,\\ i}\\ \\lvert\\ \\mathbf{X}_{k\\backslash i}\\} where\n\n\\mathbf{X}_{k\\backslash i}\\ \\equiv \\{X_{k+1,\\ 1},\\ X_{k+1,\\ 2}, \\dots, X_{k+1,\\ i-1},\\ X_{k,\\ i+1} \\dots,\\ X_{k,\\ km}\\}\n\nfor i = 1,\\ 2, \\dots,\\ m. Use {X_{k+1,\\ i} \\mid \\mathbf{X}{k\\backslash i}} \\sim p_i(x \\mid \\mathbf{X}{k\\backslash i}) to represent the sampling density for that random variable conditioned on the current elements of the Gibbs sequence, where i-1 elements are sample points at iteration k+1 and the remaining m-i elements are from the k^{th} iteration. The Gibbs sampling algorithm is as follows:\n\n(Initialization) Choose the length of the burn-in, M, and some arbitrary initial state \\mathbf{X}_0 with k=0.\nGenerate \\mathbf{X}_{k+1} by recursively updating the conditionals for each new sampling of a given variable.\n\nGenerate X_{k+1,\\ 1} \\sim p_1(x\\ \\lvert\\ \\mathbf{X}_{k\\backslash 1})\nGenerate X_{k+1,\\ 2} \\sim p_2(x\\ \\lvert\\ \\mathbf{X}_{k\\backslash 2})\n\n              \\vdots\n\nGenerate X_{k+1,\\ m} \\sim p_m(x\\ \\lvert\\ \\mathbf{X}_{k\\backslash m})\n\nRepeat step 2 until the burn-in is reached. Set \\mathbf{X}_k = \\mathbf{X}_M and continue with the procedure laid out in step 2, storing or recursively averaging the results as they are sampled.\nWhen it is possible to compute the ergodic average (i.e. sufficient convergence is observed), then estimate E[f(\\mathbf{X})] for the target distribution p(\\cdot).\n\n\n\n\n\n\n\nExtension to the multivariate case\n\n\n\nWe are only focusing on the typical case where X_{ki} is scalar (i.e., all components of X are univariate). It is sometimes applicable to make them multivariate. Students who wish to explore this further are encouraged to review example 16.7 Variance-components model from Section 16.6 Applications in Bayesian Analysis in Spall’s Introduction to Stochastic Search and Optimization: Estimation, Simulation, and Control 1.\n\n\nWhen sampling from the conditional distribution is challenging or requires excessive computational power, the conditional properties associated with a distribution network graph can help 2. Depending on the type of associations defined, it might be possible to isolate variables from the rest of the network using its independence structures, assuming that these assertions are reasonably correct.\nFor an undirected Markov network, this would involve isolating nodes contained within a Markov blanket. In directed graphs, nodes conditioned on their parents can lead to log-concave Gibbs sampling distributions. If these conditional distributions are log-concave, alternative sampling methods, such as the aforementioned adaptive rejection sampling method, can be used within the Gibbs sampling framework 5,6.\nWith this fundamental understanding of Gibbs sampling of Markov chains, we are now better prepared to explore hierarchical modeling of our data."
  },
  {
    "objectID": "Pages/Week3/Hierarchical Modeling.html#hierarchical-modeling",
    "href": "Pages/Week3/Hierarchical Modeling.html#hierarchical-modeling",
    "title": "Hierarchical Modeling",
    "section": "Hierarchical Modeling",
    "text": "Hierarchical Modeling\nGibbs sampling is inherently suited to address multivariate problems because it breaks a high-dimensional vector into lower-dimensional component blocks 1."
  },
  {
    "objectID": "Pages/Week3/Hierarchical Modeling.html#set-up-the-environment",
    "href": "Pages/Week3/Hierarchical Modeling.html#set-up-the-environment",
    "title": "Hierarchical Modeling",
    "section": "Set Up the Environment",
    "text": "Set Up the Environment\n\nInstalling JAGS\nThe rjags package requires JAGS to be installed separately. rjags will reference the JAGS libjags.4.dylib library file and the modules-4 directory, which contains seven *.so files, such as bugs.so. The next steps will walk you through installation, and an optional troubleshooting guide if R cannot find the JAGS file path.\n\nMacPC\n\n\n\nFrom JAGS, open the download link that will take you to a SourceForge page.\nDownload the latest version. At the time of this writing, the current version is JAGS-4.3.2.\nOpen and run the downloaded installer.\nConfirm that the installation is located where we need it to be: /usr/local/lib/.\n\n\nCommand-Line Application\n\n# Running both should NOT give you \"No such file or directory\"\nls -l /usr/local/lib/libjags.4.dylib\nls -l /usr/local/lib/JAGS/modules-4/\n\n\n\n\n\n\n\n\nIf the namespace load failed\n\n\n\n\n\nSometimes, the installer might place the JAGS program in a different location. For example, if you installed JAGS using Homebrew, the program might have been placed in the Homebrew library. If the file path differs, you will need to adjust the following code accordingly before reinstalling it in R.\n\n\nCommand-Line Application\n\n# This example assumes that Homebrew was used\nbrew install jags\n\n\nOpen /usr/local/lib by searching for it with “Go to Folder” in an open Finder Window. If it does not already exist, create the file path using the following code.\n\n\nCommand-Line Application\n\n# ONLY IF there is no existing directory\nsudo mkdir -p /usr/local/lib\n\nCreate a symbolic link from the Homebrew-installed libjags.4.dylib JAGS library to the expected location:\n\n\nCommand-Line Application\n\nsudo ln -s /opt/homebrew/lib/libjags.4.dylib /usr/local/lib/libjags.4.dylib\n\nCreate another symbolic link for the Homebrew-installed JAGS modules directory to the expected location:\n\n\nCommand-Line Application\n\nsudo mkdir -p /usr/local/lib/JAGS/modules-4\nsudo ln -s /opt/homebrew/lib/JAGS/modules-4/* /usr/local/lib/JAGS/modules-4/\n\nOPTIONAL: You can verify the linking worked if you see the intended directory listed for both of the following lines of code.\n\n\nCommand-Line Application\n\n# Running both should NOT give you \"No such file or directory\"\nls -l /usr/local/lib/libjags.4.dylib\nls -l /usr/local/lib/JAGS/modules-4/\n\nReinstalling rjags with the connection in place.\n\n\nRStudio\n\n# Remove existing installation\nremove.packages(\"rjags\")\n\n# Reinstall rjags\ninstall.packages(\"rjags\")\n\n# Load the package\nlibrary(rjags)\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThis page was developed on a Mac, and so the directions for a PC were not able to be tested.\n\n\n\nFrom JAGS, open the download link that will take you to a SourceForge page.\nDownload the latest version. At the time of this writing, the current version is JAGS-4.3.2.\nOpen and run the downloaded installer.\nYou will need to also download the latest version of RTools, which at the time of this writing is RTools 4.5. Add Rtools to your PATH if it is not done automatically.\n\n\nCommand-Line Application\n\nwhere make   # Should give the result \"C:\\Rtools\\bin\\make.exe\"\necho %PATH%  # Should give the result \"C:\\Rtools\\bin;C:\\Rtools\\mingw_64\\bin\"\n\nIf this does not give the expected results, you can set the path:\n\n\nCommand-Line Application\n\n# ONLY IF the PATH for Rtools is incorrect.\nsetx PATH \"%PATH%;C:\\Rtools\\bin;C:\\Rtools\\mingw_64\\bin\"\n\nConfirm that the installation is located where we need it to be: C:\\Program Files\\JAGS\\JAGS-4.x\\bin.\n\n\nCommand-Line Application\n\necho %JAGS_HOME%  # Should say \"C:\\Program Files\\JAGS\\JAGS-4.x\"\necho %PATH%       # Should say \"...;C:\\Program Files\\JAGS\\JAGS-4.x\\bin;...\"\n\n\n\n\n\n\n\n\nIf the namespace load failed\n\n\n\n\n\nSometimes the installer might place the JAGS program in a different location. If this happens, you can set the file path in the command line application as follows and retry installing the package through R.\n\nModify the environment variables as needed.\n\n\nCommand-Line Application\n\nsetx JAGS_HOME \"C:\\Program Files\\JAGS\\JAGS-4.x\"\nsetx PATH \"%PATH%;C:\\Program Files\\JAGS\\JAGS-4.x\\bin\"\n\nOPTIONAL: You can verify the setting worked.\n\n\nCommand-Line Application\n\necho %JAGS_HOME%  # Should say \"C:\\Program Files\\JAGS\\JAGS-4.x\"\necho %PATH%       # Should say \"...;C:\\Program Files\\JAGS\\JAGS-4.x\\bin;...\"\n\nReinstalling rjags with the connection in place.\n\n\nRStudio\n\n# Remove existing installation\nremove.packages(\"rjags\")\n\n# Reinstall rjags\ninstall.packages(\"rjags\")\n\n# Load the package\nlibrary(rjags)\n\n\n\n\n\n\n\n\nWith JAGS installed, we can now proceed with the standard process of installing the necessary packages. If you have not done so already, you will need to initialize the project’s renv lockfile to ensure that the same package versions are installed in the project’s local package library.\n\n\n\nRStudio\n\nrenv::init()      # Initialize the project     \nrenv::restore()   # Download packages and their version saved in the lockfile\n\n\n\n\n\nRStudio\n\nsuppressPackageStartupMessages({\n  library(\"readr\")      # For reading in the data\n  library(\"tibble\")     # For handling tidyverse tibble data classes\n  library(\"tidyr\")      # For tidying data \n  library(\"dplyr\")      # For data manipulation \n  library(\"stringr\")    # For string manipulation\n  library(\"MASS\")       # Functions/datasets for statistical analysis\n  library(\"lubridate\")  # For date manipulation\n  library(\"ggplot2\")    # For creating static visualizations\n  library(\"viridis\")    # For color scales in ggplot2\n  library(\"scales\")     # For formatting plots axis\n  library(\"gridExtra\")  # Creates multiple grid-based plots\n  library(\"rjags\")      # For running JAGS (Just Another Gibbs Sampler) models\n  library(\"coda\")       # For analysis of MCMC output from Bayesian models\n})\n\n\n# Function to select \"Not In\"\n'%!in%' &lt;- function(x,y)!('%in%'(x,y))\n\n\nTo demonstrate hierarchical modeling using Gibbs sampling to estimate the distribution, we are going to examine the data and results from Potential Impact of Higher-Valency Pneumococcal Conjugate Vaccines Among Adults in Different Localities in Germany by Ellingson et al 10.\nThe data comes from surveillance for invasive pneumococcal disease (IPD) conducted by the German National Reference Center for Streptococci (GRNCS), which collects voluntarily reported microbial diagnostic lab results and is expected to account for 50% of all IPD in Germany. Serotyping is done by GNRCS using the samples provided by these laboratories, and incidences are reported as cases per 100,000 using the age-specific population in Germany in 2018 10.\n\n\n# Read in the cleaned data directly from the instructor's GitHub.\ndf &lt;- read_csv(\"https://raw.githubusercontent.com/ysph-dsde/bdsy-phm/refs/heads/main/Data/testDE2.csv\")\n\n# Summarize the variable names included in the dataset.\ncolnames(df)\n\n [1] \"...1\"                             \"Sequencial.number\"               \n [3] \"Study\"                            \"Number\"                          \n [5] \"Inclusion\"                        \"DateOfIsolation\"                 \n [7] \"DateOfBirth\"                      \"AgeInYears\"                      \n [9] \"AgeinMonths\"                      \"Postleitzahl.patient\"            \n[11] \"Zip\"                              \"Federal.state.patient\"           \n[13] \"Residence.patient\"                \"Country.patient\"                 \n[15] \"SeroType\"                         \"Strain.identification...Genus\"   \n[17] \"Strain.identification...Species\"  \"Pneumonia\"                       \n[19] \"MD.Amoxicillin\"                   \"AmoxicillinSNS\"                  \n[21] \"MD.Cefotaxime\"                    \"CefotaximeSNS\"                   \n[23] \"MD.Chloramphenicol\"               \"ChloramphenicolSNS\"              \n[25] \"MD.Clindamycin\"                   \"ClindamycinSNS\"                  \n[27] \"MD.Erythromycin\"                  \"ErythromycinSNS\"                 \n[29] \"MD.Levofloxacin\"                  \"LevofloxaxinSNS\"                 \n[31] \"MD.Penicillin\"                    \"PenicillinSNS\"                   \n[33] \"MD.Tetracyclin\"                   \"TetracyclineSNS\"                 \n[35] \"MD.Trimetoprim.Sulfamethoxazole\"  \"Sulfamethoxazole.trimethoprimSNS\"\n[37] \"MD.Vancomycin\"                    \"VancomycinSNS\"                   \n[39] \"date\"                             \"year\"                            \n[41] \"month\"                            \"epiyr\"                           \n[43] \"agec\"                             \"vt\"                              \n[45] \"vaxperiod\"                        \"zip2\"                            \n[47] \"indic\"                           \n\n\n\nThe dataset includes a lot of information that is not required for our analysis today. We will, therefore, focus only on the variables used in the hierarchical model itself.\n\n# Summarize aspects and dimentions of our dataset.\nglimpse(df[, c(39:42, 45, 8, 15)])\n\nRows: 17,653\nColumns: 7\n$ date       &lt;date&gt; 2003-01-15, 2003-01-15, 2003-01-15, 2003-01-15, 2003-01-15…\n$ year       &lt;dbl&gt; 2003, 2003, 2003, 2003, 2003, 2003, 2003, 2003, 2003, 2003,…\n$ month      &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ epiyr      &lt;dbl&gt; 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002, 2002,…\n$ vaxperiod  &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ AgeInYears &lt;dbl&gt; 40, 63, 77, 74, 84, 89, 79, 53, 73, 42, 50, 69, 71, 72, 40,…\n$ SeroType   &lt;chr&gt; \"1\", \"3\", \"4\", \"19F\", \"14\", \"7F\", \"7C\", \"12F\", \"4\", \"4\", \"5…\n\n\n\n\nData Dictionary\nThis paper did not explicitly describe each variable in a “Data Dictionary”. The following was assembled based on the context and methods provided in the paper, other like papers by the same authors, and its supplementary materials 10,11.\n\ndate: The date when the events were recorded. It is not clear if this is the date GNRCS received the samples or when they were collected at the participating lab.\nyear: The year from the date column.\nmonth: The month from the date column.\nepiyr: The epidemiological year of the observation (MMWR definition) 12.\nvaxperiod: Numerical encoding differentiating years where the PCV vaccine recommendations differed, with approximately a one-year latency time built in. It is not clear what each phase denotes, but based on other papers by the same authors, they appear to mean 11:\n\nvaxperiod = 1 The period before Germany instituted its recommendation for all infants to receive 4 PCV doses in July 2006. The specific formula was not enforced and could either have been the 10-valent or 13-valent PCV. Spans 2001 to 2007.\nvaxperiod = 2 The period following introduction of PCV in infants in 2006 and before the updated recommendation for infants to receive 2 primary doses and a booster dose. Spans 2007 to 2015.\nvaxperiod = 3 The period following the updated recommendation in 2015. Spans 2015 to 2018.\nNOTE: 2019 is the last year reported in the dataset an was not assigned to any vaccine period.\n\nAgeInYears: The age of the patient from which the same was collected.\nSeroType: The serotype of the pathogen identified using capsular swelling with the full antiserum.\n\n\nAggregating the data to count the number of cases for each serotype-year combination.\nEnsuring that all possible serotype-year combinations are represented.\nCreating numeric IDs for SeroType and epiyr.\nCreating a list of data objects required for running the JAGS model, including case counts, numeric IDs, and the number of unique serotypes and years.\n\n\n# Aggregate the data by SeroType and epiyr to get the number of cases \n# for each combination. The function complete() ensures all combinations \n# of SeroType and epiyr are represented in the dataset, even those with \n# zero cases.\nserotype_year_counts &lt;- df %&gt;%\n  count(SeroType, epiyr, name = \"cases\") %&gt;%\n  complete(SeroType, epiyr, fill = list(cases = 0))\n\n# Convert the categorical variables SeroType and epiyr into numeric IDs \n# which are easier to work with in JAGS.\nserotype_year_counts &lt;- serotype_year_counts %&gt;%\n  mutate(sero_id = as.integer(factor(SeroType)),\n         year_id = as.integer(factor(epiyr)))\n\n# Create the data list that will be input into JAGS.\njdat &lt;- list(\n  N = nrow(serotype_year_counts),\n  cases = serotype_year_counts$cases,\n  sero_id = serotype_year_counts$sero_id,\n  year_id = serotype_year_counts$year_id,\n  n_sero = length(unique(serotype_year_counts$sero_id)),\n  n_year = length(unique(serotype_year_counts$year_id))\n)\n\n\nsource(\"Model2.R\")\n\nmod &lt;- jags.model(textConnection(jcode), data = jdat, n.chains = 2)\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 1577\n   Unobserved stochastic nodes: 166\n   Total graph size: 9656\n\nInitializing model\n\n\n\n\nupdate(mod, 1000)  # burn-in\nsamp &lt;- coda.samples(mod, variable.names = c(\"mu\", \"beta\"), n.iter = 5000)\n\nsummary(samp)\n\n\nIterations = 2001:7000\nThinning interval = 1 \nNumber of chains = 2 \nSample size per chain = 5000 \n\n1. Empirical mean and standard deviation for each variable,\n   plus standard error of the mean:\n\n               Mean        SD  Naive SE Time-series SE\nbeta[1]  -8.483e-03  0.007019 7.019e-05      9.277e-05\nbeta[2]   1.308e-01  0.009273 9.273e-05      1.657e-04\nbeta[3]   1.359e-01  0.045234 4.523e-04      8.893e-04\nbeta[4]   8.522e-03  0.100215 1.002e-03      1.326e-03\nbeta[5]   1.253e-01  0.009906 9.906e-05      1.816e-04\nbeta[6]   7.451e-02  0.082080 8.208e-04      1.263e-03\nbeta[7]  -5.324e-02  0.120523 1.205e-03      1.697e-03\nbeta[8]   1.394e-01  0.041883 4.188e-04      7.871e-04\nbeta[9]  -8.979e-02  0.156193 1.562e-03      2.769e-03\nbeta[10]  1.788e-01  0.008002 8.002e-05      1.903e-04\nbeta[11]  1.040e-01  0.045346 4.535e-04      7.570e-04\nbeta[12] -4.639e-02  0.007302 7.302e-05      9.706e-05\nbeta[13]  2.028e-01  0.012658 1.266e-04      3.163e-04\nbeta[14]  1.425e-01  0.014258 1.426e-04      2.809e-04\nbeta[15]  1.496e-01  0.019428 1.943e-04      3.753e-04\nbeta[16]  1.049e-01  0.073125 7.313e-04      1.231e-03\nbeta[17]  2.026e-01  0.019338 1.934e-04      4.996e-04\nbeta[18]  2.730e+00  1.349469 1.349e-02      2.377e-01\nbeta[19]  1.707e-01  0.019377 1.938e-04      4.194e-04\nbeta[20] -1.677e-01  0.255076 2.551e-03      5.455e-03\nbeta[21]  1.159e-01  0.035792 3.579e-04      6.357e-04\nbeta[22]  1.424e-01  0.170356 1.704e-03      3.485e-03\nbeta[23]  1.213e-02  0.014501 1.450e-04      1.757e-04\nbeta[24] -1.706e-01  0.174780 1.748e-03      3.891e-03\nbeta[25]  1.008e-01  0.005811 5.811e-05      9.625e-05\nbeta[26]  2.733e-01  0.313562 3.136e-03      1.101e-02\nbeta[27] -3.557e-01  0.343016 3.430e-03      1.437e-02\nbeta[28]  5.292e-02  0.009806 9.806e-05      1.288e-04\nbeta[29]  1.563e-01  0.015261 1.526e-04      3.191e-04\nbeta[30]  2.870e-01  0.076889 7.689e-04      2.790e-03\nbeta[31]  2.947e-02  0.087817 8.782e-04      1.167e-03\nbeta[32] -1.730e-01  0.175933 1.759e-03      4.172e-03\nbeta[33]  1.451e-01  0.006552 6.552e-05      1.300e-04\nbeta[34]  1.631e-01  0.010698 1.070e-04      2.258e-04\nbeta[35]  3.468e-01  0.343361 3.434e-03      1.416e-02\nbeta[36]  2.033e-01  0.012476 1.248e-04      3.082e-04\nbeta[37] -1.837e-01  0.267050 2.671e-03      6.534e-03\nbeta[38] -1.938e-02  0.011096 1.110e-04      1.497e-04\nbeta[39]  1.035e-01  0.104606 1.046e-03      1.731e-03\nbeta[40]  4.075e-02  0.117601 1.176e-03      1.562e-03\nbeta[41]  1.514e-01  0.011260 1.126e-04      2.273e-04\nbeta[42]  1.236e-02  0.112506 1.125e-03      1.457e-03\nbeta[43] -6.543e-03  0.097496 9.750e-04      1.247e-03\nbeta[44]  1.329e+00  0.820724 8.207e-03      9.366e-02\nbeta[45]  3.302e+00  2.113212 2.113e-02      6.809e-01\nbeta[46]  1.784e-01  0.033175 3.318e-04      7.319e-04\nbeta[47]  1.937e-01  0.056789 5.679e-04      1.442e-03\nbeta[48]  9.835e-03  0.067714 6.771e-04      8.771e-04\nbeta[49]  1.412e-01  0.004052 4.052e-05      8.133e-05\nbeta[50]  1.739e-01  0.016660 1.666e-04      3.786e-04\nbeta[51] -1.340e-01  0.057368 5.737e-04      1.157e-03\nbeta[52] -1.100e-01  0.155505 1.555e-03      2.787e-03\nbeta[53]  1.334e-01  0.012897 1.290e-04      2.393e-04\nbeta[54]  1.378e-01  0.023408 2.341e-04      4.435e-04\nbeta[55]  7.706e-02  0.066855 6.686e-04      1.002e-03\nbeta[56]  1.901e-01  0.017320 1.732e-04      4.117e-04\nbeta[57]  4.464e-02  0.058435 5.844e-04      7.961e-04\nbeta[58]  1.536e-01  0.013067 1.307e-04      2.560e-04\nbeta[59]  1.646e-01  0.050787 5.079e-04      1.087e-03\nbeta[60]  1.614e-01  0.015284 1.528e-04      3.200e-04\nbeta[61]  8.092e-04  0.235790 2.358e-03      3.175e-03\nbeta[62]  3.612e-03  0.008478 8.478e-05      1.061e-04\nbeta[63] -5.305e-02  0.043966 4.397e-04      6.163e-04\nbeta[64]  4.906e-03  0.010861 1.086e-04      1.416e-04\nbeta[65] -1.046e-01  0.241791 2.418e-03      4.446e-03\nbeta[66] -2.649e-02  0.014401 1.440e-04      1.909e-04\nbeta[67] -1.118e-01  0.239106 2.391e-03      4.219e-03\nbeta[68]  1.269e-01  0.010858 1.086e-04      1.980e-04\nbeta[69]  1.960e-01  0.110487 1.105e-03      2.723e-03\nbeta[70]  5.629e-02  0.234695 2.347e-03      3.477e-03\nbeta[71] -1.911e-01  0.277531 2.775e-03      7.445e-03\nbeta[72]  6.687e-02  0.150299 1.503e-03      2.264e-03\nbeta[73]  3.648e-01  0.080818 8.082e-04      3.732e-03\nbeta[74]  1.200e-01  0.043146 4.315e-04      7.646e-04\nbeta[75]  2.015e-02  0.005652 5.652e-05      7.236e-05\nbeta[76]  2.044e-01  0.007633 7.633e-05      2.090e-04\nbeta[77] -4.894e-02  0.073670 7.367e-04      1.002e-03\nbeta[78] -4.762e-02  0.073647 7.365e-04      1.037e-03\nbeta[79]  1.650e-01  0.008248 8.248e-05      1.861e-04\nbeta[80]  8.492e-04  0.227673 2.277e-03      2.969e-03\nbeta[81] -3.284e-02  0.010875 1.087e-04      1.442e-04\nbeta[82]  1.724e-01  0.046703 4.670e-04      1.014e-03\nbeta[83]  2.963e-01  0.227647 2.276e-03      8.236e-03\nmu[1]     3.598e+00  0.038032 3.803e-04      4.787e-04\nmu[2]     3.046e+00  0.055711 5.571e-04      1.036e-03\nmu[3]    -1.195e-01  0.273970 2.740e-03      5.302e-03\nmu[4]    -1.837e+00  0.578810 5.788e-03      9.301e-03\nmu[5]     2.963e+00  0.058452 5.845e-04      1.046e-03\nmu[6]    -1.412e+00  0.477669 4.777e-03      7.838e-03\nmu[7]    -2.263e+00  0.698987 6.990e-03      1.164e-02\nmu[8]     2.702e-03  0.254833 2.548e-03      4.873e-03\nmu[9]    -2.906e+00  0.986986 9.870e-03      2.120e-02\nmu[10]    3.379e+00  0.050233 5.023e-04      1.170e-03\nmu[11]   -1.591e-01  0.261546 2.615e-03      4.529e-03\nmu[12]    3.494e+00  0.041185 4.118e-04      5.564e-04\nmu[13]    2.435e+00  0.080798 8.080e-04      2.040e-03\nmu[14]    2.198e+00  0.085242 8.524e-04      1.671e-03\nmu[15]    1.539e+00  0.117493 1.175e-03      2.321e-03\nmu[16]   -1.150e+00  0.431392 4.314e-03      7.537e-03\nmu[17]    1.656e+00  0.122777 1.228e-03      3.203e-03\nmu[18]   -2.528e+01 11.942995 1.194e-01      2.107e+00\nmu[19]    1.565e+00  0.120775 1.208e-03      2.576e-03\nmu[20]   -4.436e+00  1.733684 1.734e-02      4.356e-02\nmu[21]    3.288e-01  0.209567 2.096e-03      3.740e-03\nmu[22]   -3.085e+00  1.085627 1.086e-02      2.491e-02\nmu[23]    2.135e+00  0.078122 7.812e-04      1.030e-03\nmu[24]   -3.206e+00  1.153488 1.153e-02      2.953e-02\nmu[25]    3.946e+00  0.033369 3.337e-04      5.251e-04\nmu[26]   -4.981e+00  2.355240 2.355e-02      9.419e-02\nmu[27]   -5.415e+00  2.576061 2.576e-02      1.145e-01\nmu[28]    2.911e+00  0.054746 5.475e-04      7.323e-04\nmu[29]    2.124e+00  0.092356 9.236e-04      1.983e-03\nmu[30]   -1.253e+00  0.530140 5.301e-03      1.960e-02\nmu[31]   -1.558e+00  0.499864 4.999e-03      7.467e-03\nmu[32]   -3.195e+00  1.152536 1.153e-02      3.006e-02\nmu[33]    3.799e+00  0.039582 3.958e-04      8.103e-04\nmu[34]    2.781e+00  0.065748 6.575e-04      1.382e-03\nmu[35]   -5.342e+00  2.589064 2.589e-02      1.224e-01\nmu[36]    2.462e+00  0.080159 8.016e-04      2.028e-03\nmu[37]   -4.528e+00  1.873174 1.873e-02      5.286e-02\nmu[38]    2.642e+00  0.061286 6.129e-04      7.579e-04\nmu[39]   -1.971e+00  0.631519 6.315e-03      1.172e-02\nmu[40]   -2.234e+00  0.698795 6.988e-03      1.125e-02\nmu[41]    2.680e+00  0.069561 6.956e-04      1.389e-03\nmu[42]   -2.190e+00  0.672194 6.722e-03      1.015e-02\nmu[43]   -1.821e+00  0.565775 5.658e-03      8.804e-03\nmu[44]   -1.307e+01  7.051681 7.052e-02      8.196e-01\nmu[45]   -3.044e+01 18.892164 1.889e-01      6.038e+00\nmu[46]    4.867e-01  0.209065 2.091e-03      4.598e-03\nmu[47]   -5.633e-01  0.362765 3.628e-03      9.608e-03\nmu[48]   -1.005e+00  0.378434 3.784e-03      5.465e-03\nmu[49]    4.707e+00  0.024094 2.409e-04      4.707e-04\nmu[50]    1.927e+00  0.103311 1.033e-03      2.317e-03\nmu[51]   -6.268e-01  0.349487 3.495e-03      6.904e-03\nmu[52]   -2.942e+00  0.984873 9.849e-03      2.041e-02\nmu[53]    2.385e+00  0.077542 7.754e-04      1.450e-03\nmu[54]    1.204e+00  0.142713 1.427e-03      2.743e-03\nmu[55]   -9.457e-01  0.380660 3.807e-03      6.346e-03\nmu[56]    1.837e+00  0.109253 1.093e-03      2.490e-03\nmu[57]   -6.675e-01  0.323598 3.236e-03      4.620e-03\nmu[58]    2.375e+00  0.079230 7.923e-04      1.525e-03\nmu[59]   -4.311e-01  0.318256 3.183e-03      6.938e-03\nmu[60]    2.089e+00  0.092807 9.281e-04      1.925e-03\nmu[61]   -4.144e+00  1.534024 1.534e-02      3.167e-02\nmu[62]    3.201e+00  0.045824 4.582e-04      6.104e-04\nmu[63]   -9.921e-02  0.243202 2.432e-03      3.776e-03\nmu[64]    2.665e+00  0.059689 5.969e-04      7.534e-04\nmu[65]   -4.224e+00  1.647465 1.647e-02      3.928e-02\nmu[66]    2.121e+00  0.078352 7.835e-04      1.020e-03\nmu[67]   -4.222e+00  1.554470 1.554e-02      3.424e-02\nmu[68]    2.728e+00  0.064233 6.423e-04      1.140e-03\nmu[69]   -2.052e+00  0.721714 7.217e-03      1.876e-02\nmu[70]   -4.167e+00  1.581742 1.582e-02      3.253e-02\nmu[71]   -4.589e+00  1.988992 1.989e-02      5.875e-02\nmu[72]   -2.851e+00  0.905025 9.050e-03      1.668e-02\nmu[73]   -1.379e+00  0.590243 5.902e-03      2.819e-02\nmu[74]   -1.881e-02  0.253284 2.533e-03      4.602e-03\nmu[75]    3.988e+00  0.031245 3.124e-04      4.014e-04\nmu[76]    3.484e+00  0.048766 4.877e-04      1.297e-03\nmu[77]   -1.180e+00  0.416508 4.165e-03      6.124e-03\nmu[78]   -1.174e+00  0.427212 4.272e-03      6.846e-03\nmu[79]    3.321e+00  0.051078 5.108e-04      1.152e-03\nmu[80]   -4.083e+00  1.507924 1.508e-02      2.886e-02\nmu[81]    2.649e+00  0.061347 6.135e-04      8.565e-04\nmu[82]   -2.215e-01  0.292863 2.929e-03      6.339e-03\nmu[83]   -3.846e+00  1.637498 1.637e-02      6.577e-02\n\n2. Quantiles for each variable:\n\n               2.5%        25%        50%        75%     97.5%\nbeta[1]   -0.022158  -0.013201 -8.408e-03 -3.743e-03  0.005039\nbeta[2]    0.112838   0.124678  1.307e-01  1.369e-01  0.149457\nbeta[3]    0.049590   0.105373  1.345e-01  1.643e-01  0.227822\nbeta[4]   -0.193351  -0.055907  9.552e-03  7.344e-02  0.204556\nbeta[5]    0.105578   0.118672  1.253e-01  1.320e-01  0.144239\nbeta[6]   -0.081776   0.018846  7.227e-02  1.283e-01  0.242547\nbeta[7]   -0.295169  -0.129883 -4.970e-02  2.697e-02  0.178445\nbeta[8]    0.060939   0.111216  1.379e-01  1.662e-01  0.227395\nbeta[9]   -0.423770  -0.184143 -8.184e-02  1.166e-02  0.198572\nbeta[10]   0.163302   0.173428  1.788e-01  1.841e-01  0.194591\nbeta[11]   0.017879   0.072708  1.029e-01  1.333e-01  0.196431\nbeta[12]  -0.060938  -0.051327 -4.630e-02 -4.143e-02 -0.032075\nbeta[13]   0.178496   0.194187  2.027e-01  2.112e-01  0.227797\nbeta[14]   0.114301   0.133008  1.424e-01  1.520e-01  0.170848\nbeta[15]   0.112391   0.136340  1.493e-01  1.624e-01  0.188416\nbeta[16]  -0.031629   0.054778  1.030e-01  1.533e-01  0.256106\nbeta[17]   0.165695   0.189306  2.024e-01  2.154e-01  0.241310\nbeta[18]   0.414032   1.712929  2.563e+00  3.757e+00  5.358092\nbeta[19]   0.133128   0.157228  1.703e-01  1.841e-01  0.209222\nbeta[20]  -0.736909  -0.313719 -1.446e-01 -3.892e-05  0.283892\nbeta[21]   0.048239   0.091664  1.147e-01  1.394e-01  0.189779\nbeta[22]  -0.164050   0.025213  1.282e-01  2.443e-01  0.515988\nbeta[23]  -0.016204   0.002198  1.211e-02  2.194e-02  0.040699\nbeta[24]  -0.556835  -0.274872 -1.559e-01 -5.157e-02  0.137835\nbeta[25]   0.089418   0.096834  1.008e-01  1.048e-01  0.111968\nbeta[26]  -0.212766   0.062303  2.284e-01  4.296e-01  1.023660\nbeta[27]  -1.203848  -0.527417 -2.993e-01 -1.231e-01  0.166904\nbeta[28]   0.033754   0.046184  5.295e-02  5.953e-02  0.072197\nbeta[29]   0.126704   0.145906  1.563e-01  1.666e-01  0.186356\nbeta[30]   0.153587   0.233554  2.819e-01  3.348e-01  0.457911\nbeta[31]  -0.143712  -0.028467  2.840e-02  8.682e-02  0.203696\nbeta[32]  -0.569523  -0.276827 -1.590e-01 -5.375e-02  0.132273\nbeta[33]   0.132315   0.140667  1.451e-01  1.495e-01  0.158186\nbeta[34]   0.142870   0.155587  1.630e-01  1.702e-01  0.184567\nbeta[35]  -0.172471   0.116121  2.920e-01  5.140e-01  1.201293\nbeta[36]   0.178535   0.194615  2.032e-01  2.118e-01  0.227017\nbeta[37]  -0.802576  -0.332348 -1.545e-01 -6.294e-03  0.272860\nbeta[38]  -0.040849  -0.026856 -1.956e-02 -1.194e-02  0.002417\nbeta[39]  -0.090249   0.031939  9.780e-02  1.718e-01  0.319160\nbeta[40]  -0.189607  -0.036758  3.911e-02  1.167e-01  0.277665\nbeta[41]   0.129460   0.143804  1.513e-01  1.590e-01  0.173353\nbeta[42]  -0.208704  -0.061022  1.166e-02  8.559e-02  0.236248\nbeta[43]  -0.202722  -0.071449 -5.342e-03  5.870e-02  0.186490\nbeta[44]   0.110302   0.697005  1.208e+00  1.872e+00  3.162159\nbeta[45]   0.468618   1.749385  2.924e+00  4.322e+00  9.033387\nbeta[46]   0.116299   0.155526  1.771e-01  2.004e-01  0.246106\nbeta[47]   0.089740   0.154443  1.909e-01  2.291e-01  0.312192\nbeta[48]  -0.120899  -0.034603  8.292e-03  5.370e-02  0.146662\nbeta[49]   0.133368   0.138460  1.412e-01  1.439e-01  0.149007\nbeta[50]   0.141723   0.162541  1.739e-01  1.851e-01  0.206978\nbeta[51]  -0.253938  -0.171302 -1.316e-01 -9.426e-02 -0.028586\nbeta[52]  -0.449320  -0.204798 -9.945e-02 -6.416e-03  0.172100\nbeta[53]   0.108320   0.124533  1.332e-01  1.420e-01  0.158612\nbeta[54]   0.092954   0.122175  1.373e-01  1.532e-01  0.185496\nbeta[55]  -0.049302   0.031801  7.590e-02  1.212e-01  0.215022\nbeta[56]   0.156316   0.178194  1.897e-01  2.017e-01  0.224996\nbeta[57]  -0.067968   0.005312  4.321e-02  8.262e-02  0.163183\nbeta[58]   0.128514   0.144503  1.534e-01  1.622e-01  0.179765\nbeta[59]   0.070257   0.129072  1.626e-01  1.972e-01  0.269911\nbeta[60]   0.131061   0.151320  1.614e-01  1.717e-01  0.191311\nbeta[61]  -0.473028  -0.141666 -2.225e-03  1.451e-01  0.474377\nbeta[62]  -0.012813  -0.002101  3.564e-03  9.338e-03  0.020356\nbeta[63]  -0.143097  -0.081904 -5.207e-02 -2.309e-02  0.030729\nbeta[64]  -0.016649  -0.002446  4.915e-03  1.236e-02  0.026146\nbeta[65]  -0.627927  -0.242130 -8.721e-02  4.974e-02  0.337586\nbeta[66]  -0.055265  -0.036115 -2.647e-02 -1.683e-02  0.001892\nbeta[67]  -0.626884  -0.254483 -1.002e-01  4.397e-02  0.336025\nbeta[68]   0.106037   0.119494  1.267e-01  1.342e-01  0.148066\nbeta[69]   0.004146   0.119178  1.869e-01  2.635e-01  0.436215\nbeta[70]  -0.392773  -0.095418  4.680e-02  1.968e-01  0.554794\nbeta[71]  -0.866173  -0.336257 -1.563e-01 -5.746e-03  0.266603\nbeta[72]  -0.215210  -0.031382  6.333e-02  1.596e-01  0.378756\nbeta[73]   0.227346   0.308129  3.580e-01  4.144e-01  0.540522\nbeta[74]   0.039093   0.089888  1.188e-01  1.493e-01  0.206868\nbeta[75]   0.008776   0.016408  2.013e-02  2.402e-02  0.031236\nbeta[76]   0.189577   0.199207  2.045e-01  2.096e-01  0.219147\nbeta[77]  -0.200728  -0.096594 -4.746e-02  8.509e-04  0.093313\nbeta[78]  -0.193328  -0.096532 -4.633e-02  1.855e-03  0.093144\nbeta[79]   0.148877   0.159365  1.649e-01  1.706e-01  0.181136\nbeta[80]  -0.464251  -0.139227 -2.271e-04  1.401e-01  0.465108\nbeta[81]  -0.054630  -0.039936 -3.278e-02 -2.552e-02 -0.011789\nbeta[82]   0.084442   0.140423  1.708e-01  2.034e-01  0.266341\nbeta[83]  -0.053028   0.137956  2.625e-01  4.148e-01  0.847470\nmu[1]      3.522734   3.571622  3.597e+00  3.624e+00  3.670472\nmu[2]      2.934665   3.010136  3.047e+00  3.085e+00  3.152774\nmu[3]     -0.697499  -0.289781 -1.004e-01  6.892e-02  0.362202\nmu[4]     -3.111531  -2.188444 -1.784e+00 -1.433e+00 -0.837955\nmu[5]      2.849737   2.922412  2.963e+00  3.002e+00  3.076407\nmu[6]     -2.466439  -1.706076 -1.381e+00 -1.070e+00 -0.583645\nmu[7]     -3.828128  -2.676097 -2.191e+00 -1.770e+00 -1.082587\nmu[8]     -0.544970  -0.153763  2.121e-02  1.811e-01  0.450621\nmu[9]     -5.233607  -3.420285 -2.761e+00 -2.216e+00 -1.421694\nmu[10]     3.278419   3.345484  3.380e+00  3.414e+00  3.476815\nmu[11]    -0.714762  -0.328312 -1.460e-01  2.463e-02  0.315043\nmu[12]     3.411630   3.466355  3.494e+00  3.522e+00  3.574178\nmu[13]     2.273891   2.381102  2.435e+00  2.490e+00  2.589476\nmu[14]     2.023863   2.141754  2.200e+00  2.255e+00  2.359443\nmu[15]     1.297227   1.460472  1.541e+00  1.621e+00  1.756937\nmu[16]    -2.089790  -1.419189 -1.116e+00 -8.494e-01 -0.385990\nmu[17]     1.407796   1.575322  1.661e+00  1.742e+00  1.886456\nmu[18]   -48.522696 -34.330195 -2.370e+01 -1.614e+01 -5.255296\nmu[19]     1.319087   1.485238  1.569e+00  1.648e+00  1.784907\nmu[20]    -8.606864  -5.326423 -4.116e+00 -3.189e+00 -1.987699\nmu[21]    -0.108506   0.194547  3.358e-01  4.740e-01  0.713125\nmu[22]    -5.630916  -3.715756 -2.922e+00 -2.288e+00 -1.433865\nmu[23]     1.978499   2.082398  2.137e+00  2.188e+00  2.286316\nmu[24]    -5.955439  -3.812509 -3.020e+00 -2.398e+00 -1.501652\nmu[25]     3.881000   3.923999  3.947e+00  3.969e+00  4.011018\nmu[26]   -11.047846  -5.998027 -4.450e+00 -3.360e+00 -2.029135\nmu[27]   -12.180395  -6.519503 -4.835e+00 -3.620e+00 -2.086769\nmu[28]     2.804561   2.873732  2.911e+00  2.948e+00  3.017415\nmu[29]     1.940585   2.063179  2.126e+00  2.187e+00  2.299606\nmu[30]    -2.438771  -1.572133 -1.212e+00 -8.736e-01 -0.361764\nmu[31]    -2.652182  -1.862803 -1.517e+00 -1.204e+00 -0.700185\nmu[32]    -5.937672  -3.791823 -3.020e+00 -2.396e+00 -1.487683\nmu[33]     3.720906   3.772908  3.800e+00  3.826e+00  3.876986\nmu[34]     2.647471   2.736792  2.782e+00  2.826e+00  2.905221\nmu[35]   -12.423926  -6.407810 -4.729e+00 -3.558e+00 -2.140114\nmu[36]     2.308081   2.407905  2.463e+00  2.517e+00  2.617400\nmu[37]    -9.183619  -5.453795 -4.137e+00 -3.211e+00 -1.949345\nmu[38]     2.519286   2.601592  2.643e+00  2.684e+00  2.758615\nmu[39]    -3.357395  -2.356619 -1.909e+00 -1.519e+00 -0.892862\nmu[40]    -3.814727  -2.645934 -2.155e+00 -1.735e+00 -1.086912\nmu[41]     2.541319   2.633491  2.681e+00  2.727e+00  2.813298\nmu[42]    -3.682709  -2.597620 -2.130e+00 -1.716e+00 -1.057331\nmu[43]    -3.077589  -2.169461 -1.774e+00 -1.417e+00 -0.866546\nmu[44]   -29.313943 -17.663825 -1.188e+01 -7.525e+00 -3.137309\nmu[45]   -82.501910 -39.497817 -2.691e+01 -1.648e+01 -5.444618\nmu[46]     0.051158   0.352574  4.934e-01  6.304e-01  0.878105\nmu[47]    -1.355548  -0.785159 -5.379e-01 -3.131e-01  0.073889\nmu[48]    -1.823836  -1.238662 -9.811e-01 -7.459e-01 -0.329499\nmu[49]     4.660445   4.691248  4.707e+00  4.724e+00  4.754929\nmu[50]     1.719717   1.858438  1.928e+00  1.998e+00  2.121282\nmu[51]    -1.376774  -0.842922 -6.082e-01 -3.826e-01  0.001217\nmu[52]    -5.249190  -3.493082 -2.802e+00 -2.247e+00 -1.422414\nmu[53]     2.227691   2.334012  2.387e+00  2.437e+00  2.529315\nmu[54]     0.904842   1.111551  1.210e+00  1.303e+00  1.463085\nmu[55]    -1.769835  -1.184510 -9.205e-01 -6.834e-01 -0.262009\nmu[56]     1.611982   1.764700  1.840e+00  1.913e+00  2.040331\nmu[57]    -1.349509  -0.871281 -6.470e-01 -4.469e-01 -0.081663\nmu[58]     2.215132   2.322070  2.375e+00  2.430e+00  2.524632\nmu[59]    -1.100501  -0.631050 -4.125e-01 -2.128e-01  0.154303\nmu[60]     1.907885   2.027226  2.090e+00  2.151e+00  2.269000\nmu[61]    -7.831460  -4.976211 -3.894e+00 -3.038e+00 -1.900003\nmu[62]     3.110104   3.170348  3.201e+00  3.233e+00  3.291422\nmu[63]    -0.612006  -0.256788 -9.081e-02  6.673e-02  0.353381\nmu[64]     2.548931   2.625213  2.665e+00  2.706e+00  2.781621\nmu[65]    -8.248273  -5.060436 -3.928e+00 -3.068e+00 -1.918927\nmu[66]     1.962224   2.069748  2.123e+00  2.175e+00  2.271533\nmu[67]    -8.122593  -5.044410 -3.964e+00 -3.118e+00 -1.903408\nmu[68]     2.598501   2.685185  2.729e+00  2.772e+00  2.850097\nmu[69]    -3.700733  -2.462015 -1.976e+00 -1.530e+00 -0.901825\nmu[70]    -8.120406  -4.987800 -3.896e+00 -3.036e+00 -1.862502\nmu[71]    -9.655365  -5.514783 -4.155e+00 -3.195e+00 -1.951355\nmu[72]    -4.924968  -3.365619 -2.730e+00 -2.203e+00 -1.432825\nmu[73]    -2.689221  -1.734784 -1.314e+00 -9.563e-01 -0.398159\nmu[74]    -0.550139  -0.181997 -4.776e-03  1.551e-01  0.438031\nmu[75]     3.926851   3.967171  3.988e+00  4.010e+00  4.048135\nmu[76]     3.386406   3.450820  3.484e+00  3.517e+00  3.578676\nmu[77]    -2.085966  -1.436318 -1.157e+00 -8.908e-01 -0.446804\nmu[78]    -2.085994  -1.438868 -1.144e+00 -8.806e-01 -0.428121\nmu[79]     3.219375   3.287702  3.322e+00  3.356e+00  3.418135\nmu[80]    -7.681546  -4.905287 -3.819e+00 -3.017e+00 -1.822577\nmu[81]     2.530423   2.606532  2.649e+00  2.691e+00  2.767835\nmu[82]    -0.853052  -0.406468 -2.022e-01 -1.516e-02  0.310442\nmu[83]    -8.051976  -4.601659 -3.517e+00 -2.686e+00 -1.621024\n\n\n\n\n# par(mar = c(2, 2, 2, 2))  # smaller margins\n# plot(samp)\n# # Density plots\n# densplot(samp, main = \"Posterior Density for Parameters\")\n# \n# posterior_summary &lt;- summary(samp)\n# round(posterior_summary$statistics, 3)  # means, SDs\n# round(posterior_summary$quantiles, 3)   # 2.5%, 50%, 97.5%\n# \n# beta_samples &lt;- as.matrix(samp)[, grep(\"beta\", colnames(as.matrix(samp)))]\n# beta_means &lt;- apply(beta_samples, 2, mean)\n# beta_ci &lt;- apply(beta_samples, 2, quantile, probs = c(0.025, 0.975))\n# \n# df &lt;- data.frame(\n#   param = colnames(beta_samples),\n#   mean = beta_means,\n#   lower = beta_ci[1, ],\n#   upper = beta_ci[2, ]\n# )\n# \n# ggplot(df, aes(x = param, y = mean)) +\n#   geom_point() +\n#   geom_errorbar(aes(ymin = lower, ymax = upper), width = 0.2) +\n#   coord_flip() +\n#   labs(title = \"Posterior Estimates for Beta\", y = \"Effect Size\", x = \"\") +\n#   theme_minimal()\n# \n# # Heatmap\n# # Your original year and serotype info\n# year_seq &lt;- sort(unique(jdat$year_id))  # e.g., 2000:2006\n# sero_seq &lt;- 1:jdat$n_sero\n# mean_year &lt;- (length(year_seq) + 1) / 2\n# \n# # Create a dataframe of all serotype-year combinations\n# pred_grid &lt;- expand.grid(\n#   sero = sero_seq,\n#   year = year_seq\n# )\n\n# # Predict expected log incidence\n# pred_grid$log_lambda &lt;- mu_means[pred_grid$sero] +\n#   beta_means[pred_grid$sero] * (pred_grid$year - mean_year)\n# \n# # Back-transform to incidence\n# pred_grid$lambda &lt;- exp(pred_grid$log_lambda)\n# \n# ggplot(pred_grid, aes(x = year, y = factor(sero), fill = lambda)) +\n#   geom_tile(color = \"white\") +\n#   scale_fill_viridis_c(name = \"Expected\\nCases\", option = \"C\") +\n#   labs(x = \"Year\", y = \"Serotype\", title = \"Expected Cases by Year and Serotype\") +\n#   theme_minimal() +\n#   theme(axis.text.y = element_text(size = 8))\n# \n# ggplot(serotype_year_counts, aes(x = epiyr, y = SeroType, fill = cases)) +\n#   geom_tile(color = \"white\") +\n#   scale_fill_viridis_c(option = \"C\", name = \"Cases\") +\n#   labs(title = \"IPD Cases by Serotype and Year\",\n#        x = \"Year\",\n#        y = \"Serotype\") +\n#   theme_minimal() +\n#   theme(axis.text.x = element_text(angle = 45, hjust = 1))\n# \n# # Model version\n# mean_year &lt;- (max(serotype_year_counts$year_id) + 1) / 2\n# \n# # Create a data frame with expected log lambda for each serotype-year\n# serotype_year_counts$expected_log_lambda &lt;- mu_means[serotype_year_counts$sero_id] + \n#   beta_means[serotype_year_counts$sero_id] * (serotype_year_counts$year_id - mean_year)\n# \n# # Convert to expected cases (lambda)\n# serotype_year_counts$expected_cases &lt;- exp(serotype_year_counts$expected_log_lambda)\n# \n# # Plot\n# ggplot(serotype_year_counts, aes(x = epiyr)) +\n#   geom_point(aes(y = cases), color = \"blue\", alpha = 0.5) +\n#   geom_line(aes(y = expected_cases, group = SeroType), color = \"red\") +\n#   facet_wrap(~ SeroType, scales = \"free_y\") +\n#   labs(y = \"Cases\", x = \"Year\", title = \"Observed (points) vs Expected (lines) Cases by Serotype\") +\n#   theme_minimal() +\n#   theme(axis.text.x = element_text(angle = 45, hjust = 1))"
  }
]