[
  {
    "objectID": "Pages/Schedule.html",
    "href": "Pages/Schedule.html",
    "title": "Public Health Modeling Group",
    "section": "",
    "text": "At the end of the program, you will present your work at a symposium, which will include both a presentation with slides and a poster session. Each group will have 25 minutes to present their slide deck, followed by 5 minutes for questions. The poster session will last 45 minutes.\n\n\n\n\n\n\nImportant\n\n\n\nDuring the poster session, at least one member of your group should be stationed at the poster at all times, while the other two can explore the room. We expect that you will switch every 15 minutes so that everyone spends at least 15 minutes at the poster and the remaining 30 minutes exploring other projects.\n\n\nTo help you stay on track with compiling your materials, we have prepared the following scheduled assignments. Over the coming weeks, we will guide you through preparing the materials for this symposium and aim to expose you to the process of developing these materials in a professional environment.\nWhen working on a research team, it is expected that you can clearly articulate your research to people outside your specific area of expertise and provide meaningful, constructive feedback to help teammates improve their research. At some companies, to be considered for a promotion, you will be required to present an annual summary of our research and a literature review to the group/department, as well as regularly provide insightful comments and constructive criticisms to colleagues.\nThis is an ambitious schedule, and we acknowledge the extensive workload you each have during the program. It is equally important for you to experience the research project process as it is to execute the techniques we will teach you to support your project hypothesis/research question. You will not be expected to conduct additional analyses beyond the content we provide and assign.\nIf you encounter insurmountable difficulties implementing any part of the study, please prioritize completing the overall study over finishing any single part that proves too challenging. We are here to answer any questions you might have and help troubleshoot any implementation problems."
  },
  {
    "objectID": "Pages/Schedule.html#expectations-overview",
    "href": "Pages/Schedule.html#expectations-overview",
    "title": "Public Health Modeling Group",
    "section": "",
    "text": "At the end of the program, you will present your work at a symposium, which will include both a presentation with slides and a poster session. Each group will have 25 minutes to present their slide deck, followed by 5 minutes for questions. The poster session will last 45 minutes.\n\n\n\n\n\n\nImportant\n\n\n\nDuring the poster session, at least one member of your group should be stationed at the poster at all times, while the other two can explore the room. We expect that you will switch every 15 minutes so that everyone spends at least 15 minutes at the poster and the remaining 30 minutes exploring other projects.\n\n\nTo help you stay on track with compiling your materials, we have prepared the following scheduled assignments. Over the coming weeks, we will guide you through preparing the materials for this symposium and aim to expose you to the process of developing these materials in a professional environment.\nWhen working on a research team, it is expected that you can clearly articulate your research to people outside your specific area of expertise and provide meaningful, constructive feedback to help teammates improve their research. At some companies, to be considered for a promotion, you will be required to present an annual summary of our research and a literature review to the group/department, as well as regularly provide insightful comments and constructive criticisms to colleagues.\nThis is an ambitious schedule, and we acknowledge the extensive workload you each have during the program. It is equally important for you to experience the research project process as it is to execute the techniques we will teach you to support your project hypothesis/research question. You will not be expected to conduct additional analyses beyond the content we provide and assign.\nIf you encounter insurmountable difficulties implementing any part of the study, please prioritize completing the overall study over finishing any single part that proves too challenging. We are here to answer any questions you might have and help troubleshoot any implementation problems."
  },
  {
    "objectID": "Pages/Schedule.html#format",
    "href": "Pages/Schedule.html#format",
    "title": "Public Health Modeling Group",
    "section": "Format",
    "text": "Format\nEach Thursday, you will be expected to have completed specific aspects of your analysis to allow time to process the upcoming material. Additionally, each group will present summaries of their work. The specifics of these expectations are outlined below for each week.\n\n\n\n\n\n\nImportant\n\n\n\nTime around 1 minute of discussion for each slide. If you need pointers on how to effectively present your work, please check out our tips provided in the Project Materials page.\n\n\nWe expect the audience to provide insightful, constructive feedback to help improve and compliment the presenting groups on their work. Additionally, we expect you to foster a positive, collaborative environment within your group, allowing everyone the opportunity to participate meaningfully.\nIt is critical that each member of your group equally shares the responsibilities of conducting the research and presenting the weekly updates. Complimenting colleagues’ work demonstrates good professionalism and helps the team identify and maintain effective practices."
  },
  {
    "objectID": "Pages/Schedule.html#thursday-july-2nd",
    "href": "Pages/Schedule.html#thursday-july-2nd",
    "title": "Public Health Modeling Group",
    "section": "Thursday July 2nd",
    "text": "Thursday July 2nd\nHave Completed:\n\nConducted a literature search for peer-reviewed references (at least 5 relevant papers), focusing on publications from the last 5-10 years.\nAny exploratory data analysis (EDA) of your dataset, with simple visualization of the variables (i.e. histograms of variables, etc.).\nPrepared a background outline.\nDeveloped a hypothesis/research question.\n\nPresent (12-15 slides):\n\n1 slide describing your hypothesis/research question. Try to explain how you hope the techniques learned thus far will help you justify any conclusion about this hypothesis/research question.\n“Journal Club” like presentation where you will include:\n\n5-6 slides, with each slide providing a high-level overview of one peer-reviewed reference. Cover 5-6 of your references. For each paper, briefly explain what the paper is about, methods, if it applies any of the techniques we will be learning, and how it will help you with your study.\n3 slides giving a deeper dive into one paper.\n\nIn less than 3-5 slides, summarize your EDA findings, like:\n\nTime-span it covers, categories (i.e. age groups or regions), number of unique data points, etc.\nBriefly discuss the variables, especially if the variables imply features that are not readily obvious.\nDiscuss any potential gaps or pitfalls found with the data, such as data collection limitations, missing data, or combined outcomes that could confound the observations."
  },
  {
    "objectID": "Pages/Schedule.html#thursday-july-10th",
    "href": "Pages/Schedule.html#thursday-july-10th",
    "title": "Public Health Modeling Group",
    "section": "Thursday, July 10th",
    "text": "Thursday, July 10th\nHave Completed:\n\nImplemented Time Series and Hierarchical Modeling, focusing on how they support your hypothesis/research questions.\nStarted on methods-specific visualizations and drafting the methods report.\n\nPresent (7-9 slides):\n\n3-5 slides summarizing current results and any conclusions you might have with visualizations and any interesting/unexpected findings.\n3 slides explaining what Time Series and Hierarchical Modeling are addressing in layman’s terms and how they support your hypothesis/study question.\n1 slide to discuss whether working with the data and seeing the results has shaped the research question/hypothesis."
  },
  {
    "objectID": "Pages/Schedule.html#thursday-july-16th",
    "href": "Pages/Schedule.html#thursday-july-16th",
    "title": "Public Health Modeling Group",
    "section": "Thursday, July 16th",
    "text": "Thursday, July 16th\nHave Completed:\n\nImplemented GPS and Market Basket analysis, focusing on how they support your hypothesis/research questions.\nFinished the methods section with GPS and Market Basket analysis and relevant visualizations.\nSlide deck first draft for the 25 minute symposium presentation.\nPoster first draft.\n\nPresent (4 slides):\n\n1 slide summarizing current results and any conclusions you might have with visualizations and any interesting/unexpected findings.\n2 slides explaining what GPS and Market Basket are addressing in layman’s terms and how they support your hypothesis/study question.\n1 slide to discuss whether working with the data and seeing the results has shaped the research question/hypothesis.\n\nMock Symposium:\nAfter a brief catch-up on the group’s progress, we aim to spend most of the day practicing the presentations and poster sessions. This time is intended to help you practice with peers who understand the type of study you conducted and give you an opportunity to implement any last-minute changes to improve your content.\nEach group will be given 25 minutes to present their slides followed by 5-10 minutes of peer-feedback. For the audience, provide feedback about:\n\nWhether the presentation was cohesive from start to finish and told a “story” about the study.\nHighlighted interesting findings unique to their data and hypothesis/research question.\nIf presenters clearly explained the takeaway from each slide and equally shared the responsibilities of presenting.\nWhether the group explained any shortcomings in their analysis, such as limitations imposed by the dataset’s original data collection methods.\nWhether the slides were light on text and effectively used visuals to communicate the core takeaways from each slide.\nWhether the presentation formatting was consistent enough to make the progression of information easy to follow. For example, ensuring serotypes are color-coded the same way in each figure.\n\nYou will then be mixed into groups of three people with one representative from each group. Each person will be given time to pull up their groups poster on their computer and give a 30 second to 1 minute elevator speech, or “hook”. Then explain some of the more detailed highlights you might provide someone who stops at your poster to hear more.\nFor poster visiters, practice asking questions from the perspective of someone from another project group. Provide feedback about:\n\nIf the primary 30-second to 1-minute “hook” is engaging and covers relevant points that encompass the study’s purpose and findings.\nHow easy it is to understand the project goals and conclusions if you have not conducted a similar study.\nThe effectiveness of the visualizations.\nWhether the poster is easy for casual browsers to understand, or if it requires a lot of focus to comprehend."
  },
  {
    "objectID": "Pages/Schedule.html#monday-july-21st",
    "href": "Pages/Schedule.html#monday-july-21st",
    "title": "Public Health Modeling Group",
    "section": "Monday July 21st",
    "text": "Monday July 21st\nUse this time to finalize any remaining details and practice the slide presentation or poster with your group.\nHave Completed:\n\nFinalized poster by end of day (EOD) for printing. The instructors will send these to Jackson/Aquielle for printing."
  },
  {
    "objectID": "Pages/Week2/Time Series Analysis_Hands-On Exercise.html",
    "href": "Pages/Week2/Time Series Analysis_Hands-On Exercise.html",
    "title": "Time Series Analysis",
    "section": "",
    "text": "This workshop was developed by Dr. Dan Weinberger from the Yale School of Public Health (YSPH) for the 12^{th} International Symposium on Pneumococci and Pneumococcal Diseases (ISPPD-12) hosted in Toronto, Canada in June 2022. Here, we have adapted his workshop session #1, which compares different methods for fitting a time series dataset 1. Students are encouraged to explore the workshop content that Dr. Weinberger put together to learn more about the subject:\n\nGitHub: Find the code, original data, and slides 1.\nWorkshop Webpage: Find a longer description of the workshop in the format of a mini course with links to video presentations of select sections - Workshop on the evaluation of vaccine impact 2.\n\nThe data used here is a subset of the complete set used in Oliveira, L. H. D. et al. 2020 3. In their paper, national-level mortality data spanning from 2000 to 2016 for children under 5 years of age was compiled and standardized by national mortality registers in 10 Latin American and Caribbean countries. The primary cause of death was classified using the International Classification of Diseases, Tenth Revision (ICD-10) codes (Influenza and pneumonia J09-J18 or Oliveira, L. H. D. et al. 2020 Supplementary Table 2) 4,5. The goal of the paper was to evaluate the efficacy of pneumococcal conjugate vaccines (PCVs) in children who received any part of the standard-of-care vaccination series.\nToday, we are going to examine the efficacy of pneumococcal conjugate vaccines (PCVs) in children aged 2 to 59 months in Ecuador. The standard-of-care vaccination series involves a primary series of either two doses (administered at 2 and 4 months of age) or three doses (administered at 2, 4, and 6 months of age), with the option of an additional booster dose (administered at 12-18 months of age). This dataset does not differentiate between patients who received varying levels of inoculation, and instead reports the data if any PCV was delivered."
  },
  {
    "objectID": "Pages/Week2/Time Series Analysis_Hands-On Exercise.html#introduction",
    "href": "Pages/Week2/Time Series Analysis_Hands-On Exercise.html#introduction",
    "title": "Time Series Analysis",
    "section": "",
    "text": "This workshop was developed by Dr. Dan Weinberger from the Yale School of Public Health (YSPH) for the 12^{th} International Symposium on Pneumococci and Pneumococcal Diseases (ISPPD-12) hosted in Toronto, Canada in June 2022. Here, we have adapted his workshop session #1, which compares different methods for fitting a time series dataset 1. Students are encouraged to explore the workshop content that Dr. Weinberger put together to learn more about the subject:\n\nGitHub: Find the code, original data, and slides 1.\nWorkshop Webpage: Find a longer description of the workshop in the format of a mini course with links to video presentations of select sections - Workshop on the evaluation of vaccine impact 2.\n\nThe data used here is a subset of the complete set used in Oliveira, L. H. D. et al. 2020 3. In their paper, national-level mortality data spanning from 2000 to 2016 for children under 5 years of age was compiled and standardized by national mortality registers in 10 Latin American and Caribbean countries. The primary cause of death was classified using the International Classification of Diseases, Tenth Revision (ICD-10) codes (Influenza and pneumonia J09-J18 or Oliveira, L. H. D. et al. 2020 Supplementary Table 2) 4,5. The goal of the paper was to evaluate the efficacy of pneumococcal conjugate vaccines (PCVs) in children who received any part of the standard-of-care vaccination series.\nToday, we are going to examine the efficacy of pneumococcal conjugate vaccines (PCVs) in children aged 2 to 59 months in Ecuador. The standard-of-care vaccination series involves a primary series of either two doses (administered at 2 and 4 months of age) or three doses (administered at 2, 4, and 6 months of age), with the option of an additional booster dose (administered at 12-18 months of age). This dataset does not differentiate between patients who received varying levels of inoculation, and instead reports the data if any PCV was delivered."
  },
  {
    "objectID": "Pages/Week2/Time Series Analysis_Hands-On Exercise.html#preamble",
    "href": "Pages/Week2/Time Series Analysis_Hands-On Exercise.html#preamble",
    "title": "Time Series Analysis",
    "section": "Preamble",
    "text": "Preamble\nThis section is drawn from Dr. Weinberger’s recorded presentation What is ‘vaccine impact’?; and Administrative data: challenges and opportunities for evaluation studies 6.\nStatistical inference can be reduced to quantitatively answering three types of questions:\n\nCausation: Was there a change, and can we identify what caused the change?\nPrediction: What do we expect to see in the future?\nCertainty: How reliable are the answers to the first two questions?\n\nA biotechnologist developing a vaccine is primarily concerned with proving that there was a favorable change, such as mitigating the disease process in an individual. In contrast, a public health analyst aims to determine the overall impact of the vaccine on the population. For example, an effective vaccine not only protects an individual but also attenuates transmission within their immediate social circle.\nThis requires contextualizing the vaccine’s impact as a combination of direct and indirect effects. It is important to identify and control for unexplained linear and non-linear trends unrelated to the introduction of a vaccine to a population. For example, were there changes in overall population health at the same time or changes to diagnostic methods?\nWe can appreciate that this is a Sisyphean task, but one that can be addressed with time series analysis. Keep in mind that the methods discussed here are most applicable to endemic diseases that are consistently present in the population, existing at a relatively stable and predictable level. The core reason is that these methods require us to have an idea of what we would expect to have seen had the vaccine not been introduced, allowing us to posit a realistic counterfactual for model evaluation.\nOur study is framed by PICO. In black text are the general definitions for the acronym and in red text is the application to our example:\n\nPopulation: The target population where the impact of a vaccine is to measured. Children 2-59 months old.\nIntervention: The date and timeframe for a vaccine intervention. Introduction of PCV10 to the Brazilian national immunization program.\nComparator: Diseases or groups of diseases to compare against the intervention, which are not expected to be impacted by the intervention itself. Counterfactual to demonstrate what would have happened if the vaccine had not been introduced, compared to the factual scenario.\nOutcome: The condition representing our expected results from the intervention. Deaths due to pneumonia."
  },
  {
    "objectID": "Pages/Week2/Time Series Analysis_Hands-On Exercise.html#set-up-the-environment",
    "href": "Pages/Week2/Time Series Analysis_Hands-On Exercise.html#set-up-the-environment",
    "title": "Time Series Analysis",
    "section": "Set Up the Environment",
    "text": "Set Up the Environment\n\nrenv::init()      # Initialize the project     \nrenv::restore()   # Download packages and their version saved in the lockfile\n\n\nsuppressPackageStartupMessages({\n  library(\"readr\")      # For reading in the data\n  library(\"tibble\")     # For handling tidyverse tibble data classes\n  library(\"tidyr\")      # For tidying data \n  library(\"dplyr\")      # For data manipulation \n  library(\"stringr\")    # For string manipulation\n  library(\"MASS\")       # Functions/datasets for statistical analysis\n  library(\"lubridate\")  # For date manipulation\n  library(\"ggplot2\")    # For creating static visualizations\n  library(\"scales\")     # For formatting plots axis\n  library(\"gridExtra\")  # Creates multiple grid-based plots\n})\n\n\n# Function to select \"Not In\"\n'%!in%' &lt;- function(x,y)!('%in%'(x,y))\n\nThe data has been cleaned and standardized for use here, and is imported directly using the GitHub raw URL. You can explore the additional data cleaning steps applied to all of the data in the instructor’s GitHub repository: ysph-dsde/bdsy-phm. The original dataset and prior data cleaning, validation, and standardization can be found in the paper’s GitHub repository and Dr. Weinberger’s workshop GitHub repository 1,3.\n\n# Read in the cleaned data directly from the instructor's GitHub.\ndf &lt;- read_csv(\"https://raw.githubusercontent.com/ysph-dsde/bdsy-phm/refs/heads/main/Data/ec_2to59m.csv\")\n\n# Summarize aspects and dimentions of our dataset.\nglimpse(df)\n\nRows: 144\nColumns: 6\n$ date         &lt;date&gt; 2005-01-01, 2005-02-01, 2005-03-01, 2005-04-01, 2005-05-…\n$ country      &lt;chr&gt; \"Ecuador\", \"Ecuador\", \"Ecuador\", \"Ecuador\", \"Ecuador\", \"E…\n$ age_group    &lt;chr&gt; \"2-59m\", \"2-59m\", \"2-59m\", \"2-59m\", \"2-59m\", \"2-59m\", \"2-…\n$ doses        &lt;chr&gt; \"Any\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\", \"…\n$ J12_J18_prim &lt;dbl&gt; 50, 51, 53, 46, 62, 34, 38, 38, 28, 26, 33, 31, 46, 53, 5…\n$ acm_noj_prim &lt;dbl&gt; 204, 171, 166, 197, 212, 180, 158, 167, 144, 149, 153, 14…\n\n\n\nData Dictionary\nIt is crucial that we understand the meaning of each variable in our dataset. Sometimes, there are surprising aspects embedded within the variables that are not immediately discernible from the table itself. Many sources provide a “Data Dictionary” for this purpose, but at times, you may need to interpret the variable meanings based on context and methods.\nThis paper did not explicitly describe each variable in a “Data Dictionary”; therefore, the following was assembled based on the context and methods provided in the paper and its supplementary materials 3,4.\n\ndate: The month when the events were recorded. This spans from 2005-01-01 to 2016-12-01.\ncountry: Specifies the country where the events were observed. This dataset only represents events recorded in Ecuador.\nage_group: The age of the person who is represented in the counts. This dataset only represents infants aged 2 months to almost 5 years of age (59 months).\ndoses: Specifies the doses of PCV received. As described earlier, all possible combinations of the standard-of-care vaccination series and booster are represented here.\nJ12_J18_prim: Primary cause of death is assigned to the ICD-10 codes J12-J18. We encourage you to read more about what these ICD-10 codes represent 4,5.\nacm_noj_prim: Primary cause of death was assigned any other ICD=10 code, excluding only the J chapter, diseases of the respiratory system."
  },
  {
    "objectID": "Pages/Week2/Time Series Analysis_Hands-On Exercise.html#initial-plot-of-the-time-series",
    "href": "Pages/Week2/Time Series Analysis_Hands-On Exercise.html#initial-plot-of-the-time-series",
    "title": "Time Series Analysis",
    "section": "Initial Plot of the Time Series",
    "text": "Initial Plot of the Time Series\nWe begin each time series analysis by examining the entire span of data, typically plotted as a line or scatter plot. All methods for modeling the vaccination introduction time series data require the dates when the vaccine was introduced, in this case the PCV10 vaccine.\n\n# Estimation when the vaccine was introduced in YYYY-MM-DD format.\nvax.intro.date &lt;- as.Date(\"2010-08-01\")\n\n# Date when vaccine efficacy evaluations started; at least 12 months\n# following administration.\nvax.eval.date &lt;- as.Date(\"2011-08-01\")\n\nWe are not going to spend time explaining how to plot using the tidyverse package ggplot2() here. Later in the week you will receive a lecture covering this topic. In the meantime, you are welcome to explore the ggplot2 package documentation or the Data Science and Data Equity (DSDE) group’s online Book of Workshops 7,8.\n\np1 &lt;- \n  ggplot(df, aes(x = date, y = J12_J18_prim)) +\n      geom_line() +\n      labs(title = \"Deaths Categorized by\\nICD-10 Codes J12-18\",\n         x = \"Date\", y = \"Counts for &lt;5 yo\") +\n      # Have y-axis for the two plots be the same.\n      ylim(0, 250) +\n      # Reference line.\n      geom_vline(xintercept = vax.intro.date, col = \"red\", lty = 2) +\n      theme_linedraw()\n   \n\np2 &lt;- \n  ggplot(df, aes(x = date, y = acm_noj_prim)) +\n      geom_line() +\n      labs(title = \"Deaths Categorized by\\nNon-J chapter ICD-10 Codes\",\n         x = \"Date\", y = \"Counts for &lt;5 yo\") +\n      # Have y-axis for the two plots be the same.\n      ylim(0, 250) +\n      # Reference line.\n      geom_vline(xintercept = vax.intro.date, col = \"red\", lty = 2) +\n      theme_linedraw()\n\n# Display the plots side-by-side.\ngrid.arrange(p1, p2, nrow = 1)"
  },
  {
    "objectID": "Pages/Week2/Time Series Analysis_Hands-On Exercise.html#simple-linear-model",
    "href": "Pages/Week2/Time Series Analysis_Hands-On Exercise.html#simple-linear-model",
    "title": "Time Series Analysis",
    "section": "Simple Linear Model",
    "text": "Simple Linear Model\nTo correctly fit a simple linear model to the data, we need to apply a linearization that appropriately reflects its distribution. There are more analytical approaches to achieve this that will not be covered here. We start by visually examining the distribution with a scatter plot.\n\n\n\n\n\n\nNote\n\n\n\nIt is helpful to consider the data generation method, which can provide insights into the likely distribution.\n\n\n\np_base &lt;- \n  ggplot(df, aes(x = date, y = J12_J18_prim)) +\n      geom_point() +\n      labs(title = \"Deaths Scatter Plot\",\n         x = \"Date\", y = \"Counts for &lt;5 yo\") +\n      # Have y-axis for the two plots be the same.\n      ylim(0, NA) +\n      theme_linedraw()\n\np_base\n\n\n\n\n\n\n\n\nVariables that reflect counts (i.e., the number of deaths per month) can be distributed as either Poisson or negative binomial. Note that Poisson regression may artificially narrow confidence intervals when the data is overdispersed. To address this, you can use a negative binomial regression or a quasipoisson model, which accounts for the unexplained variation. In this context, we will apply a negative binomial regression.\n\n# For modeling, we need to use an ordered, discrete variable. Simply, we \n# can use the rownames for this purpose.\ndf &lt;- tibble::rownames_to_column(df, var = \"index\") %&gt;%\n  mutate(index = as.numeric(index))\n\n# Apply the negative binomial regression.\nmod1 &lt;- glm.nb(J12_J18_prim ~ index , data = df)\n\n# Examine the fitting results.\nsummary(mod1)\n\n\nCall:\nglm.nb(formula = J12_J18_prim ~ index, data = df, init.theta = 38.18819754, \n    link = log)\n\nCoefficients:\n              Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  3.8223994  0.0389150   98.22   &lt;2e-16 ***\nindex       -0.0062897  0.0004981  -12.63   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Negative Binomial(38.1882) family taken to be 1)\n\n    Null deviance: 304.02  on 143  degrees of freedom\nResidual deviance: 142.25  on 142  degrees of freedom\nAIC: 976.12\n\nNumber of Fisher Scoring iterations: 1\n\n              Theta:  38.2 \n          Std. Err.:  10.2 \n\n 2 x log-likelihood:  -970.12 \n\n\n\n# Make predictions with confidence intervals.\npred &lt;- predict(mod1, type = \"response\", se.fit = TRUE)\n\n# Add the model predictions and 95% COI to the dataframe.\ndf.pred &lt;- df %&gt;%\n  mutate(se.fit = pred$se.fit, pred = pred$fit) %&gt;%\n  mutate(\n    conf.low = pred - 1.96 * se.fit,\n    conf.high = pred + 1.96 * se.fit\n  )\n\n# Plot the newly created model fitting.\np_sm &lt;- p_base +\n  # Add the fitting line.\n  geom_line(data = df.pred, aes(x = date, y = pred),\n            color = \"#e41a1c\") +\n  # Add the confidence interval.\n  geom_ribbon(data = df.pred, aes(ymin = conf.low, ymax = conf.high), \n              alpha = 0.2, fill = \"blue\") +\n  # Change the title name.\n  labs(title = \"Deaths with a Negative Binomial Fit\")\n\np_sm\n\n\n\n\n\n\n\n\nThis is not a terrible predictor of our trend, but it overlooks many known sources of variance. For instance, we know that infections have seasonal trends, sometimes referred to by its technical term periodicity. Additionally, our baseline population may change from 2005 to 2016, which can consequently shift the overall disease trend in tandem with these baseline changes."
  },
  {
    "objectID": "Pages/Week2/Time Series Analysis_Hands-On Exercise.html#building-the-multiple-linear-model",
    "href": "Pages/Week2/Time Series Analysis_Hands-On Exercise.html#building-the-multiple-linear-model",
    "title": "Time Series Analysis",
    "section": "Building the Multiple Linear Model",
    "text": "Building the Multiple Linear Model\n\n\n\n\nPrinciples of Seasonality - Generated with Yale’s AI Clarity\n\n\n\n\nAdd Controls for Seasonality\nSeasonality often manifests as periodicity, where recurring patterns repeat at regular, fixed intervals of time. This differs from the related concept of cyclicity, which represents recurring patterns that do not occur at regular intervals or consistently appear in regular seasons 9.\nEach seasonal pattern can be decomposed into three components that can either remain constant (stationary) or change over time (non-stationary): mean, variance, and covariance. The figure on the left illustrates the four possible seasonal trends separately. The top-left panel shows a recurring pattern with time-invariant, stationary parameters, while the other panels demonstrate the effects of non-stationary parameters.\n\n\n\n\n\n\nNote\n\n\n\nMost time series data in the natural sciences exhibit seasonality, periodicity, or cyclicity, though these patterns may be difficult to detect when the data is noisy 10.\n\n\nTo control for temporal variations such as seasonality, we employ a simple form of dynamic linear regression (DLR). Essentially, we add variables to our simple linear regression model that represents different subsections of time as new regressors. Applying a DLR allows for changes in the mean value of the underlying regression relationship 11. For the regression function to recognize each date, we need to factorize the date variable to assign the month the observation occured.\n\ndf$month &lt;- as.factor(month(df$date))\n\n# Inspect the first 36 entries.\ndf$month[1:36]\n\n [1] 1  2  3  4  5  6  7  8  9  10 11 12 1  2  3  4  5  6  7  8  9  10 11 12 1 \n[26] 2  3  4  5  6  7  8  9  10 11 12\nLevels: 1 2 3 4 5 6 7 8 9 10 11 12\n\n# Update the model.\nmod2 &lt;- glm.nb(J12_J18_prim ~ date + month, data = df)\nsummary(mod2)\n\n\nCall:\nglm.nb(formula = J12_J18_prim ~ date + month, data = df, init.theta = 88.28907799, \n    link = log)\n\nCoefficients:\n              Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  6.496e+00  2.156e-01  30.132  &lt; 2e-16 ***\ndate        -1.988e-04  1.426e-05 -13.941  &lt; 2e-16 ***\nmonth2      -2.717e-02  8.166e-02  -0.333 0.739391    \nmonth3       3.446e-02  8.086e-02   0.426 0.669971    \nmonth4      -1.892e-01  8.441e-02  -2.241 0.025011 *  \nmonth5      -4.964e-02  8.228e-02  -0.603 0.546315    \nmonth6      -1.571e-01  8.409e-02  -1.868 0.061714 .  \nmonth7      -2.118e-01  8.513e-02  -2.488 0.012840 *  \nmonth8      -2.076e-01  8.518e-02  -2.437 0.014804 *  \nmonth9      -3.266e-01  8.749e-02  -3.733 0.000189 ***\nmonth10     -2.253e-01  8.574e-02  -2.627 0.008603 ** \nmonth11     -2.467e-01  8.626e-02  -2.859 0.004243 ** \nmonth12     -3.386e-01  8.813e-02  -3.842 0.000122 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Negative Binomial(88.2891) family taken to be 1)\n\n    Null deviance: 404.40  on 143  degrees of freedom\nResidual deviance: 143.84  on 131  degrees of freedom\nAIC: 959.32\n\nNumber of Fisher Scoring iterations: 1\n\n              Theta:  88.3 \n          Std. Err.:  41.2 \n\n 2 x log-likelihood:  -931.319 \n\n\n\n# Make predictions with confidence intervals.\npred2 &lt;- predict(mod2, type = \"response\", se.fit = TRUE)\n\n# Add the model predictions and 95% COI to the dataframe.\ndf.pred2 &lt;- df %&gt;%\n  mutate(se.fit = pred2$se.fit, pred = pred2$fit) %&gt;%\n  mutate(\n    conf.low = pred - 1.96 * se.fit,\n    conf.high = pred + 1.96 * se.fit\n  )\n\n# Plot the newly created model fitting.\np_season &lt;- p_base +\n  # Add the fitting line.\n  geom_line(data = df.pred2, aes(x = date, y = pred),\n            color = \"#e41a1c\") +\n  # Add the confidence interval.\n  geom_ribbon(data = df.pred2, aes(ymin = conf.low, ymax = conf.high), \n              alpha = 0.2, fill = \"blue\") +\n  # Change the title name.\n  labs(title = \"Deaths with a Negative Binomial Fit\\nSeasonality Term Included\")\n\np_season\n\n\n\n\n\n\n\n\n\n\nAdd Controls for Baseline Shifts\n\n\n\n\nFigure by Dr. Dan Weinberger - Generated with Yale’s AI Clarity  6\n\n\n\nWe can expect that the baseline population changes over time, introducing unaccounted-for heterogeneous variance to our model. In epidemiology, it is standard to contextualize raw data by applying a denominator, or offset, that represents the population at risk of the disease.\nThe figure on the right illustrates how the baseline for the number of hospitalizations normalizes when we apply the population offset. Keep in mind that this ratio is often scaled to “per 100,000 persons.” 6.\n\n\\frac{n_{cases}}{n_{population}} \\times 100,000\n\nIn real-world scenarios, the total base population is not always reported, or the coverage might be unreliable for the entire span of the time series. Alternatively, we can apply denominators such as the number of people using the healthcare system, the number of people hospitalized, and similar metrics 6. In this example, we could apply the total population size or all non-respiratory causes of mortality.\n\n# Create the offset using all deaths not coded as J. Transform the values\n# to log before use in the negative binomial fitting.\ndf$log.offset &lt;- log(df$acm_noj_prim)\n\n# Refit the model with an offset.\nmodel3 &lt;- glm.nb(J12_J18_prim ~ index + month + offset(log.offset), data = df)\nsummary(model3)\n\n\nCall:\nglm.nb(formula = J12_J18_prim ~ index + month + offset(log.offset), \n    data = df, init.theta = 59.08482937, link = log)\n\nCoefficients:\n              Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -1.3687586  0.0672076 -20.366  &lt; 2e-16 ***\nindex       -0.0034881  0.0004599  -7.584 3.36e-14 ***\nmonth2       0.1651225  0.0873636   1.890   0.0587 .  \nmonth3       0.1509818  0.0867654   1.740   0.0818 .  \nmonth4      -0.0005318  0.0899497  -0.006   0.9953    \nmonth5       0.0704091  0.0880910   0.799   0.4241    \nmonth6       0.0227796  0.0896446   0.254   0.7994    \nmonth7      -0.0122476  0.0906691  -0.135   0.8925    \nmonth8       0.0077403  0.0907734   0.085   0.9320    \nmonth9      -0.0526956  0.0929670  -0.567   0.5708    \nmonth10     -0.0053806  0.0912550  -0.059   0.9530    \nmonth11      0.0443924  0.0916610   0.484   0.6282    \nmonth12     -0.0283404  0.0935272  -0.303   0.7619    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Negative Binomial(59.0848) family taken to be 1)\n\n    Null deviance: 219.59  on 143  degrees of freedom\nResidual deviance: 145.49  on 131  degrees of freedom\nAIC: 977.59\n\nNumber of Fisher Scoring iterations: 1\n\n              Theta:  59.1 \n          Std. Err.:  21.1 \n\n 2 x log-likelihood:  -949.594 \n\n\n\n# Make predictions with confidence intervals.\npred3 &lt;- predict(model3, type = \"response\", se.fit = TRUE)\n\n# Add the model predictions and 95% COI to the dataframe.\ndf.pred3 &lt;- df %&gt;%\n  mutate(se.fit = pred3$se.fit, pred = pred3$fit) %&gt;%\n  mutate(\n    conf.low = pred - 1.96 * se.fit,\n    conf.high = pred + 1.96 * se.fit\n  )\n\n# Plot the newly created model fitting.\np_offset &lt;- p_base +\n  # Add the fitting line.\n  geom_line(data = df.pred3, aes(x = date, y = pred),\n            color = \"#e41a1c\") +\n  # Add the confidence interval.\n  geom_ribbon(data = df.pred3, aes(ymin = conf.low, ymax = conf.high), \n              alpha = 0.2, fill = \"blue\") +\n  # Change the title name.\n  labs(title = \"Deaths with a Negative Binomial Fit\\nSeasonality Term and Offset\")\n\np_offset\n\n\n\n\n\n\n\n\nNotice that the seasonality trend, which was clear in the previous plot, is disrupted when the population offset is applied. Now that we have our baseline model, we are ready to proceed with examining the effect of the intervention with PCV10 after its introduction on 2010-08-01."
  },
  {
    "objectID": "Pages/Week2/Time Series Analysis_Hands-On Exercise.html#does-the-disease-trend-or-level-change",
    "href": "Pages/Week2/Time Series Analysis_Hands-On Exercise.html#does-the-disease-trend-or-level-change",
    "title": "Time Series Analysis",
    "section": "Does the Disease Trend or Level Change?",
    "text": "Does the Disease Trend or Level Change?\nIn this section, we will test whether the trend or level of pneumonia deaths due to pneumococcal disease changes after the introduction of the PCV10 vaccine. To accomplish this, we will set up a counterfactual model to represent what our base model predicts the death counts would have been if the vaccine had not been introduced to the population. Similarly, a factual model will be applied to represent what actually happened with the vaccine rollout.\nWe will then compare the factual model against the counterfactual model to determine if a difference is identified in the period following vaccine introduction. This process is generally referred to as detecting vaccine effect or vaccine impact.\n\n\n\n\n\n\nWarning\n\n\n\nYou want to be careful when applying causal inference methods to model building, perturbation, or prediction questions. While our analysis uses a causal framework to detect the possible impact of the vaccine introduction, this does not permit us to make definitive claims of causation.\nAdditionally, it is important to remember that the model we have built may have other flaws influencing its sensitivity to the pre/post vaccination period. It is always best practice to test a model under different conditions and subject it to various tests to robustly justify its accuracy, precision, and generalizability.\n\n\nThe figure below illustrates the three phases of the vaccine rollout: pre-vaccination, post-vaccination, and a latency period between the start of vaccine distribution and efficacy evaluation. Recall that our base negative binomial model accounts for three effector variables: an ordered, discrete variable organizing the outcomes temporally without seasonality, the month to capture seasonal periodicity, and an offset to the baseline accounting for population changes. Our baseline model is expected to, and is therefore assumed to, detect the drop in average cases during the post-vaccination period.\n\n\n\nFigure by Dr. Dan Weinberger - From his recorded lecture Interrupted time series analysis 12.\n\n\nThis model, however, does not explicitly assign outcomes to the three phases represented in the figure. Therefore, we need to set up variables that distinguish outcomes before and after the vaccine introduction, as well as before and after the entry to the vaccine evaluation phase, in order to convert the base model into our factual and counterfactual models.\nWe also need to regenerate the model to produce factually-based and counterfactually-based predictions for comparison. There are three modeling approaches we will evaluate here, each employing a different method to define the pre/post and latency vaccination periods. These differences will impact our inference about vaccine effectiveness.\n\nInterrupted Time Series with Disconnected Segments: This method fits different line segments through the data and tests whether the slope or level of the disease changes. It can sometimes result in abrupt jumps when fitting the model.\nInterrupted Time Series with Connected Segments (Spline Model): This method allows the slope to change in the post-vaccine period but ensures the change is smooth.\nExtrapolation Based on the Pre-Vaccine Period: This method fits the model to data from the pre-vaccine period only and extrapolates the trend to the post-vaccine period.\n\n\nMethod 1. Interrupted Time Series with Disconnected Segments\nWe will add two new binary variables to our dataset: vax_intro and vax_eval. The vax_intro variable will be assigned a value of 0 before 2010-08-01 and 1 afterward; similarly, the vax_eval variable will be assigned a value of 0 before 2011-08-01 and 1 afterward.\n\n# Assign outcomes to the known vaccine phases.\ndf &lt;- df %&gt;%\n  mutate(\n    vax_intro = ifelse(date &gt;= vax.intro.date, 1, 0),\n    vax_eval = ifelse(date &gt;= vax.eval.date, 1, 0)\n  )\n\n# View the changes by randomly selecting dates.\ndf[sort(sample(1:144, 10)), c(\"index\", \"date\", \"vax_intro\", \"vax_eval\")]\n\n# A tibble: 10 × 4\n   index date       vax_intro vax_eval\n   &lt;dbl&gt; &lt;date&gt;         &lt;dbl&gt;    &lt;dbl&gt;\n 1    12 2005-12-01         0        0\n 2    22 2006-10-01         0        0\n 3    31 2007-07-01         0        0\n 4    36 2007-12-01         0        0\n 5    37 2008-01-01         0        0\n 6    41 2008-05-01         0        0\n 7    71 2010-11-01         1        0\n 8    93 2012-09-01         1        1\n 9   108 2013-12-01         1        1\n10   127 2015-07-01         1        1\n\n\nIn linear modeling, it is common to include interaction terms. These terms allow the model to capture interdependencies between variables, showing how the effect of one variable on the outcome changes depending on the level of another variable. Interaction terms also enable the identification of synergistic effects, where the combined effect of two variables is greater (or less) than the sum of their individual effects 13.\nEquation 1 shows a multiple linear regression without an interaction term, and Equation 2 shows the inclusion of the interaction term. Notice that including the interaction term impacts the slope, making it dependent on the value of the other predictor. For example, the impact of X_1, holding X_2 constant, on the slope of Y is a function of (\\beta_1 + \\beta_3)\\ X_2 13.\n\\begin{align}\nY &= \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\epsilon\\\\\nY &= \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 (X_1 \\times X_2) + \\epsilon\n\\end{align}\nWe will not spend more time discussing interaction terms but will briefly examine if they enhance the model’s predictive power. Note that while interaction terms can improve predictive accuracy, they also complicate the model and may result in a trade-off between predictive power and generalizability to other datasets.\nFirst we will generate the two models, then we will evaluate their performance side-by-side.\n\nNo Interaction Terms\nCreate a simple step-change model without an interaction term involving the newly added variables, vax_intro and vax_eval.\n\n# Additional seasonality controls covered in the collapsed box above.\nmod_method1a &lt;- glm.nb(J12_J18_prim ~ index + month + offset(log.offset) +\n                         # Change in disease during administration and \n                         # evaluation period.\n                         vax_intro + vax_eval, data = df)\n\nsummary(mod_method1a)\n\n\nCall:\nglm.nb(formula = J12_J18_prim ~ index + month + offset(log.offset) + \n    vax_intro + vax_eval, data = df, init.theta = 64.37470892, \n    link = log)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -1.382710   0.069290 -19.956   &lt;2e-16 ***\nindex       -0.002510   0.001014  -2.476   0.0133 *  \nmonth2       0.163233   0.086008   1.898   0.0577 .  \nmonth3       0.147780   0.085409   1.730   0.0836 .  \nmonth4      -0.004393   0.088661  -0.050   0.9605    \nmonth5       0.065323   0.086796   0.753   0.4517    \nmonth6       0.017042   0.088446   0.193   0.8472    \nmonth7      -0.018766   0.089510  -0.210   0.8339    \nmonth8       0.006893   0.089471   0.077   0.9386    \nmonth9      -0.054212   0.091693  -0.591   0.5544    \nmonth10     -0.006595   0.089942  -0.073   0.9416    \nmonth11      0.041625   0.090381   0.461   0.6451    \nmonth12     -0.031725   0.092285  -0.344   0.7310    \nvax_intro   -0.167101   0.083391  -2.004   0.0451 *  \nvax_eval     0.076690   0.083253   0.921   0.3570    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Negative Binomial(64.3747) family taken to be 1)\n\n    Null deviance: 225.73  on 143  degrees of freedom\nResidual deviance: 145.44  on 129  degrees of freedom\nAIC: 977.59\n\nNumber of Fisher Scoring iterations: 1\n\n              Theta:  64.4 \n          Std. Err.:  24.3 \n\n 2 x log-likelihood:  -945.595 \n\n\n\n## --------------------\n## Generate the factual model\n\n# Make predictions with confidence intervals.\npred4 &lt;- predict(mod_method1a, type = \"response\", se.fit = TRUE)\n\n\n## --------------------\n## Prepare a plot to visualize the factual model predictions\n\n# Add the model predictions and 95% COI to the dataframe.\ndf.pred.its &lt;- df %&gt;%\n  mutate(se.fit = pred4$se.fit, pred = pred4$fit) %&gt;%\n  mutate(\n    conf.low = pred - 1.96 * se.fit,\n    conf.high = pred + 1.96 * se.fit\n  )\n\n# Plot the newly created model fitting.\np_m1a &lt;- p_base +\n  # Add the fitting line.\n  geom_line(data = df.pred.its, aes(x = date, y = pred),\n            color = \"#377eb8\") +\n  # Add the confidence interval.\n  geom_ribbon(data = df.pred.its, aes(ymin = conf.low, ymax = conf.high), \n              alpha = 0.2, fill = \"blue\") +\n  # Change the title name.\n  labs(title = \"Deaths with a Negative Binomial Fit\\nInterupted Time Series - No Interaction Terms\")\n\n\n## --------------------\n## Generate the counterfactual model\n\n# Initialize the vacccine effect variables by setting them to 0.\ndf.counterfactual.a &lt;- df\ndf.counterfactual.a$vax_intro &lt;- 0\ndf.counterfactual.a$vax_eval &lt;- 0\n\n# Generate the fitted values using the counterfactual setup instead.\ndf.pred.its$pred4.cf.a &lt;- predict(mod_method1a, type = \"response\", \n                                 newdata = df.counterfactual.a)\n\n\n## --------------------\n## Rate ratio to evaluate performance\n\n# Generate the rate ratio between the factual and counterfactual values.\ndf.pred.its$rr.its.a &lt;- df.pred.its$pred/df.pred.its$pred4.cf.a\n\n\n\nWith Interaction Terms\nCreate the more complicated model option by adding an interaction term involving vax_intro and vax_eval, keeping both individually and including with an interaction with index.\n\nmod_method1b &lt;- glm.nb(J12_J18_prim ~ index + month + offset(log.offset) +\n                         # Add the counterfactuals with an interaction term.\n                         vax_intro + vax_intro*index +\n                         vax_eval + vax_eval*index, data = df)\n\nsummary(mod_method1b)\n\n\nCall:\nglm.nb(formula = J12_J18_prim ~ index + month + offset(log.offset) + \n    vax_intro + vax_intro * index + vax_eval + vax_eval * index, \n    data = df, init.theta = 65.62229438, link = log)\n\nCoefficients:\n                 Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)     -1.410897   0.074005 -19.065   &lt;2e-16 ***\nindex           -0.001660   0.001288  -1.288   0.1976    \nmonth2           0.163960   0.085713   1.913   0.0558 .  \nmonth3           0.148234   0.085153   1.741   0.0817 .  \nmonth4          -0.003138   0.088498  -0.035   0.9717    \nmonth5           0.067516   0.086678   0.779   0.4360    \nmonth6           0.019034   0.088478   0.215   0.8297    \nmonth7          -0.016266   0.089654  -0.181   0.8560    \nmonth8           0.003650   0.089526   0.041   0.9675    \nmonth9          -0.055524   0.091668  -0.606   0.5447    \nmonth10         -0.008492   0.089787  -0.095   0.9247    \nmonth11          0.039981   0.090176   0.443   0.6575    \nmonth12         -0.032091   0.092017  -0.349   0.7273    \nvax_intro        0.267869   1.533398   0.175   0.8613    \nvax_eval        -0.184445   1.541114  -0.120   0.9047    \nindex:vax_intro -0.006384   0.020854  -0.306   0.7595    \nindex:vax_eval   0.004214   0.020870   0.202   0.8400    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Negative Binomial(65.6223) family taken to be 1)\n\n    Null deviance: 227.08  on 143  degrees of freedom\nResidual deviance: 145.13  on 127  degrees of freedom\nAIC: 980.44\n\nNumber of Fisher Scoring iterations: 1\n\n              Theta:  65.6 \n          Std. Err.:  25.0 \n\n 2 x log-likelihood:  -944.439 \n\n\n\n## --------------------\n## Generate the factual model\n\n# Make predictions with confidence intervals.\npred5 &lt;- predict(mod_method1b, type = \"response\", se.fit = TRUE)\n\n\n## --------------------\n## Prepare a plot to visualize the factual model predictions\n\n# Add the model predictions and 95% COI to the dataframe.\ndf.pred.its &lt;- df.pred.its %&gt;%\n  mutate(se.fit = pred5$se.fit, pred = pred4$fit) %&gt;%\n  mutate(\n    conf.low = pred - 1.96 * se.fit,\n    conf.high = pred + 1.96 * se.fit\n  )\n\n# Plot the newly created model fitting.\np_m1b &lt;- p_base +\n  # Add the fitting line.\n  geom_line(data = df.pred.its, aes(x = date, y = pred),\n            color = \"#377eb8\") +\n  # Add the confidence interval.\n  geom_ribbon(data = df.pred.its, aes(ymin = conf.low, ymax = conf.high), \n              alpha = 0.2, fill = \"blue\") +\n  # Change the title name.\n  labs(title = \"Deaths with a Negative Binomial Fit\\nInterupted Time Series - With Interaction Terms\")\n\n\n## --------------------\n## Generate the counterfactual model\n\n# Initialize the vacccine effect variables by setting them to 0.\ndf.counterfactual.b &lt;- df\ndf.counterfactual.b$vax_intro &lt;- 0\ndf.counterfactual.b$vax_eval &lt;- 0\n\n# Generate the fitted values using the counterfactual setup instead.\ndf.pred.its$pred4.cf.b &lt;- predict(mod_method1b, type = \"response\", \n                                  newdata = df.counterfactual.b)\n\n\n## --------------------\n## Rate ratio to evaluate performance\n\n# Generate the rate ratio between the factual and counterfactual values.\ndf.pred.its$rr.its.b &lt;- df.pred.its$pred/df.pred.its$pred4.cf.b\n\n\n\nCompare Model Complexity\nLet’s start by comparing how the fit differs between the models.\n\n# Overlay onto plot.\np_m1a_pred &lt;- p_m1a +\n geom_line(data = df.pred.its, aes(x = date, y = pred4.cf.a),\n           color = '#e41a1c', lty = 2)\n\n# Overlay onto plot.\np_m1b_pred &lt;- p_m1b +\n geom_line(data = df.pred.its, aes(x = date, y = pred4.cf.b),\n           color = '#e41a1c', lty = 2)\n\ngrid.arrange(p_m1a_pred, p_m1b_pred, ncol = 1)\n\n\n\n\n\n\n\n\nWe do see differences with the prediction adherance to the data we trained the model on. By qualitative examination, it appears the model with interaction terms diverge more in the post-vaccination period.\nThe Akaike Information Criterion (AIC) is a one measure used to compare the goodness of fit of different statistical models, while also accounting for model complexity. It helps in model selection by balancing model fit and complexity, otherwise called the bias/variance trade-off.\nThere are different variations of AIC, but the basic definition is\n\n\\text{AIC} = 2k - 2\\ln(\\mathcal{L})\n\nwhere k is the number of parameters in the model and \\mathcal{L} is the maximum likelihood of the model.\n\nAIC(mod_method1a, mod_method1b)\n\n             df      AIC\nmod_method1a 16 977.5948\nmod_method1b 18 980.4387\n\n\nThe results indicate that the AIC score is worse (higher) when interaction terms are included. Since there is no significant gain in performance, we will prefer the simpler model for better generalizability.\nBelow, we also observe that the interaction terms create unexpected trends in the factual/counterfactual ratio. Notably, the ratio is not 1 (indicating that factual and counterfactual predictions are the same) in the pre-vaccination period, which is not ideal.\n\n# Inspect methods sensitivity to vaccine impact.\np_ratio &lt;- \n  ggplot(df.pred.its, aes(x = date, y = rr.its.a)) +\n      geom_line() +\n      labs(title = \"Rate Ratio of the ITS Model\",\n         x = \"Date\", y = \"Rate ratio\") +\n      ylim(0, NA) +\n      geom_vline(xintercept = vax.intro.date, col = \"red\", lty = 2) +\n      annotate(\"text\", x = vax.intro.date + 30, y = 00.1, label = \"Vaccine Introduced\", color = \"red\", hjust = 0) +\n      theme_linedraw()\n\np_ratio +\n  geom_line(data = df.pred.its, aes(x = date, y = rr.its.b),\n            color = \"#4daf4a\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode Like a Pro\n\n\n\nWe won’t try to estimate a confidence interval on the rate ratio here. Students are encouraged to checkout the the InterventionEvaluatR package, which will automatically calculate the confidence intervals for these ratios.\n\n\n\n\n\nMethod 2. Interrupted Time Series with Connected Segments (Spline Model)\nIn this case, we force the changes to be smooth so that we don’t get a drastic jump after vaccination introduction that prematurly shows impact during the expected latency period. Therefore, we will add two new variables to our dataset: spl1 and spl2. The spl1 variable will be assigned a value of 0 before 2010-08-01 and index - intro.index + 1 afterward; similarly, the spl2 variable will be assigned a value of 0 before 2011-08-01 and index - eval.index + 1 afterward.\n\n# Identifies the row index that represents when the vaccine was introduced\n# and when the evaluation period started.\nintro.index &lt;- which(df$date == vax.intro.date)\neval.index  &lt;- which(df$date == vax.eval.date)\n\n# Assign outcomes to the known vaccine phases.\ndf &lt;- df %&gt;%\n  mutate(\n    spl1 = ifelse(index - intro.index + 1 &lt; 0, 0, index - intro.index + 1),\n    spl2 = ifelse(index - eval.index + 1 &lt; 0, 0, index - eval.index + 1)\n  )\n\n# Inspect the changes.\ndf[sort(sample(1:144, 10)), c(\"date\", \"index\", \"spl1\", \"spl2\")]\n\n# A tibble: 10 × 4\n   date       index  spl1  spl2\n   &lt;date&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 2005-08-01     8     0     0\n 2 2008-08-01    44     0     0\n 3 2008-11-01    47     0     0\n 4 2009-07-01    55     0     0\n 5 2011-11-01    83    16     4\n 6 2012-09-01    93    26    14\n 7 2012-10-01    94    27    15\n 8 2014-09-01   117    50    38\n 9 2014-10-01   118    51    39\n10 2015-06-01   126    59    47\n\n\n\nmod_method2 &lt;- glm.nb(J12_J18_prim ~ index + month + offset(log.offset) +\n                        # Post-vaccine changes.\n                        spl1 + spl2, data = df)\n\nsummary(mod_method2)\n\n\nCall:\nglm.nb(formula = J12_J18_prim ~ index + month + offset(log.offset) + \n    spl1 + spl2, data = df, init.theta = 61.04668124, link = log)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -1.397121   0.074586 -18.732   &lt;2e-16 ***\nindex       -0.002349   0.001237  -1.899   0.0576 .  \nmonth2       0.164546   0.086841   1.895   0.0581 .  \nmonth3       0.151023   0.086225   1.752   0.0799 .  \nmonth4      -0.001397   0.089442  -0.016   0.9875    \nmonth5       0.069656   0.087565   0.795   0.4263    \nmonth6       0.020998   0.089150   0.236   0.8138    \nmonth7      -0.013534   0.090170  -0.150   0.8807    \nmonth8       0.006429   0.090270   0.071   0.9432    \nmonth9      -0.055311   0.092521  -0.598   0.5500    \nmonth10     -0.008406   0.090793  -0.093   0.9262    \nmonth11      0.040934   0.091221   0.449   0.6536    \nmonth12     -0.032326   0.093106  -0.347   0.7284    \nspl1        -0.007961   0.007038  -1.131   0.2580    \nspl2         0.007591   0.007271   1.044   0.2965    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Negative Binomial(61.0467) family taken to be 1)\n\n    Null deviance: 221.95  on 143  degrees of freedom\nResidual deviance: 145.72  on 129  degrees of freedom\nAIC: 980.3\n\nNumber of Fisher Scoring iterations: 1\n\n              Theta:  61.0 \n          Std. Err.:  22.3 \n\n 2 x log-likelihood:  -948.296 \n\n\n\n## --------------------\n## Generate the factual model\n\n# Make predictions with confidence intervals.\ndf.pred.spl &lt;- df %&gt;%\n  mutate(pred.spl = predict(mod_method2, type = \"response\"))\n\n\n## --------------------\n## Generate the counterfactual model\n\n# Initialize the vacccine effect variables by setting them to 0.\ndf.counterfactual.spl &lt;- df\ndf.counterfactual.spl$spl1 &lt;- 0\ndf.counterfactual.spl$spl2 &lt;- 0\n\n# Generate the fitted values.\ndf.pred.spl$pred.spl.cf &lt;- predict(mod_method2, type = \"response\", \n                                   newdata = df.counterfactual.spl)\n\n\n## --------------------\n## Rate ratio to evaluate performance\n\n# Generate the rate ratio between the fitted and counterfactual values.\ndf.pred.spl$rr.spline &lt;- df.pred.spl$pred.spl/df.pred.spl$pred.spl.cf\n\n\n## --------------------\n## Prepare a plot to visualize the factual model predictions\n\n# Make predictions with confidence intervals.\npred6 &lt;- predict(mod_method2, type = \"response\", se.fit = TRUE)\n\n# Add the model predictions and 95% COI to the dataframe.\ndf.pred.spl &lt;- df.pred.spl %&gt;%\n  mutate(se.fit = pred6$se.fit, pred = pred6$fit) %&gt;%\n  mutate(\n    conf.low = pred - 1.96 * se.fit,\n    conf.high = pred + 1.96 * se.fit\n  )\n\n\np_m2 &lt;- \n  ggplot(df.pred.spl, aes(x = date, y = J12_J18_prim)) +\n      geom_point() +\n      # Add the fitted line.\n      geom_line(data = df.pred.spl, aes(x = date, y = pred.spl),\n                color = \"#377eb8\") +\n      # Add the confidence interval.\n      geom_ribbon(data = df.pred.spl, aes(ymin = conf.low, ymax = conf.high),\n                  alpha = 0.2, fill = \"blue\") +\n      # Add the counterfactual line.\n      geom_line(data = df.pred.spl, aes(x = date, y = pred.spl.cf),\n                color = \"#e41a1c\", lty = 2) +\n      labs(title = \"Deaths with a Negative Binomial Fit\\nITS with Spline Smoothing\",\n         x = \"Date\", y = \"Counts for &lt;5 yo\") +\n      theme_linedraw()\n\np_m2\n\n\n\n\n\n\n\n\nWe can see here that the decline follows a smoother trajectory, by design. We can check the counterfactual performace by examining the ratio again.\n\n# Inspect methods sensitivity to vaccine impact.\np_ratio &lt;- p_ratio +\n     geom_line(data = df.pred.spl, aes(x = date, y = rr.spline),\n               color = \"#4daf4a\") +\n      # Update the title.\n      labs(title = \"Rate Ratio with the Spline Model\")\n\np_ratio\n\n\n\n\n\n\n\n\n\n\nMethod #3: Extrapolation Based on the Pre-Vaccine Period\nNow we are going to model only the pre-vaccination period and forecast outcomes based on the restricted fitting into the post-vaccination period. We will create a new variable that excludes any records following 2010-08-01, J12_J18_prim_pre.\n\n# Create a new variable where it is J12_J18_prim until the vaccine is introduced,\n# then it is NA afterwards.\ndf$J12_J18_prim_pre &lt;- df$J12_J18_prim\ndf$J12_J18_prim_pre[which(df$date &gt;= vax.intro.date)] &lt;- NA\n\n# Inspect the changes.\ndf[sort(sample(1:144, 10)), c(\"date\", \"J12_J18_prim_pre\")]\n\n# A tibble: 10 × 2\n   date       J12_J18_prim_pre\n   &lt;date&gt;                &lt;dbl&gt;\n 1 2006-02-01               53\n 2 2006-12-01               42\n 3 2007-06-01               35\n 4 2011-06-01               NA\n 5 2011-11-01               NA\n 6 2012-06-01               NA\n 7 2012-09-01               NA\n 8 2012-10-01               NA\n 9 2013-06-01               NA\n10 2016-04-01               NA\n\n\n\nmod_method3 &lt;- glm.nb(J12_J18_prim_pre ~ index + month + offset(log.offset), data = df)\n\n# Add the prediction using the smoothed model.\ndf.pred.pre &lt;- df %&gt;%\n  mutate(pred.pre = predict(mod_method3, type = \"response\", newdata = df))\n\nUnlike before, the original observed variable J12_J18_prim now represents the factual model, as we regressed on a subset of data assuming nothing was known following the vaccine introduction. The prediction results using the restricted regressand represents the counterfactual model, since it extrapolates data points missing from the model fitting.\n\n## --------------------\n## Rate ratio to evaluate performance\n\n# Generate the rate ratio between the factual and counterfactual values.\ndf.pred.pre$rr.trend &lt;- df$J12_J18_prim/df.pred.pre$pred.pre\n\n\n## --------------------\n## Prepare a plot to visualize the factual model predictions\n\n# Make predictions with confidence intervals.\npred7 &lt;- predict(mod_method3, type = \"response\", se.fit = TRUE, newdata = df)\n\n# Add the model predictions and 95% COI to the dataframe.\ndf.pred.pre &lt;- df.pred.pre %&gt;%\n  mutate(se.fit = pred7$se.fit, pred = pred7$fit) %&gt;%\n  mutate(\n    conf.low = pred - 1.96 * se.fit,\n    conf.high = pred + 1.96 * se.fit\n  )\n\n\np_m3 &lt;- \n  ggplot(df.pred.pre, aes(x = date, y = J12_J18_prim)) +\n      geom_point() +\n      # Add the fitted line.\n      geom_line(data = df.pred.pre, aes(x = date, y = pred.pre),\n                color = \"#377eb8\") +\n      # Add the confidence interval.\n      geom_ribbon(data = df.pred.pre, aes(ymin = conf.low, ymax = conf.high),\n                  alpha = 0.2, fill = \"blue\") +\n      labs(title = \"Deaths with a Negative Binomial Fit\\nExtrapolating Based on the Pre-Vaccine Period\",\n         x = \"Date\", y = \"Counts for &lt;5 yo\") +\n      theme_linedraw()\n\np_m3\n\n\n\n\n\n\n\n\n\n# Inspect methods sensitivity to vaccine impact.\np_ratio &lt;- p_ratio +\n  geom_line(data = df.pred.pre, aes(x = date, y = rr.trend),\n            color = '#377eb8') +\n  # Update the title.\n  labs(title = \"Rate Ratio with the Extrapolation Model\")\n\np_ratio\n\n\n\n\n\n\n\n\n\n\nCompare the Methods\nLet’s compare the rate ratio estimates from the three models. As a rough estimate, we will average the point-by-point estimates of the rate ratio during the evaluation period.\n\n# Create a Boolean variable that defines the vaccine evaluation period.\neval.period &lt;- df$date &gt; vax.eval.date\n\n# Calculate the average outcome during the vaccine evaluation period.\nrr.its.eval    &lt;- mean(df.pred.its$rr.its.a[eval.period])\nrr.spline.eval &lt;- mean(df.pred.spl$rr.spline[eval.period])\nrr.trend.eval  &lt;- sum(df.pred.pre$J12_J18_prim[eval.period])/sum(df.pred.pre$pred.pre[eval.period])\n\n# Average decline detected from 1.\nround(c(\"ITS - Disconnected\" = rr.its.eval, \"ITS - Spline\" = rr.spline.eval,\n        \"Extrapolation\" = rr.trend.eval), digits = 2)\n\nITS - Disconnected       ITS - Spline      Extrapolation \n              0.91               0.90               0.86 \n\n# Percent decline detected.\n100*(1 - round(c(\"ITS - Disconnected\" = rr.its.eval, \"ITS - Spline\" = rr.spline.eval, \n                 \"Extrapolation\" = rr.trend.eval), digits = 2))\n\nITS - Disconnected       ITS - Spline      Extrapolation \n                 9                 10                 14 \n\n\nTwo of the modeling methods identified a decrease of approximately 10% in pneumonia deaths due to pneumococcal disease after the release of the PCV10 vaccine. The extrapolation method indicates a stronger vaccine effect; however, we expect some of this result to be spurious due to the level of random noise introduced by applying the observed values in the counterfactual ratio calculation.\n\n\n\n\n\n\nCaution\n\n\n\nDepending on the dataset, these three methods may or may not agree to a greater extent."
  },
  {
    "objectID": "Pages/Week2/Time Series Analysis_Hands-On Exercise.html#references",
    "href": "Pages/Week2/Time Series Analysis_Hands-On Exercise.html#references",
    "title": "Time Series Analysis",
    "section": "References",
    "text": "References\n\n\n1. Weinberger, D. M. (2022).\n\n\n2. Weinberger, D. M. et al. Data analysis workshop at ISPPD | workshop on the evaluation of vaccine impact webpage. (2022).\n\n\n3. Oliveira, L. H. D. et al. Declines in pneumonia mortality following the introduction of pneumococcal conjugate vaccines in latin american and caribbean countries. Clinical Infectious Diseases 73, 306–313 (2021).\n\n\n4. Oliveira, L. H. D. et al. Declines in pneumonia mortality following the introduction of pneumococcal conjugate vaccines in latin american and caribbean countries - supplementary table 2. Clinical Infectious Diseases 73, 306–313 (2021).\n\n\n5. icd10data.com. 2025 ICD-10-CM codes J09-J18: Influenza and pneumonia.\n\n\n6. Weinberger, D. M. et al. 1. Vaccine impact; administrative data sources and their pitfalls - YouTube. YouTube (2021).\n\n\n7. Wickham, H. et al. Create Elegant Data Visualisations Using the Grammar of Graphics • Ggplot2. (Springer-Verlag, 2016).\n\n\n8. Golden, S. Data Visualization with Ggplot2 – Book of Workshops. (2025).\n\n\n9. Ninja, N. N. Periodicity: Detecting rhythms in data - let’s data science. (2023).\n\n\n10. Ramanathan, K. et al. Assessing seasonality variation with harmonic regression: Accommodations for sharp peaks. International Journal of Environmental Research and Public Health 17, (2020).\n\n\n11. Young, P. C. Recursive Estimation and Time-Series Analysis. Recursive Estimation and Time-Series Analysis (Springer Berlin Heidelberg, 2011). doi:10.1007/978-3-642-21981-8.\n\n\n12. Weinberger, D. M. et al. 2. Interrupted time series analysis. YouTube (2021).\n\n\n13. C, D. M., A, E. P. & Vining, G. G. Introducing to Linear Regression Analysis (6th Ed.). John Wiley and Sons 642 (Wiley Series, 2012)."
  },
  {
    "objectID": "Pages/Materials/Student Codespaces/bdsy-phm-individual/Week2/Time Series Analysis_Hands-On Exercise.html",
    "href": "Pages/Materials/Student Codespaces/bdsy-phm-individual/Week2/Time Series Analysis_Hands-On Exercise.html",
    "title": "Time Series Analysis",
    "section": "",
    "text": "This workshop was developed by Dr. Dan Weinberger from the Yale School of Public Health (YSPH) for the 12^{th} International Symposium on Pneumococci and Pneumococcal Diseases (ISPPD-12) hosted in Toronto, Canada in June 2022. Here, we have adapted his workshop session #1, which compares different methods for fitting a time series dataset 1. Students are encouraged to explore the workshop content that Dr. Weinberger put together to learn more about the subject:\n\nGitHub: Find the code, original data, and slides 1.\nWorkshop Webpage: Find a longer description of the workshop in the format of a mini course with links to video presentations of select sections - Workshop on the evaluation of vaccine impact 2.\n\nThe data used here is a subset of the complete set used in Oliveira, L. H. D. et al. 2020 3. In their paper, national-level mortality data spanning from 2000 to 2016 for children under 5 years of age was compiled and standardized by national mortality registers in 10 Latin American and Caribbean countries. The primary cause of death was classified using the International Classification of Diseases, Tenth Revision (ICD-10) codes (Influenza and pneumonia J09-J18 or Oliveira, L. H. D. et al. 2020 Supplementary Table 2) 4,5. The goal of the paper was to evaluate the efficacy of pneumococcal conjugate vaccines (PCVs) in children who received any part of the standard-of-care vaccination series.\nToday, we are going to examine the efficacy of pneumococcal conjugate vaccines (PCVs) in children aged 2 to 59 months in Ecuador. The standard-of-care vaccination series involves a primary series of either two doses (administered at 2 and 4 months of age) or three doses (administered at 2, 4, and 6 months of age), with the option of an additional booster dose (administered at 12-18 months of age). This dataset does not differentiate between patients who received varying levels of inoculation, and instead reports the data if any PCV was delivered."
  },
  {
    "objectID": "Pages/Materials/Student Codespaces/bdsy-phm-individual/Week2/Time Series Analysis_Hands-On Exercise.html#introduction",
    "href": "Pages/Materials/Student Codespaces/bdsy-phm-individual/Week2/Time Series Analysis_Hands-On Exercise.html#introduction",
    "title": "Time Series Analysis",
    "section": "",
    "text": "This workshop was developed by Dr. Dan Weinberger from the Yale School of Public Health (YSPH) for the 12^{th} International Symposium on Pneumococci and Pneumococcal Diseases (ISPPD-12) hosted in Toronto, Canada in June 2022. Here, we have adapted his workshop session #1, which compares different methods for fitting a time series dataset 1. Students are encouraged to explore the workshop content that Dr. Weinberger put together to learn more about the subject:\n\nGitHub: Find the code, original data, and slides 1.\nWorkshop Webpage: Find a longer description of the workshop in the format of a mini course with links to video presentations of select sections - Workshop on the evaluation of vaccine impact 2.\n\nThe data used here is a subset of the complete set used in Oliveira, L. H. D. et al. 2020 3. In their paper, national-level mortality data spanning from 2000 to 2016 for children under 5 years of age was compiled and standardized by national mortality registers in 10 Latin American and Caribbean countries. The primary cause of death was classified using the International Classification of Diseases, Tenth Revision (ICD-10) codes (Influenza and pneumonia J09-J18 or Oliveira, L. H. D. et al. 2020 Supplementary Table 2) 4,5. The goal of the paper was to evaluate the efficacy of pneumococcal conjugate vaccines (PCVs) in children who received any part of the standard-of-care vaccination series.\nToday, we are going to examine the efficacy of pneumococcal conjugate vaccines (PCVs) in children aged 2 to 59 months in Ecuador. The standard-of-care vaccination series involves a primary series of either two doses (administered at 2 and 4 months of age) or three doses (administered at 2, 4, and 6 months of age), with the option of an additional booster dose (administered at 12-18 months of age). This dataset does not differentiate between patients who received varying levels of inoculation, and instead reports the data if any PCV was delivered."
  },
  {
    "objectID": "Pages/Materials/Student Codespaces/bdsy-phm-individual/Week2/Time Series Analysis_Hands-On Exercise.html#preamble",
    "href": "Pages/Materials/Student Codespaces/bdsy-phm-individual/Week2/Time Series Analysis_Hands-On Exercise.html#preamble",
    "title": "Time Series Analysis",
    "section": "Preamble",
    "text": "Preamble\nThis section is drawn from Dr. Weinberger’s recorded presentation What is ‘vaccine impact’?; and Administrative data: challenges and opportunities for evaluation studies 6.\nStatistical inference can be reduced to quantitatively answering three types of questions:\n\nCausation: Was there a change, and can we identify what caused the change?\nPrediction: What do we expect to see in the future?\nCertainty: How reliable are the answers to the first two questions?\n\nA biotechnologist developing a vaccine is primarily concerned with proving that there was a favorable change, such as mitigating the disease process in an individual. In contrast, a public health analyst aims to determine the overall impact of the vaccine on the population. For example, an effective vaccine not only protects an individual but also attenuates transmission within their immediate social circle.\nThis requires contextualizing the vaccine’s impact as a combination of direct and indirect effects. It is important to identify and control for unexplained linear and non-linear trends unrelated to the introduction of a vaccine to a population. For example, were there changes in overall population health at the same time or changes to diagnostic methods?\nWe can appreciate that this is a Sisyphean task, but one that can be addressed with time series analysis. Keep in mind that the methods discussed here are most applicable to endemic diseases that are consistently present in the population, existing at a relatively stable and predictable level. The core reason is that these methods require us to have an idea of what we would expect to have seen had the vaccine not been introduced, allowing us to posit a realistic counterfactual for model evaluation.\nOur study is framed by PICO. In black text are the general definitions for the acronym and in red text is the application to our example:\n\nPopulation: The target population where the impact of a vaccine is to measured. Children 2-59 months old.\nIntervention: The date and timeframe for a vaccine intervention. Introduction of PCV10 to the Brazilian national immunization program.\nComparator: Diseases or groups of diseases to compare against the intervention, which are not expected to be impacted by the intervention itself. Counterfactual to demonstrate what would have happened if the vaccine had not been introduced, compared to the factual scenario.\nOutcome: The condition representing our expected results from the intervention. Deaths due to pneumonia."
  },
  {
    "objectID": "Pages/Materials/Student Codespaces/bdsy-phm-individual/Week2/Time Series Analysis_Hands-On Exercise.html#set-up-the-environment",
    "href": "Pages/Materials/Student Codespaces/bdsy-phm-individual/Week2/Time Series Analysis_Hands-On Exercise.html#set-up-the-environment",
    "title": "Time Series Analysis",
    "section": "Set Up the Environment",
    "text": "Set Up the Environment\n\nrenv::init()      # Initialize the project     \nrenv::restore()   # Download packages and their version saved in the lockfile\n\n\nsuppressPackageStartupMessages({\n  library(\"readr\")      # For reading in the data\n  library(\"tibble\")     # For handling tidyverse tibble data classes\n  library(\"tidyr\")      # For tidying data \n  library(\"dplyr\")      # For data manipulation \n  library(\"stringr\")    # For string manipulation\n  library(\"MASS\")       # Functions/datasets for statistical analysis\n  library(\"lubridate\")  # For date manipulation\n  library(\"ggplot2\")    # For creating static visualizations\n  library(\"scales\")     # For formatting plots axis\n  library(\"gridExtra\")  # Creates multiple grid-based plots\n})\n\n\n# Function to select \"Not In\"\n'%!in%' &lt;- function(x,y)!('%in%'(x,y))\n\nThe data has been cleaned and standardized for use here, and is imported directly using the GitHub raw URL. You can explore the additional data cleaning steps applied to all of the data in the instructor’s GitHub repository: ysph-dsde/bdsy-phm. The original dataset and prior data cleaning, validation, and standardization can be found in the paper’s GitHub repository and Dr. Weinberger’s workshop GitHub repository 1,3.\n\n# Read in the cleaned data directly from the instructor's GitHub.\ndf &lt;- read_csv(\"https://raw.githubusercontent.com/ysph-dsde/bdsy-phm/refs/heads/main/Data/ec_2to59m.csv\")\n\n# Summarize aspects and dimentions of our dataset.\nglimpse(df)\n\nRows: 144\nColumns: 6\n$ date         &lt;date&gt; 2005-01-01, 2005-02-01, 2005-03-01, 2005-04-01, 2005-05-…\n$ country      &lt;chr&gt; \"Ecuador\", \"Ecuador\", \"Ecuador\", \"Ecuador\", \"Ecuador\", \"E…\n$ age_group    &lt;chr&gt; \"2-59m\", \"2-59m\", \"2-59m\", \"2-59m\", \"2-59m\", \"2-59m\", \"2-…\n$ doses        &lt;chr&gt; \"Any\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\", \"…\n$ J12_J18_prim &lt;dbl&gt; 50, 51, 53, 46, 62, 34, 38, 38, 28, 26, 33, 31, 46, 53, 5…\n$ acm_noj_prim &lt;dbl&gt; 204, 171, 166, 197, 212, 180, 158, 167, 144, 149, 153, 14…\n\n\n\nData Dictionary\nIt is crucial that we understand the meaning of each variable in our dataset. Sometimes, there are surprising aspects embedded within the variables that are not immediately discernible from the table itself. Many sources provide a “Data Dictionary” for this purpose, but at times, you may need to interpret the variable meanings based on context and methods.\nThis paper did not explicitly describe each variable in a “Data Dictionary”; therefore, the following was assembled based on the context and methods provided in the paper and its supplementary materials 3,4.\n\ndate: The month when the events were recorded. This spans from 2005-01-01 to 2016-12-01.\ncountry: Specifies the country where the events were observed. This dataset only represents events recorded in Ecuador.\nage_group: The age of the person who is represented in the counts. This dataset only represents infants aged 2 months to almost 5 years of age (59 months).\ndoses: Specifies the doses of PCV received. As described earlier, all possible combinations of the standard-of-care vaccination series and booster are represented here.\nJ12_J18_prim: Primary cause of death is assigned to the ICD-10 codes J12-J18. We encourage you to read more about what these ICD-10 codes represent 4,5.\nacm_noj_prim: Primary cause of death was assigned any other ICD=10 code, excluding only the J chapter, diseases of the respiratory system."
  },
  {
    "objectID": "Pages/Materials/Student Codespaces/bdsy-phm-individual/Week2/Time Series Analysis_Hands-On Exercise.html#initial-plot-of-the-time-series",
    "href": "Pages/Materials/Student Codespaces/bdsy-phm-individual/Week2/Time Series Analysis_Hands-On Exercise.html#initial-plot-of-the-time-series",
    "title": "Time Series Analysis",
    "section": "Initial Plot of the Time Series",
    "text": "Initial Plot of the Time Series\nWe begin each time series analysis by examining the entire span of data, typically plotted as a line or scatter plot. All methods for modeling the vaccination introduction time series data require the dates when the vaccine was introduced, in this case the PCV10 vaccine.\n\n# Estimation when the vaccine was introduced in YYYY-MM-DD format.\nvax.intro.date &lt;- as.Date(\"2010-08-01\")\n\n# Date when vaccine efficacy evaluations started; at least 12 months\n# following administration.\nvax.eval.date &lt;- as.Date(\"2011-08-01\")\n\nWe are not going to spend time explaining how to plot using the tidyverse package ggplot2() here. Later in the week you will receive a lecture covering this topic. In the meantime, you are welcome to explore the ggplot2 package documentation or the Data Science and Data Equity (DSDE) group’s online Book of Workshops 7,8.\n\np1 &lt;- \n  ggplot(df, aes(x = date, y = J12_J18_prim)) +\n      geom_line() +\n      labs(title = \"Deaths Categorized by\\nICD-10 Codes J12-18\",\n         x = \"Date\", y = \"Counts for &lt;5 yo\") +\n      # Have y-axis for the two plots be the same.\n      ylim(0, 250) +\n      # Reference line.\n      geom_vline(xintercept = vax.intro.date, col = \"red\", lty = 2) +\n      theme_linedraw()\n   \n\np2 &lt;- \n  ggplot(df, aes(x = date, y = acm_noj_prim)) +\n      geom_line() +\n      labs(title = \"Deaths Categorized by\\nNon-J chapter ICD-10 Codes\",\n         x = \"Date\", y = \"Counts for &lt;5 yo\") +\n      # Have y-axis for the two plots be the same.\n      ylim(0, 250) +\n      # Reference line.\n      geom_vline(xintercept = vax.intro.date, col = \"red\", lty = 2) +\n      theme_linedraw()\n\n# Display the plots side-by-side.\ngrid.arrange(p1, p2, nrow = 1)"
  },
  {
    "objectID": "Pages/Materials/Student Codespaces/bdsy-phm-individual/Week2/Time Series Analysis_Hands-On Exercise.html#simple-linear-model",
    "href": "Pages/Materials/Student Codespaces/bdsy-phm-individual/Week2/Time Series Analysis_Hands-On Exercise.html#simple-linear-model",
    "title": "Time Series Analysis",
    "section": "Simple Linear Model",
    "text": "Simple Linear Model\nTo correctly fit a simple linear model to the data, we need to apply a linearization that appropriately reflects its distribution. There are more analytical approaches to achieve this that will not be covered here. We start by visually examining the distribution with a scatter plot.\n\n\n\n\n\n\nNote\n\n\n\nIt is helpful to consider the data generation method, which can provide insights into the likely distribution.\n\n\n\np_base &lt;- \n  ggplot(df, aes(x = date, y = J12_J18_prim)) +\n      geom_point() +\n      labs(title = \"Deaths Scatter Plot\",\n         x = \"Date\", y = \"Counts for &lt;5 yo\") +\n      # Have y-axis for the two plots be the same.\n      ylim(0, NA) +\n      theme_linedraw()\n\np_base\n\n\n\n\n\n\n\n\nVariables that reflect counts (i.e., the number of deaths per month) can be distributed as either Poisson or negative binomial. Note that Poisson regression may artificially narrow confidence intervals when the data is overdispersed. To address this, you can use a negative binomial regression or a quasipoisson model, which accounts for the unexplained variation. In this context, we will apply a negative binomial regression.\n\n# For modeling, we need to use an ordered, discrete variable. Simply, we \n# can use the rownames for this purpose.\ndf &lt;- tibble::rownames_to_column(df, var = \"index\") %&gt;%\n  mutate(index = as.numeric(index))\n\n# Apply the negative binomial regression.\nmod1 &lt;- glm.nb(J12_J18_prim ~ index , data = df)\n\n# Examine the fitting results.\nsummary(mod1)\n\n\nCall:\nglm.nb(formula = J12_J18_prim ~ index, data = df, init.theta = 38.18819754, \n    link = log)\n\nCoefficients:\n              Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  3.8223994  0.0389150   98.22   &lt;2e-16 ***\nindex       -0.0062897  0.0004981  -12.63   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Negative Binomial(38.1882) family taken to be 1)\n\n    Null deviance: 304.02  on 143  degrees of freedom\nResidual deviance: 142.25  on 142  degrees of freedom\nAIC: 976.12\n\nNumber of Fisher Scoring iterations: 1\n\n              Theta:  38.2 \n          Std. Err.:  10.2 \n\n 2 x log-likelihood:  -970.12 \n\n\n\n# Make predictions with confidence intervals.\npred &lt;- predict(mod1, type = \"response\", se.fit = TRUE)\n\n# Add the model predictions and 95% COI to the dataframe.\ndf.pred &lt;- df %&gt;%\n  mutate(se.fit = pred$se.fit, pred = pred$fit) %&gt;%\n  mutate(\n    conf.low = pred - 1.96 * se.fit,\n    conf.high = pred + 1.96 * se.fit\n  )\n\n# Plot the newly created model fitting.\np_sm &lt;- p_base +\n  # Add the fitting line.\n  geom_line(data = df.pred, aes(x = date, y = pred),\n            color = \"#e41a1c\") +\n  # Add the confidence interval.\n  geom_ribbon(data = df.pred, aes(ymin = conf.low, ymax = conf.high), \n              alpha = 0.2, fill = \"blue\") +\n  # Change the title name.\n  labs(title = \"Deaths with a Negative Binomial Fit\")\n\np_sm\n\n\n\n\n\n\n\n\nThis is not a terrible predictor of our trend, but it overlooks many known sources of variance. For instance, we know that infections have seasonal trends, sometimes referred to by its technical term periodicity. Additionally, our baseline population may change from 2005 to 2016, which can consequently shift the overall disease trend in tandem with these baseline changes."
  },
  {
    "objectID": "Pages/Materials/Student Codespaces/bdsy-phm-individual/Week2/Time Series Analysis_Hands-On Exercise.html#building-the-multiple-linear-model",
    "href": "Pages/Materials/Student Codespaces/bdsy-phm-individual/Week2/Time Series Analysis_Hands-On Exercise.html#building-the-multiple-linear-model",
    "title": "Time Series Analysis",
    "section": "Building the Multiple Linear Model",
    "text": "Building the Multiple Linear Model\n\n\n\n\nPrinciples of Seasonality - Generated with Yale’s AI Clarity\n\n\n\n\nAdd Controls for Seasonality\nSeasonality often manifests as periodicity, where recurring patterns repeat at regular, fixed intervals of time. This differs from the related concept of cyclicity, which represents recurring patterns that do not occur at regular intervals or consistently appear in regular seasons 9.\nEach seasonal pattern can be decomposed into three components that can either remain constant (stationary) or change over time (non-stationary): mean, variance, and covariance. The figure on the left illustrates the four possible seasonal trends separately. The top-left panel shows a recurring pattern with time-invariant, stationary parameters, while the other panels demonstrate the effects of non-stationary parameters.\n\n\n\n\n\n\nNote\n\n\n\nMost time series data in the natural sciences exhibit seasonality, periodicity, or cyclicity, though these patterns may be difficult to detect when the data is noisy 10.\n\n\nTo control for temporal variations such as seasonality, we employ a simple form of dynamic linear regression (DLR). Essentially, we add variables to our simple linear regression model that represents different subsections of time as new regressors. Applying a DLR allows for changes in the mean value of the underlying regression relationship 11. For the regression function to recognize each date, we need to factorize the date variable to assign the month the observation occured.\n\ndf$month &lt;- as.factor(month(df$date))\n\n# Inspect the first 36 entries.\ndf$month[1:36]\n\n [1] 1  2  3  4  5  6  7  8  9  10 11 12 1  2  3  4  5  6  7  8  9  10 11 12 1 \n[26] 2  3  4  5  6  7  8  9  10 11 12\nLevels: 1 2 3 4 5 6 7 8 9 10 11 12\n\n# Update the model.\nmod2 &lt;- glm.nb(J12_J18_prim ~ date + month, data = df)\nsummary(mod2)\n\n\nCall:\nglm.nb(formula = J12_J18_prim ~ date + month, data = df, init.theta = 88.28907799, \n    link = log)\n\nCoefficients:\n              Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  6.496e+00  2.156e-01  30.132  &lt; 2e-16 ***\ndate        -1.988e-04  1.426e-05 -13.941  &lt; 2e-16 ***\nmonth2      -2.717e-02  8.166e-02  -0.333 0.739391    \nmonth3       3.446e-02  8.086e-02   0.426 0.669971    \nmonth4      -1.892e-01  8.441e-02  -2.241 0.025011 *  \nmonth5      -4.964e-02  8.228e-02  -0.603 0.546315    \nmonth6      -1.571e-01  8.409e-02  -1.868 0.061714 .  \nmonth7      -2.118e-01  8.513e-02  -2.488 0.012840 *  \nmonth8      -2.076e-01  8.518e-02  -2.437 0.014804 *  \nmonth9      -3.266e-01  8.749e-02  -3.733 0.000189 ***\nmonth10     -2.253e-01  8.574e-02  -2.627 0.008603 ** \nmonth11     -2.467e-01  8.626e-02  -2.859 0.004243 ** \nmonth12     -3.386e-01  8.813e-02  -3.842 0.000122 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Negative Binomial(88.2891) family taken to be 1)\n\n    Null deviance: 404.40  on 143  degrees of freedom\nResidual deviance: 143.84  on 131  degrees of freedom\nAIC: 959.32\n\nNumber of Fisher Scoring iterations: 1\n\n              Theta:  88.3 \n          Std. Err.:  41.2 \n\n 2 x log-likelihood:  -931.319 \n\n\n\n# Make predictions with confidence intervals.\npred2 &lt;- predict(mod2, type = \"response\", se.fit = TRUE)\n\n# Add the model predictions and 95% COI to the dataframe.\ndf.pred2 &lt;- df %&gt;%\n  mutate(se.fit = pred2$se.fit, pred = pred2$fit) %&gt;%\n  mutate(\n    conf.low = pred - 1.96 * se.fit,\n    conf.high = pred + 1.96 * se.fit\n  )\n\n# Plot the newly created model fitting.\np_season &lt;- p_base +\n  # Add the fitting line.\n  geom_line(data = df.pred2, aes(x = date, y = pred),\n            color = \"#e41a1c\") +\n  # Add the confidence interval.\n  geom_ribbon(data = df.pred2, aes(ymin = conf.low, ymax = conf.high), \n              alpha = 0.2, fill = \"blue\") +\n  # Change the title name.\n  labs(title = \"Deaths with a Negative Binomial Fit\\nSeasonality Term Included\")\n\np_season\n\n\n\n\n\n\n\n\n\n\nAdd Controls for Baseline Shifts\n\n\n\n\nFigure by Dr. Dan Weinberger - Generated with Yale’s AI Clarity  6\n\n\n\nWe can expect that the baseline population changes over time, introducing unaccounted-for heterogeneous variance to our model. In epidemiology, it is standard to contextualize raw data by applying a denominator, or offset, that represents the population at risk of the disease.\nThe figure on the right illustrates how the baseline for the number of hospitalizations normalizes when we apply the population offset. Keep in mind that this ratio is often scaled to “per 100,000 persons.” 6.\n\n\\frac{n_{cases}}{n_{population}} \\times 100,000\n\nIn real-world scenarios, the total base population is not always reported, or the coverage might be unreliable for the entire span of the time series. Alternatively, we can apply denominators such as the number of people using the healthcare system, the number of people hospitalized, and similar metrics 6. In this example, we could apply the total population size or all non-respiratory causes of mortality.\n\n# Create the offset using all deaths not coded as J. Transform the values\n# to log before use in the negative binomial fitting.\ndf$log.offset &lt;- log(df$acm_noj_prim)\n\n# Refit the model with an offset.\nmodel3 &lt;- glm.nb(J12_J18_prim ~ index + month + offset(log.offset), data = df)\nsummary(model3)\n\n\nCall:\nglm.nb(formula = J12_J18_prim ~ index + month + offset(log.offset), \n    data = df, init.theta = 59.08482937, link = log)\n\nCoefficients:\n              Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -1.3687586  0.0672076 -20.366  &lt; 2e-16 ***\nindex       -0.0034881  0.0004599  -7.584 3.36e-14 ***\nmonth2       0.1651225  0.0873636   1.890   0.0587 .  \nmonth3       0.1509818  0.0867654   1.740   0.0818 .  \nmonth4      -0.0005318  0.0899497  -0.006   0.9953    \nmonth5       0.0704091  0.0880910   0.799   0.4241    \nmonth6       0.0227796  0.0896446   0.254   0.7994    \nmonth7      -0.0122476  0.0906691  -0.135   0.8925    \nmonth8       0.0077403  0.0907734   0.085   0.9320    \nmonth9      -0.0526956  0.0929670  -0.567   0.5708    \nmonth10     -0.0053806  0.0912550  -0.059   0.9530    \nmonth11      0.0443924  0.0916610   0.484   0.6282    \nmonth12     -0.0283404  0.0935272  -0.303   0.7619    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Negative Binomial(59.0848) family taken to be 1)\n\n    Null deviance: 219.59  on 143  degrees of freedom\nResidual deviance: 145.49  on 131  degrees of freedom\nAIC: 977.59\n\nNumber of Fisher Scoring iterations: 1\n\n              Theta:  59.1 \n          Std. Err.:  21.1 \n\n 2 x log-likelihood:  -949.594 \n\n\n\n# Make predictions with confidence intervals.\npred3 &lt;- predict(model3, type = \"response\", se.fit = TRUE)\n\n# Add the model predictions and 95% COI to the dataframe.\ndf.pred3 &lt;- df %&gt;%\n  mutate(se.fit = pred3$se.fit, pred = pred3$fit) %&gt;%\n  mutate(\n    conf.low = pred - 1.96 * se.fit,\n    conf.high = pred + 1.96 * se.fit\n  )\n\n# Plot the newly created model fitting.\np_offset &lt;- p_base +\n  # Add the fitting line.\n  geom_line(data = df.pred3, aes(x = date, y = pred),\n            color = \"#e41a1c\") +\n  # Add the confidence interval.\n  geom_ribbon(data = df.pred3, aes(ymin = conf.low, ymax = conf.high), \n              alpha = 0.2, fill = \"blue\") +\n  # Change the title name.\n  labs(title = \"Deaths with a Negative Binomial Fit\\nSeasonality Term and Offset\")\n\np_offset\n\n\n\n\n\n\n\n\nNotice that the seasonality trend, which was clear in the previous plot, is disrupted when the population offset is applied. Now that we have our baseline model, we are ready to proceed with examining the effect of the intervention with PCV10 after its introduction on 2010-08-01."
  },
  {
    "objectID": "Pages/Materials/Student Codespaces/bdsy-phm-individual/Week2/Time Series Analysis_Hands-On Exercise.html#does-the-disease-trend-or-level-change",
    "href": "Pages/Materials/Student Codespaces/bdsy-phm-individual/Week2/Time Series Analysis_Hands-On Exercise.html#does-the-disease-trend-or-level-change",
    "title": "Time Series Analysis",
    "section": "Does the Disease Trend or Level Change?",
    "text": "Does the Disease Trend or Level Change?\nIn this section, we will test whether the trend or level of pneumonia deaths due to pneumococcal disease changes after the introduction of the PCV10 vaccine. To accomplish this, we will set up a counterfactual model to represent what our base model predicts the death counts would have been if the vaccine had not been introduced to the population. Similarly, a factual model will be applied to represent what actually happened with the vaccine rollout.\nWe will then compare the factual model against the counterfactual model to determine if a difference is identified in the period following vaccine introduction. This process is generally referred to as detecting vaccine effect or vaccine impact.\n\n\n\n\n\n\nWarning\n\n\n\nYou want to be careful when applying causal inference methods to model building, perturbation, or prediction questions. While our analysis uses a causal framework to detect the possible impact of the vaccine introduction, this does not permit us to make definitive claims of causation.\nAdditionally, it is important to remember that the model we have built may have other flaws influencing its sensitivity to the pre/post vaccination period. It is always best practice to test a model under different conditions and subject it to various tests to robustly justify its accuracy, precision, and generalizability.\n\n\nThe figure below illustrates the three phases of the vaccine rollout: pre-vaccination, post-vaccination, and a latency period between the start of vaccine distribution and efficacy evaluation. Recall that our base negative binomial model accounts for three effector variables: an ordered, discrete variable organizing the outcomes temporally without seasonality, the month to capture seasonal periodicity, and an offset to the baseline accounting for population changes. Our baseline model is expected to, and is therefore assumed to, detect the drop in average cases during the post-vaccination period.\n\n\n\nFigure by Dr. Dan Weinberger - From his recorded lecture Interrupted time series analysis 12.\n\n\nThis model, however, does not explicitly assign outcomes to the three phases represented in the figure. Therefore, we need to set up variables that distinguish outcomes before and after the vaccine introduction, as well as before and after the entry to the vaccine evaluation phase, in order to convert the base model into our factual and counterfactual models.\nWe also need to regenerate the model to produce factually-based and counterfactually-based predictions for comparison. There are three modeling approaches we will evaluate here, each employing a different method to define the pre/post and latency vaccination periods. These differences will impact our inference about vaccine effectiveness.\n\nInterrupted Time Series with Disconnected Segments: This method fits different line segments through the data and tests whether the slope or level of the disease changes. It can sometimes result in abrupt jumps when fitting the model.\nInterrupted Time Series with Connected Segments (Spline Model): This method allows the slope to change in the post-vaccine period but ensures the change is smooth.\nExtrapolation Based on the Pre-Vaccine Period: This method fits the model to data from the pre-vaccine period only and extrapolates the trend to the post-vaccine period.\n\n\nMethod 1. Interrupted Time Series with Disconnected Segments\nWe will add two new binary variables to our dataset: vax_intro and vax_eval. The vax_intro variable will be assigned a value of 0 before 2010-08-01 and 1 afterward; similarly, the vax_eval variable will be assigned a value of 0 before 2011-08-01 and 1 afterward.\n\n# Assign outcomes to the known vaccine phases.\ndf &lt;- df %&gt;%\n  mutate(\n    vax_intro = ifelse(date &gt;= vax.intro.date, 1, 0),\n    vax_eval = ifelse(date &gt;= vax.eval.date, 1, 0)\n  )\n\n# View the changes by randomly selecting dates.\ndf[sort(sample(1:144, 10)), c(\"index\", \"date\", \"vax_intro\", \"vax_eval\")]\n\n# A tibble: 10 × 4\n   index date       vax_intro vax_eval\n   &lt;dbl&gt; &lt;date&gt;         &lt;dbl&gt;    &lt;dbl&gt;\n 1    11 2005-11-01         0        0\n 2    12 2005-12-01         0        0\n 3    25 2007-01-01         0        0\n 4    42 2008-06-01         0        0\n 5    47 2008-11-01         0        0\n 6    53 2009-05-01         0        0\n 7    55 2009-07-01         0        0\n 8    76 2011-04-01         1        0\n 9    80 2011-08-01         1        1\n10   119 2014-11-01         1        1\n\n\nIn linear modeling, it is common to include interaction terms. These terms allow the model to capture interdependencies between variables, showing how the effect of one variable on the outcome changes depending on the level of another variable. Interaction terms also enable the identification of synergistic effects, where the combined effect of two variables is greater (or less) than the sum of their individual effects 13.\nEquation 1 shows a multiple linear regression without an interaction term, and Equation 2 shows the inclusion of the interaction term. Notice that including the interaction term impacts the slope, making it dependent on the value of the other predictor. For example, the impact of X_1, holding X_2 constant, on the slope of Y is a function of (\\beta_1 + \\beta_3)\\ X_2 13.\n\\begin{align}\nY &= \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\epsilon\\\\\nY &= \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 (X_1 \\times X_2) + \\epsilon\n\\end{align}\nWe will not spend more time discussing interaction terms but will briefly examine if they enhance the model’s predictive power. Note that while interaction terms can improve predictive accuracy, they also complicate the model and may result in a trade-off between predictive power and generalizability to other datasets.\nFirst we will generate the two models, then we will evaluate their performance side-by-side.\n\nNo Interaction Terms\nCreate a simple step-change model without an interaction term involving the newly added variables, vax_intro and vax_eval.\n\n# Additional seasonality controls covered in the collapsed box above.\nmod_method1a &lt;- glm.nb(J12_J18_prim ~ index + month + offset(log.offset) +\n                         # Change in disease during administration and \n                         # evaluation period.\n                         vax_intro + vax_eval, data = df)\n\nsummary(mod_method1a)\n\n\nCall:\nglm.nb(formula = J12_J18_prim ~ index + month + offset(log.offset) + \n    vax_intro + vax_eval, data = df, init.theta = 64.37470892, \n    link = log)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -1.382710   0.069290 -19.956   &lt;2e-16 ***\nindex       -0.002510   0.001014  -2.476   0.0133 *  \nmonth2       0.163233   0.086008   1.898   0.0577 .  \nmonth3       0.147780   0.085409   1.730   0.0836 .  \nmonth4      -0.004393   0.088661  -0.050   0.9605    \nmonth5       0.065323   0.086796   0.753   0.4517    \nmonth6       0.017042   0.088446   0.193   0.8472    \nmonth7      -0.018766   0.089510  -0.210   0.8339    \nmonth8       0.006893   0.089471   0.077   0.9386    \nmonth9      -0.054212   0.091693  -0.591   0.5544    \nmonth10     -0.006595   0.089942  -0.073   0.9416    \nmonth11      0.041625   0.090381   0.461   0.6451    \nmonth12     -0.031725   0.092285  -0.344   0.7310    \nvax_intro   -0.167101   0.083391  -2.004   0.0451 *  \nvax_eval     0.076690   0.083253   0.921   0.3570    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Negative Binomial(64.3747) family taken to be 1)\n\n    Null deviance: 225.73  on 143  degrees of freedom\nResidual deviance: 145.44  on 129  degrees of freedom\nAIC: 977.59\n\nNumber of Fisher Scoring iterations: 1\n\n              Theta:  64.4 \n          Std. Err.:  24.3 \n\n 2 x log-likelihood:  -945.595 \n\n\n\n## --------------------\n## Generate the factual model\n\n# Make predictions with confidence intervals.\npred4 &lt;- predict(mod_method1a, type = \"response\", se.fit = TRUE)\n\n\n## --------------------\n## Prepare a plot to visualize the factual model predictions\n\n# Add the model predictions and 95% COI to the dataframe.\ndf.pred.its &lt;- df %&gt;%\n  mutate(se.fit = pred4$se.fit, pred = pred4$fit) %&gt;%\n  mutate(\n    conf.low = pred - 1.96 * se.fit,\n    conf.high = pred + 1.96 * se.fit\n  )\n\n# Plot the newly created model fitting.\np_m1a &lt;- p_base +\n  # Add the fitting line.\n  geom_line(data = df.pred.its, aes(x = date, y = pred),\n            color = \"#377eb8\") +\n  # Add the confidence interval.\n  geom_ribbon(data = df.pred.its, aes(ymin = conf.low, ymax = conf.high), \n              alpha = 0.2, fill = \"blue\") +\n  # Change the title name.\n  labs(title = \"Deaths with a Negative Binomial Fit\\nInterupted Time Series - No Interaction Terms\")\n\n\n## --------------------\n## Generate the counterfactual model\n\n# Initialize the vacccine effect variables by setting them to 0.\ndf.counterfactual.a &lt;- df\ndf.counterfactual.a$vax_intro &lt;- 0\ndf.counterfactual.a$vax_eval &lt;- 0\n\n# Generate the fitted values using the counterfactual setup instead.\ndf.pred.its$pred4.cf.a &lt;- predict(mod_method1a, type = \"response\", \n                                 newdata = df.counterfactual.a)\n\n\n## --------------------\n## Rate ratio to evaluate performance\n\n# Generate the rate ratio between the factual and counterfactual values.\ndf.pred.its$rr.its.a &lt;- df.pred.its$pred/df.pred.its$pred4.cf.a\n\n\n\nWith Interaction Terms\nCreate the more complicated model option by adding an interaction term involving vax_intro and vax_eval, keeping both individually and including with an interaction with index.\n\nmod_method1b &lt;- glm.nb(J12_J18_prim~index + month + offset(log.offset) +\n                         # Add the counterfactuals with an interaction term.\n                         vax_intro + vax_intro*index +\n                         vax_eval + vax_eval*index, data = df)\n\nsummary(mod_method1b)\n\n\nCall:\nglm.nb(formula = J12_J18_prim ~ index + month + offset(log.offset) + \n    vax_intro + vax_intro * index + vax_eval + vax_eval * index, \n    data = df, init.theta = 65.62229438, link = log)\n\nCoefficients:\n                 Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)     -1.410897   0.074005 -19.065   &lt;2e-16 ***\nindex           -0.001660   0.001288  -1.288   0.1976    \nmonth2           0.163960   0.085713   1.913   0.0558 .  \nmonth3           0.148234   0.085153   1.741   0.0817 .  \nmonth4          -0.003138   0.088498  -0.035   0.9717    \nmonth5           0.067516   0.086678   0.779   0.4360    \nmonth6           0.019034   0.088478   0.215   0.8297    \nmonth7          -0.016266   0.089654  -0.181   0.8560    \nmonth8           0.003650   0.089526   0.041   0.9675    \nmonth9          -0.055524   0.091668  -0.606   0.5447    \nmonth10         -0.008492   0.089787  -0.095   0.9247    \nmonth11          0.039981   0.090176   0.443   0.6575    \nmonth12         -0.032091   0.092017  -0.349   0.7273    \nvax_intro        0.267869   1.533398   0.175   0.8613    \nvax_eval        -0.184445   1.541114  -0.120   0.9047    \nindex:vax_intro -0.006384   0.020854  -0.306   0.7595    \nindex:vax_eval   0.004214   0.020870   0.202   0.8400    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Negative Binomial(65.6223) family taken to be 1)\n\n    Null deviance: 227.08  on 143  degrees of freedom\nResidual deviance: 145.13  on 127  degrees of freedom\nAIC: 980.44\n\nNumber of Fisher Scoring iterations: 1\n\n              Theta:  65.6 \n          Std. Err.:  25.0 \n\n 2 x log-likelihood:  -944.439 \n\n\n\n## --------------------\n## Generate the factual model\n\n# Make predictions with confidence intervals.\npred5 &lt;- predict(mod_method1b, type = \"response\", se.fit = TRUE)\n\n\n## --------------------\n## Prepare a plot to visualize the factual model predictions\n\n# Add the model predictions and 95% COI to the dataframe.\ndf.pred.its &lt;- df.pred.its %&gt;%\n  mutate(se.fit = pred5$se.fit, pred = pred4$fit) %&gt;%\n  mutate(\n    conf.low = pred - 1.96 * se.fit,\n    conf.high = pred + 1.96 * se.fit\n  )\n\n# Plot the newly created model fitting.\np_m1b &lt;- p_base +\n  # Add the fitting line.\n  geom_line(data = df.pred.its, aes(x = date, y = pred),\n            color = \"#377eb8\") +\n  # Add the confidence interval.\n  geom_ribbon(data = df.pred.its, aes(ymin = conf.low, ymax = conf.high), \n              alpha = 0.2, fill = \"blue\") +\n  # Change the title name.\n  labs(title = \"Deaths with a Negative Binomial Fit\\nInterupted Time Series - With Interaction Terms\")\n\n\n## --------------------\n## Generate the counterfactual model\n\n# Initialize the vacccine effect variables by setting them to 0.\ndf.counterfactual.b &lt;- df\ndf.counterfactual.b$vax_intro &lt;- 0\ndf.counterfactual.b$vax_eval &lt;- 0\n\n# Generate the fitted values using the counterfactual setup instead.\ndf.pred.its$pred4.cf.b &lt;- predict(mod_method1b, type = \"response\", \n                                  newdata = df.counterfactual.b)\n\n\n## --------------------\n## Rate ratio to evaluate performance\n\n# Generate the rate ratio between the factual and counterfactual values.\ndf.pred.its$rr.its.b &lt;- df.pred.its$pred/df.pred.its$pred4.cf.b\n\n\n\nCompare Model Complexity\nLet’s start by comparing how the fit differs between the models.\n\n# Overlay onto plot.\np_m1a_pred &lt;- p_m1a +\n geom_line(data = df.pred.its, aes(x = date, y = pred4.cf.a),\n           color = '#e41a1c', lty = 2)\n\n# Overlay onto plot.\np_m1b_pred &lt;- p_m1b +\n geom_line(data = df.pred.its, aes(x = date, y = pred4.cf.b),\n           color = '#e41a1c', lty = 2)\n\ngrid.arrange(p_m1a_pred, p_m1b_pred, ncol = 1)\n\n\n\n\n\n\n\n\nWe do see differences with the prediction adherance to the data we trained the model on. By qualitative examination, it appears the model with interaction terms diverge more in the post-vaccination period.\nThe Akaike Information Criterion (AIC) is a one measure used to compare the goodness of fit of different statistical models, while also accounting for model complexity. It helps in model selection by balancing model fit and complexity, otherwise called the bias/variance trade-off.\nThere are different variations of AIC, but the basic definition is\n\n\\text{AIC} = 2k - 2\\ln(\\mathcal{L})\n\nwhere k is the number of parameters in the model and \\mathcal{L} is the maximum likelihood of the model.\n\nAIC(mod_method1a, mod_method1b)\n\n             df      AIC\nmod_method1a 16 977.5948\nmod_method1b 18 980.4387\n\n\nThe results indicate that the AIC score is worse (higher) when interaction terms are included. Since there is no significant gain in performance, we will prefer the simpler model for better generalizability.\nBelow, we also observe that the interaction terms create unexpected trends in the factual/counterfactual ratio. Notably, the ratio is not 1 (indicating that factual and counterfactual predictions are the same) in the pre-vaccination period, which is not ideal.\n\n# Inspect methods sensitivity to vaccine impact.\np_ratio &lt;- \n  ggplot(df.pred.its, aes(x = date, y = rr.its.a)) +\n      geom_line() +\n      labs(title = \"Rate Ratio of the ITS Model\",\n         x = \"Date\", y = \"Rate ratio\") +\n      ylim(0, NA) +\n      geom_vline(xintercept = vax.intro.date, col = \"red\", lty = 2) +\n      annotate(\"text\", x = vax.intro.date + 30, y = 00.1, label = \"Vaccine Introduced\", color = \"red\", hjust = 0) +\n      theme_linedraw()\n\np_ratio +\n  geom_line(data = df.pred.its, aes(x = date, y = rr.its.b),\n            color = \"#4daf4a\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode Like a Pro\n\n\n\nWe won’t try to estimate a confidence interval on the rate ratio here. Students are encouraged to checkout the the InterventionEvaluatR package, which will automatically calculate the confidence intervals for these ratios.\n\n\n\n\n\nMethod 2. Interrupted Time Series with Connected Segments (Spline Model)\nIn this case, we force the changes to be smooth so that we don’t get a drastic jump after vaccination introduction that prematurly shows impact during the expected latency period. Therefore, we will add two new variables to our dataset: spl1 and spl2. The spl1 variable will be assigned a value of 0 before 2010-08-01 and index - intro.index + 1 afterward; similarly, the spl2 variable will be assigned a value of 0 before 2011-08-01 and index - eval.index + 1 afterward.\n\n# Identifies the row index that represents when the vaccine was introduced\n# and when the evaluation period started.\nintro.index &lt;- which(df$date == vax.intro.date)\neval.index  &lt;- which(df$date == vax.eval.date)\n\n# Assign outcomes to the known vaccine phases.\ndf &lt;- df %&gt;%\n  mutate(\n    spl1 = ifelse(index - intro.index + 1 &lt; 0, 0, index - intro.index + 1),\n    spl2 = ifelse(index - eval.index + 1 &lt; 0, 0, index - eval.index + 1)\n  )\n\n# Inspect the changes.\ndf[sort(sample(1:144, 10)), c(\"date\", \"index\", \"spl1\", \"spl2\")]\n\n# A tibble: 10 × 4\n   date       index  spl1  spl2\n   &lt;date&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 2005-05-01     5     0     0\n 2 2006-01-01    13     0     0\n 3 2006-05-01    17     0     0\n 4 2007-04-01    28     0     0\n 5 2010-11-01    71     4     0\n 6 2011-01-01    73     6     0\n 7 2011-12-01    84    17     5\n 8 2013-07-01   103    36    24\n 9 2015-05-01   125    58    46\n10 2016-11-01   143    76    64\n\n\n\nmod_method2 &lt;- glm.nb(J12_J18_prim ~ index + month + offset(log.offset) +\n                        # Post-vaccine changes.\n                        spl1 + spl2, data = df)\n\nsummary(mod_method2)\n\n\nCall:\nglm.nb(formula = J12_J18_prim ~ index + month + offset(log.offset) + \n    spl1 + spl2, data = df, init.theta = 61.04668124, link = log)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -1.397121   0.074586 -18.732   &lt;2e-16 ***\nindex       -0.002349   0.001237  -1.899   0.0576 .  \nmonth2       0.164546   0.086841   1.895   0.0581 .  \nmonth3       0.151023   0.086225   1.752   0.0799 .  \nmonth4      -0.001397   0.089442  -0.016   0.9875    \nmonth5       0.069656   0.087565   0.795   0.4263    \nmonth6       0.020998   0.089150   0.236   0.8138    \nmonth7      -0.013534   0.090170  -0.150   0.8807    \nmonth8       0.006429   0.090270   0.071   0.9432    \nmonth9      -0.055311   0.092521  -0.598   0.5500    \nmonth10     -0.008406   0.090793  -0.093   0.9262    \nmonth11      0.040934   0.091221   0.449   0.6536    \nmonth12     -0.032326   0.093106  -0.347   0.7284    \nspl1        -0.007961   0.007038  -1.131   0.2580    \nspl2         0.007591   0.007271   1.044   0.2965    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Negative Binomial(61.0467) family taken to be 1)\n\n    Null deviance: 221.95  on 143  degrees of freedom\nResidual deviance: 145.72  on 129  degrees of freedom\nAIC: 980.3\n\nNumber of Fisher Scoring iterations: 1\n\n              Theta:  61.0 \n          Std. Err.:  22.3 \n\n 2 x log-likelihood:  -948.296 \n\n\n\n## --------------------\n## Generate the factual model\n\n# Make predictions with confidence intervals.\ndf.pred.spl &lt;- df %&gt;%\n  mutate(pred.spl = predict(mod_method2, type = \"response\"))\n\n\n## --------------------\n## Generate the counterfactual model\n\n# Initialize the vacccine effect variables by setting them to 0.\ndf.counterfactual.spl &lt;- df\ndf.counterfactual.spl$spl1 &lt;- 0\ndf.counterfactual.spl$spl2 &lt;- 0\n\n# Generate the fitted values.\ndf.pred.spl$pred.spl.cf &lt;- predict(mod_method2, type = \"response\", \n                                   newdata = df.counterfactual.spl)\n\n\n## --------------------\n## Rate ratio to evaluate performance\n\n# Generate the rate ratio between the fitted and counterfactual values.\ndf.pred.spl$rr.spline &lt;- df.pred.spl$pred.spl/df.pred.spl$pred.spl.cf\n\n\n## --------------------\n## Prepare a plot to visualize the factual model predictions\n\n# Make predictions with confidence intervals.\npred6 &lt;- predict(mod_method2, type = \"response\", se.fit = TRUE)\n\n# Add the model predictions and 95% COI to the dataframe.\ndf.pred.spl &lt;- df.pred.spl %&gt;%\n  mutate(se.fit = pred6$se.fit, pred = pred6$fit) %&gt;%\n  mutate(\n    conf.low = pred - 1.96 * se.fit,\n    conf.high = pred + 1.96 * se.fit\n  )\n\n\np_m2 &lt;- \n  ggplot(df.pred.spl, aes(x = date, y = J12_J18_prim)) +\n      geom_point() +\n      # Add the fitted line.\n      geom_line(data = df.pred.spl, aes(x = date, y = pred.spl),\n                color = \"#377eb8\") +\n      # Add the confidence interval.\n      geom_ribbon(data = df.pred.spl, aes(ymin = conf.low, ymax = conf.high),\n                  alpha = 0.2, fill = \"blue\") +\n      # Add the counterfactual line.\n      geom_line(data = df.pred.spl, aes(x = date, y = pred.spl.cf),\n                color = \"#e41a1c\", lty = 2) +\n      labs(title = \"Deaths with a Negative Binomial Fit\\nITS with Spline Smoothing\",\n         x = \"Date\", y = \"Counts for &lt;5 yo\") +\n      theme_linedraw()\n\np_m2\n\n\n\n\n\n\n\n\nWe can see here that the decline follows a smoother trajectory, by design. We can check the counterfactual performace by examining the ratio again.\n\n# Inspect methods sensitivity to vaccine impact.\np_ratio &lt;- p_ratio +\n     geom_line(data = df.pred.spl, aes(x = date, y = rr.spline),\n               color = \"#4daf4a\") +\n      # Update the title.\n      labs(title = \"Rate Ratio with the Spline Model\")\n\np_ratio\n\n\n\n\n\n\n\n\n\n\nMethod #3: Extrapolation Based on the Pre-Vaccine Period\nNow we are going to model only the pre-vaccination period and forecast outcomes based on the restricted fitting into the post-vaccination period. We will create a new variable that excludes any records following 2010-08-01, J12_J18_prim.\n\n# Create a new variable where it is J12_J18_prim until the vaccine is introduced,\n# then it is NA afterwards.\ndf$J12_J18_prim_pre &lt;- df$J12_J18_prim\ndf$J12_J18_prim_pre[which(df$date &gt;= vax.intro.date)] &lt;- NA\n\n# Inspect the changes.\ndf[sort(sample(1:144, 10)), c(\"date\", \"J12_J18_prim_pre\")]\n\n# A tibble: 10 × 2\n   date       J12_J18_prim_pre\n   &lt;date&gt;                &lt;dbl&gt;\n 1 2005-05-01               62\n 2 2006-11-01               26\n 3 2007-08-01               28\n 4 2007-11-01               28\n 5 2008-06-01               45\n 6 2008-07-01               29\n 7 2009-07-01               20\n 8 2010-06-01               26\n 9 2013-02-01               NA\n10 2014-10-01               NA\n\n\n\nmod_method3 &lt;- glm.nb(J12_J18_prim_pre ~ index + month + offset(log.offset), data = df)\n\n# Add the prediction using the smoothed model.\ndf.pred.pre &lt;- df %&gt;%\n  mutate(pred.pre = predict(mod_method3, type = \"response\", newdata = df))\n\nUnlike before, the original observed variable J12_J18_prim now represents the factual model, as we regressed on a subset of data assuming nothing was known following the vaccine introduction. The prediction results using the restricted regressand represents the counterfactual model, since it extrapolates data points missing from the model fitting.\n\n## --------------------\n## Rate ratio to evaluate performance\n\n# Generate the rate ratio between the factual and counterfactual values.\ndf.pred.pre$rr.trend &lt;- df$J12_J18_prim/df.pred.pre$pred.pre\n\n\n## --------------------\n## Prepare a plot to visualize the factual model predictions\n\n# Make predictions with confidence intervals.\npred7 &lt;- predict(mod_method3, type = \"response\", se.fit = TRUE, newdata = df)\n\n# Add the model predictions and 95% COI to the dataframe.\ndf.pred.pre &lt;- df.pred.pre %&gt;%\n  mutate(se.fit = pred7$se.fit, pred = pred7$fit) %&gt;%\n  mutate(\n    conf.low = pred - 1.96 * se.fit,\n    conf.high = pred + 1.96 * se.fit\n  )\n\n\np_m3 &lt;- \n  ggplot(df.pred.pre, aes(x = date, y = J12_J18_prim)) +\n      geom_point() +\n      # Add the fitted line.\n      geom_line(data = df.pred.pre, aes(x = date, y = pred.pre),\n                color = \"#377eb8\") +\n      # Add the confidence interval.\n      geom_ribbon(data = df.pred.pre, aes(ymin = conf.low, ymax = conf.high),\n                  alpha = 0.2, fill = \"blue\") +\n      labs(title = \"Deaths with a Negative Binomial Fit\\nExtrapolating Based on the Pre-Vaccine Period\",\n         x = \"Date\", y = \"Counts for &lt;5 yo\") +\n      theme_linedraw()\n\np_m3\n\n\n\n\n\n\n\n\n\n# Inspect methods sensitivity to vaccine impact.\np_ratio &lt;- p_ratio +\n  geom_line(data = df.pred.pre, aes(x = date, y = rr.trend),\n            color = '#377eb8') +\n  # Update the title.\n  labs(title = \"Rate Ratio with the Extrapolation Model\")\n\np_ratio\n\n\n\n\n\n\n\n\n\n\nCompare the Methods\nLet’s compare the rate ratio estimates from the three models. As a rough estimate, we will average the point-by-point estimates of the rate ratio during the evaluation period.\n\n# Create a Boolean variable that defines the vaccine evaluation period.\neval.period &lt;- df$date &gt; vax.eval.date\n\n# Calculate the average outcome during the vaccine evaluation period.\nrr.its.eval    &lt;- mean(df.pred.its$rr.its.a[eval.period])\nrr.spline.eval &lt;- mean(df.pred.spl$rr.spline[eval.period])\nrr.trend.eval  &lt;- sum(df.pred.pre$J12_J18_prim[eval.period])/sum(df.pred.pre$pred.pre[eval.period])\n\n# Average decline detected from 1.\nround(c(\"ITS - Disconnected\" = rr.its.eval, \"ITS - Spline\" = rr.spline.eval,\n        \"Extrapolation\" = rr.trend.eval), digits = 2)\n\nITS - Disconnected       ITS - Spline      Extrapolation \n              0.91               0.90               0.86 \n\n# Percent decline detected.\n100*(1 - round(c(\"ITS - Disconnected\" = rr.its.eval, \"ITS - Spline\" = rr.spline.eval, \n                 \"Extrapolation\" = rr.trend.eval), digits = 2))\n\nITS - Disconnected       ITS - Spline      Extrapolation \n                 9                 10                 14 \n\n\nTwo of the modeling methods identified a decrease of approximately 10% in pneumonia deaths due to pneumococcal disease after the release of the PCV10 vaccine. The extrapolation method indicates a stronger vaccine effect; however, we expect some of this result to be spurious due to the level of random noise introduced by applying the observed values in the counterfactual ratio calculation.\n\n\n\n\n\n\nCaution\n\n\n\nDepending on the dataset, these three methods may or may not agree to a greater extent."
  },
  {
    "objectID": "Pages/Materials/Student Codespaces/bdsy-phm-individual/Week2/Time Series Analysis_Hands-On Exercise.html#references",
    "href": "Pages/Materials/Student Codespaces/bdsy-phm-individual/Week2/Time Series Analysis_Hands-On Exercise.html#references",
    "title": "Time Series Analysis",
    "section": "References",
    "text": "References\n\n\n1. Weinberger, D. M. (2022).\n\n\n2. Weinberger, D. M. et al. Data analysis workshop at ISPPD | workshop on the evaluation of vaccine impact webpage. (2022).\n\n\n3. Oliveira, L. H. D. et al. Declines in pneumonia mortality following the introduction of pneumococcal conjugate vaccines in latin american and caribbean countries. Clinical Infectious Diseases 73, 306–313 (2021).\n\n\n4. Oliveira, L. H. D. et al. Declines in pneumonia mortality following the introduction of pneumococcal conjugate vaccines in latin american and caribbean countries - supplementary table 2. Clinical Infectious Diseases 73, 306–313 (2021).\n\n\n5. icd10data.com. 2025 ICD-10-CM codes J09-J18: Influenza and pneumonia.\n\n\n6. Weinberger, D. M. et al. 1. Vaccine impact; administrative data sources and their pitfalls - YouTube. YouTube (2021).\n\n\n7. Wickham, H. et al. Create Elegant Data Visualisations Using the Grammar of Graphics • Ggplot2. (Springer-Verlag, 2016).\n\n\n8. Golden, S. Data Visualization with Ggplot2 – Book of Workshops. (2025).\n\n\n9. Ninja, N. N. Periodicity: Detecting rhythms in data - let’s data science. (2023).\n\n\n10. Ramanathan, K. et al. Assessing seasonality variation with harmonic regression: Accommodations for sharp peaks. International Journal of Environmental Research and Public Health 17, (2020).\n\n\n11. Young, P. C. Recursive Estimation and Time-Series Analysis. Recursive Estimation and Time-Series Analysis (Springer Berlin Heidelberg, 2011). doi:10.1007/978-3-642-21981-8.\n\n\n12. Weinberger, D. M. et al. 2. Interrupted time series analysis. YouTube (2021).\n\n\n13. C, D. M., A, E. P. & Vining, G. G. Introducing to Linear Regression Analysis (6th Ed.). John Wiley and Sons 642 (Wiley Series, 2012)."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome!",
    "section": "",
    "text": "Welcome to our project on modeling pneumococcal disease, a leading cause of death in children under 5 worldwide. Public health is an interdisciplinary field that measures and tracks disease trends and promotes health through policy and treatment interventions. We will investigate the impact of pneumococcal vaccines and the phenomenon of serotype replacement, where uncommon serotypes variations not targeted by the therapy flourish and take over as the predominant causes of disease after vaccination. Using data from infectious disease surveillance systems and statistical methods such as time series analysis, hierarchical modeling, and serotype clustering and association, we aim to understand serotype replacement and its implications. Our analyses will be conducted in R.\nOur objectives for this course will be to:\nWe will accomplish these objectives using data from national and multinational infectious disease surveillance systems, and the following statistical methods:\nFull project statement"
  },
  {
    "objectID": "index.html#purpose-of-this-site",
    "href": "index.html#purpose-of-this-site",
    "title": "Welcome!",
    "section": "Purpose of This Site",
    "text": "Purpose of This Site\nSharing code-based materials poses a challenging technical problem that Canvas is not well suited to handle. Therefore, we have created an instructor GitHub webpage where we can effectively create, communicate, and distribute course materials. This platform provides students with the most current information in an easily accessible format and allows us, the instructors, to distribute materials more efficiently.\nOn this site, you will find:\n\nA schedule of important project goals and assignment dates.\nWebpages with full weekly hands-on demonstrations of concepts.\nWeekly codebases for students to apply in their own projects, reflecting that week’s concepts.\nLinks to the subgroup dataset zip files, the associated paper, and brief descriptions of them.\nLinks to other course materials, including blank slide decks, poster layouts, slides, and more.\n\n\n\n    About the Instructors\n\n\n\n\n\n  \n  People Tiles\n  \n  \n\n\n  \n    \n      \n        \n          \n            \n          \n          \n            Stephanie Perniciaro, PhD, MPH\n            Associate Research Scientist in Epidemiology (Microbial Diseases)\n\n            \n              \n                \n                  \n                \n                \n                \n                  \n                \n                \n                  \n                \n                \n                  \n                \n              \n            \n            The surveillance and epidemiology of vaccine-preventable respiratory diseases, especially pneumococcal disease, is my passion. I want to determine how to efficiently and effectively protect populations from pneumococcal disease, both invasive (bacteremia, meningitis) and non-invasive (otitis media, non-bacteremic pneumonia). I'm interested in respiratory disease surveillance, serotype replacement, antibiotic resistance, vulnerable populations, vaccine schedules, vaccine advocacy, and overcoming vaccine hesitancy.\n\n            \n              \n                Email: stephanie.perniciaro@yale.edu\n              \n              \n            \n            \n          \n        \n      \n        \n          \n            \n          \n          \n            Shelby Golden, MS\n            Data Scientist I\n\n            \n              \n                \n                  \n                \n                \n                  \n                \n                \n                  \n                \n                \n                  \n                \n                \n                  \n                \n              \n            \n            With my work I aim to contribute to impactful projects that advance scientific understanding by utilizing strong analytical skills and a deep commitment to reliable data-driven decision-making. \n\n\nMy background is in computational mathematics, molecular biology, and biochemistry. I hold a Master of Science in Applied Computational Mathematics from Johns Hopkins University and dual Bachelor of Science degrees in Molecular, Cellular, Developmental Biology and Biochemistry, alongside a minor in Engineering in Applied Mathematics from the University of Colorado at Boulder.\n\n            \n              \n                Email: shelby.golden@yale.edu\n              \n              \n            \n            \n          \n        \n      \n    \n  \n\n\n\nNo matching items"
  },
  {
    "objectID": "Pages/Materials/Materials.html",
    "href": "Pages/Materials/Materials.html",
    "title": "Public Health Modeling Group",
    "section": "",
    "text": "Please note that this page contains numerous links and download buttons, which may take a few minutes to load completely."
  },
  {
    "objectID": "Pages/Materials/Materials.html#course-papers",
    "href": "Pages/Materials/Materials.html#course-papers",
    "title": "Public Health Modeling Group",
    "section": "Course Papers",
    "text": "Course Papers\n\nHierarchical Models Weinberger and Warren et al. 2020 (view paper)\nSerotype Replacement Weinberger and Warren et al. 2018 (view paper)\nCorrelates of Nonrandom Patterns of Serotype Switching in Pneumococcus Joshi and Al-Mamun et al. 2020 (view paper)\n2022 Annual Epidemiological Report for Invasive Pneumococcal Disease (IPD) (view paper)"
  },
  {
    "objectID": "Pages/Materials/Materials.html#lecture-slides",
    "href": "Pages/Materials/Materials.html#lecture-slides",
    "title": "Public Health Modeling Group",
    "section": "Lecture Slides",
    "text": "Lecture Slides\n Day 4 Slide Deck   Day 5 Slide Deck"
  },
  {
    "objectID": "Pages/Materials/Materials.html#codespaces",
    "href": "Pages/Materials/Materials.html#codespaces",
    "title": "Public Health Modeling Group",
    "section": "Codespaces",
    "text": "Codespaces\n\nWeekly Modules\nPlease download the codespaces provided for each weekly module release. Note that you may need to adjust the filepath for some references inside the code. It is recommended that you use the starter codespace package, which includes the following:\n\nAn Rproj file to set your root directory.\nAn renv lockfile.\nA suggested .gitignore.\nA README.md file with directions for using the codespace and guidelines for adding the code to your own GitHub remote repository.\n\nSubsequent weekly releases should be placed inside your root project directory. For instance, if you are using the directory “~/bdsy-phm-individual”, place the weekly releases within that directory.\n Starter for individual project with Week 2 code \n Week 3 codespace \n\n\nProject Datasets and Materials\n\n\nComplete Global Pneumococcal Sequencing Project (GPS) Data Viewer and Download (link to page)\n\n\n GPS Database Dictionary \n Malawi: raw incidents data, GPS data, and literature papers \n Nepal: raw incidents data, GPS data, and literature papers \n Peru: raw incidents data, GPS data, and data dictionary \n\n\nPeru Epidemiological surveillance of Acute Respiratory Infections (ARI) Data Page (link to page)"
  },
  {
    "objectID": "Pages/Materials/Materials.html#templates",
    "href": "Pages/Materials/Materials.html#templates",
    "title": "Public Health Modeling Group",
    "section": "Templates",
    "text": "Templates\n BDSY Slides Template 2025"
  },
  {
    "objectID": "Pages/Materials/Materials.html#tips-and-advice",
    "href": "Pages/Materials/Materials.html#tips-and-advice",
    "title": "Public Health Modeling Group",
    "section": "Tips and Advice",
    "text": "Tips and Advice\n\n\nTips for Effective Presentations (view slides)\n\n\n\n\n\n\n\n\nResearch Like a Pro\n\n\n\nIt is vital that you keep track of and disclose all the references you use to support your research. There are many tools available to help you with this, so discuss with your team and decide on a preferred method for reference tracking.\nIn my research, I use Mendeley, which comes with a Web Importer plug-in that makes it easy to compile different types of references. It conveniently formats these entries to various styles, including LaTeX and BibTeX."
  },
  {
    "objectID": "Pages/Week3/Hierarchical Modeling.html",
    "href": "Pages/Week3/Hierarchical Modeling.html",
    "title": "Hierarchical Modeling",
    "section": "",
    "text": "Page still in progress!!"
  },
  {
    "objectID": "Pages/Week3/Hierarchical Modeling.html#introduction",
    "href": "Pages/Week3/Hierarchical Modeling.html#introduction",
    "title": "Hierarchical Modeling",
    "section": "Introduction",
    "text": "Introduction\nReal-world analytical problems are often complex, making it difficult to define or derive deterministic solutions. Two common approaches to support complex analytical projects are:\n\nApproximating with a known solvable function.\nUsing stochastic search and optimization, often referred to as Monte Carlo methods.\n\nWhile the first approach has many advantages, there are circumstances where approximating a problem with a known distribution is computationally prohibitive, overly reductive, or infeasible. In such cases, we leverage abstract probabilistic and statistical concepts, using stochastic search and optimization to transform randomness into inferential statistics. When applied correctly, these concepts can surprisingly guarantee robust solutions to otherwise unsolvable problems.\nOne of these powerful techniques is Markov chain Monte Carlo (MCMC). Random samples generated with MCMC can be used for various purposes, including computing statistical estimates, numerical integrals, and estimating the marginal or joint probabilities of multivariate distributions and densities. Notably, MCMC provides a means to generate samples from joint distributions by utilizing conditional, factorized distributions.\nTwo common MCMC methods are:\n\nMetropolis-Hastings (M-H)\nGibbs sampling\n\nStudents are encouraged to explore M-H further on their own by reading the relevant sections of the supporting textbooks listed below. Today, we will focus on Gibbs sampling, as this technique is particularly well-suited for reducing high-dimensional problems into several one-dimensional ones.\n\n\n\n\n\n\nSolid theory supports effective and accurate application!\n\n\n\nThe concepts discussed here have been compiled and summarized from various resources listed in the bibliography at the end of this page. We would like to specifically highlight the following textbooks, with suggested readings, to help students deepen their theoretical understanding of this week’s material. This foundation will aid in generating better analysis in practice."
  },
  {
    "objectID": "Pages/Week3/Hierarchical Modeling.html#distribution-estimation-by-particle-sampling",
    "href": "Pages/Week3/Hierarchical Modeling.html#distribution-estimation-by-particle-sampling",
    "title": "Hierarchical Modeling",
    "section": "Distribution Estimation by Particle Sampling",
    "text": "Distribution Estimation by Particle Sampling\n\nRecall Markov Chains - A Stochastic Process\nThis section is drawn from Wasserman, specifically Chapter 23 and 24.\nA stochastic process \\{X_t : t \\in T\\} is a collection of random variables taken from a state space, \\mathcal{X}, and indexed by an ordered set T, which can be interpreted as time. The index set T can be discrete, such as T = \\{0, 1, 2, \\dots\\}, or continuous, such as T = [0, \\infty). To denote the stochastic nature of X, it can be written as X_t or X(t).\n\n\n\n\nIllustration of state transition diagram for the Markov chain.\n\n\n\nA Markov chain is a type of stochastic process where the distribution of future states depends only on the current state and not on the sequence of events that preceded it. For example, the outcome for X_t depends only on what the outcome was for X_{t-1}. Notice that sometimes, authors with specify that X is a Markov chain stochastic process by writing X_n instead of X_t.\nThe figure on the left shows a state transition diagram for a discrete-time Markov chain with transition probabilities p_{ij} = \\mathbb{P}(X_{n+1}= j\\ \\lvert\\ X_n = i). Below the diagram is the matrix representation of these interactions, known as the transition matrix, \\mathbf{P}, where each row is a probability mass function. A generalized definition for a Markov chain is given by\n\\begin{align}\n\\mathbb{P}(X_n = x\\ \\lvert\\ X_0, \\dots, X_{n-1}) = \\mathbb{P}(X_n = x\\ \\lvert\\ X_{n-1})\n\\end{align}\nfor all n and for all x \\in \\mathcal{X}. Markov chains have many aspects, but for this application, it is essential to consider the properties of a specific transitional stage denoted by, \\pi = (\\pi_i : i \\in \\mathcal{X}). The vector \\pi consists of non-negative numbers that sum to one and is stationary (or invariant) distribution if \\pi = \\pi\\mathbf{P}.\nBy applying the Chapman-Kolmogorov equations for n-step probabilities (p_{ij}(m+n) = \\sum_k p_{ik}(m) p_{kj}(n)), we derive that a key characteristic of a stationary distribution \\pi for a Markov chain is that it is limiting. This means that, after any number of steps n, the distribution remains \\pi:\n\nDraw X_0 from the distribution \\pi gives \\mu_0 = \\pi.\nDraw X_1 from the distribution \\pi gives \\mu_1 = \\mu_0\\mathbf{P} = \\pi\\mathbf{P} = \\pi.\nDraw X_2 from the distribution \\pi gives \\pi\\mathbf{P}^2 = (\\pi\\mathbf{P})\\mathbf{P} = \\pi\\mathbf{P} = \\pi.\n\\dots etc.\n\nHere, \\mu_n(i) = \\mathbb{P}(X_n = i) denotes the marginal probability that the chain is in state i at time n, while \\mu_0 represents the initial distribution. Once the chain limits to the distribution \\pi, it will remain in this distribution indefinitely. However, ensuring the process is stationary is not sufficient; it is also important to guarantee that \\pi is unique. This uniqueness is guaranteed if the Markov chain is ergodic, which must satisfy the following two characteristics:\n\nAperiodicity: The states do not get trapped in cycles of fixed length. This means revisiting any state does not happen at regular intervals, ensuring more general behavior. For d = \\text{gcd}\\{n\\ :\\ p_{ii}(n) &gt; 0\\} then d(i) = 1, where gcd means “greatest common divisor”.\nPositive Recurrence: The expected number of steps to return to that state is finite. A Markov chain is positive recurrent if every state is positive recurrent.\n\nIf the Markov chain is also irreducible, then all states communicate, meaning it is possible to get from any state to any other state, denoted by i \\leftrightarrow j. Having an irreducible, ergodic Markov chain guarantees that it has a unique stationary distribution, \\pi.\n\n\n\n\n\n\nWarning\n\n\n\nA Markov chain with a stationary distrubtion does not mean that it converges.\n\n\nThere is much more to learn about Markov chains beyond what was discussed here, but these fundamental concepts provide a foundation for understanding their use in Monte Carlo methods.\n\n\nGibbs Sampling\n\\pi satisfies detailed balance if \\pi_i p_{ij} = \\pi_j p_{ji}, guaranteeing that \\pi is a stationary distribution.\n\n\nTranslating Equations to Graphical Models"
  },
  {
    "objectID": "Pages/Week3/Hierarchical Modeling.html#higherarchical-modeling",
    "href": "Pages/Week3/Hierarchical Modeling.html#higherarchical-modeling",
    "title": "Hierarchical Modeling",
    "section": "Higherarchical Modeling",
    "text": "Higherarchical Modeling"
  },
  {
    "objectID": "Pages/Week3/Hierarchical Modeling.html#set-up-the-environment",
    "href": "Pages/Week3/Hierarchical Modeling.html#set-up-the-environment",
    "title": "Hierarchical Modeling",
    "section": "Set Up the Environment",
    "text": "Set Up the Environment\n\nInstalling JAGS\nThe rjags package requires JAGS to be installed separately. rjags will reference the JAGS libjags.4.dylib library file and the modules-4 directory, which contains seven *.so files, such as bugs.so. The next steps will walk you through installation, and an optional troubleshooting guide if R cannot find the JAGS file path.\n\nMacPC\n\n\n\nFrom JAGS, open the download link that will take you to a SourceForge page.\nDownload the latest version. At the time of this writing, the current version is JAGS-4.3.2.\nOpen and run the downloaded installer.\nConfirm that the installation is located where we need it to be: /usr/local/lib/.\n\n\nCommand-Line Application\n\n# Running both should NOT give you \"No such file or directory\"\nls -l /usr/local/lib/libjags.4.dylib\nls -l /usr/local/lib/JAGS/modules-4/\n\n\n\n\n\n\n\n\nIf the namespace load failed\n\n\n\n\n\nSometimes, the installer might place the JAGS program in a different location. For example, if you installed JAGS using Homebrew, the program might have been placed in the Homebrew library. If the file path differs, you will need to adjust the following code accordingly before reinstalling it in R.\n\n\nCommand-Line Application\n\n# This example assumes that Homebrew was used\nbrew install jags\n\n\nOpen /usr/local/lib by searching for it with “Go to Folder” in an open Finder Window. If it does not already exist, create the file path using the following code.\n\n\nCommand-Line Application\n\n# ONLY IF there is no existing directory\nsudo mkdir -p /usr/local/lib\n\nCreate a symbolic link from the Homebrew-installed libjags.4.dylib JAGS library to the expected location:\n\n\nCommand-Line Application\n\nsudo ln -s /opt/homebrew/lib/libjags.4.dylib /usr/local/lib/libjags.4.dylib\n\nCreate another symbolic link for the Homebrew-installed JAGS modules directory to the expected location:\n\n\nCommand-Line Application\n\nsudo mkdir -p /usr/local/lib/JAGS/modules-4\nsudo ln -s /opt/homebrew/lib/JAGS/modules-4/* /usr/local/lib/JAGS/modules-4/\n\nOPTIONAL: You can verify the linking worked if you see the intended directory listed for both of the following lines of code.\n\n\nCommand-Line Application\n\n# Running both should NOT give you \"No such file or directory\"\nls -l /usr/local/lib/libjags.4.dylib\nls -l /usr/local/lib/JAGS/modules-4/\n\nReinstalling rjags with the connection in place.\n\n\nRStudio\n\n# Remove existing installation\nremove.packages(\"rjags\")\n\n# Reinstall rjags\ninstall.packages(\"rjags\")\n\n# Load the package\nlibrary(rjags)\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThis page was developed on a Mac, and so the directions for a PC were not able to be tested.\n\n\n\nFrom JAGS, open the download link that will take you to a SourceForge page.\nDownload the latest version. At the time of this writing, the current version is JAGS-4.3.2.\nOpen and run the downloaded installer.\nYou will need to also download the latest version of RTools, which at the time of this writing is RTools 4.5. Add Rtools to your PATH if it is not done automatically.\n\n\nCommand-Line Application\n\nwhere make   # Should give the result \"C:\\Rtools\\bin\\make.exe\"\necho %PATH%  # Should give the result \"C:\\Rtools\\bin;C:\\Rtools\\mingw_64\\bin\"\n\nIf this does not give the expected results, you can set the path:\n\n\nCommand-Line Application\n\n# ONLY IF the PATH for Rtools is incorrect.\nsetx PATH \"%PATH%;C:\\Rtools\\bin;C:\\Rtools\\mingw_64\\bin\"\n\nConfirm that the installation is located where we need it to be: C:\\Program Files\\JAGS\\JAGS-4.x\\bin.\n\n\nCommand-Line Application\n\necho %JAGS_HOME%  # Should say \"C:\\Program Files\\JAGS\\JAGS-4.x\"\necho %PATH%       # Should say \"...;C:\\Program Files\\JAGS\\JAGS-4.x\\bin;...\"\n\n\n\n\n\n\n\n\nIf the namespace load failed\n\n\n\n\n\nSometimes the installer might place the JAGS program in a different location. If this happens, you can set the file path in the command line application as follows and retry installing the package through R.\n\nModify the environment variables as needed.\n\n\nCommand-Line Application\n\nsetx JAGS_HOME \"C:\\Program Files\\JAGS\\JAGS-4.x\"\nsetx PATH \"%PATH%;C:\\Program Files\\JAGS\\JAGS-4.x\\bin\"\n\nOPTIONAL: You can verify the setting worked.\n\n\nCommand-Line Application\n\necho %JAGS_HOME%  # Should say \"C:\\Program Files\\JAGS\\JAGS-4.x\"\necho %PATH%       # Should say \"...;C:\\Program Files\\JAGS\\JAGS-4.x\\bin;...\"\n\nReinstalling rjags with the connection in place.\n\n\nRStudio\n\n# Remove existing installation\nremove.packages(\"rjags\")\n\n# Reinstall rjags\ninstall.packages(\"rjags\")\n\n# Load the package\nlibrary(rjags)\n\n\n\n\n\n\n\n\nWith JAGS installed, we can now proceed with the standard process of installing the packages.\n\nrenv::init()      # Initialize the project     \nrenv::restore()   # Download packages and their version saved in the lockfile\n\n\nsuppressPackageStartupMessages({\n  library(\"readr\")      # For reading in the data\n  library(\"tibble\")     # For handling tidyverse tibble data classes\n  library(\"tidyr\")      # For tidying data \n  library(\"dplyr\")      # For data manipulation \n  library(\"stringr\")    # For string manipulation\n  library(\"MASS\")       # Functions/datasets for statistical analysis\n  library(\"lubridate\")  # For date manipulation\n  library(\"ggplot2\")    # For creating static visualizations\n  library(\"viridis\")    # For color scales in ggplot2\n  library(\"scales\")     # For formatting plots axis\n  library(\"gridExtra\")  # Creates multiple grid-based plots\n  library(\"rjags\")      # For running JAGS (Just Another Gibbs Sampler) models\n  library(\"coda\")       # For analysis of MCMC output from Bayesian models\n})\n\n\n# Function to select \"Not In\"\n'%!in%' &lt;- function(x,y)!('%in%'(x,y))\n\n\n# Read in the cleaned data directly from the instructor's GitHub.\ndf &lt;- read_csv(\"https://raw.githubusercontent.com/ysph-dsde/bdsy-phm/refs/heads/main/Data/testDE2.csv\")\n\n# Summarize aspects and dimentions of our dataset.\nglimpse(df)\n\nRows: 17,653\nColumns: 47\n$ ...1                             &lt;dbl&gt; 4, 7, 8, 9, 10, 11, 17, 18, 19, 20, 2…\n$ Sequencial.number                &lt;dbl&gt; 2465, 2470, 2472, 2473, 2474, 2475, 2…\n$ Study                            &lt;chr&gt; \"PS\", \"PS\", \"PS\", \"PS\", \"PS\", \"PS\", \"…\n$ Number                           &lt;chr&gt; \"4399\", \"4402\", \"4404\", \"4405\", \"4406…\n$ Inclusion                        &lt;chr&gt; \"yes\", \"yes\", \"yes\", \"yes\", \"yes\", \"y…\n$ DateOfIsolation                  &lt;chr&gt; \"15-01-03\", \"15-01-03\", \"15-01-03\", \"…\n$ DateOfBirth                      &lt;chr&gt; \"1/3/1962\", \"16-12-39\", \"20-02-25\", \"…\n$ AgeInYears                       &lt;dbl&gt; 40, 63, 77, 74, 84, 89, 79, 53, 73, 4…\n$ AgeinMonths                      &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ Postleitzahl.patient             &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ Zip                              &lt;chr&gt; \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0…\n$ Federal.state.patient            &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ Residence.patient                &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ Country.patient                  &lt;chr&gt; \"Deutschland\", \"Deutschland\", \"Deutsc…\n$ SeroType                         &lt;chr&gt; \"1\", \"3\", \"4\", \"19F\", \"14\", \"7F\", \"7C…\n$ Strain.identification...Genus    &lt;chr&gt; \"Streptococcus\", \"Streptococcus\", \"St…\n$ Strain.identification...Species  &lt;chr&gt; \"pneumoniae\", \"pneumoniae\", \"pneumoni…\n$ Pneumonia                        &lt;chr&gt; \"no\", \"no\", \"no\", \"yes\", \"no\", \"no\", …\n$ MD.Amoxicillin                   &lt;dbl&gt; 0.015, 0.030, 0.015, 0.060, 0.030, 0.…\n$ AmoxicillinSNS                   &lt;chr&gt; \"S\", \"S\", \"S\", \"S\", \"S\", \"S\", \"S\", \"S…\n$ MD.Cefotaxime                    &lt;dbl&gt; 0.015, 0.015, 0.015, 0.030, 0.015, 0.…\n$ CefotaximeSNS                    &lt;chr&gt; \"S\", \"S\", \"S\", \"S\", \"S\", \"S\", \"S\", \"S…\n$ MD.Chloramphenicol               &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ ChloramphenicolSNS               &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ MD.Clindamycin                   &lt;dbl&gt; 0.060, 0.060, 0.060, 0.060, 0.060, 0.…\n$ ClindamycinSNS                   &lt;chr&gt; \"S\", \"S\", \"S\", \"S\", \"S\", \"S\", \"S\", \"S…\n$ MD.Erythromycin                  &lt;dbl&gt; 0.12, 0.06, 0.06, 0.06, 4.00, 0.06, 0…\n$ ErythromycinSNS                  &lt;chr&gt; \"S\", \"S\", \"S\", \"S\", \"NS\", \"S\", \"S\", \"…\n$ MD.Levofloxacin                  &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ LevofloxaxinSNS                  &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ MD.Penicillin                    &lt;dbl&gt; 0.015, 0.015, 0.015, 0.060, 0.015, 0.…\n$ PenicillinSNS                    &lt;chr&gt; \"S\", \"S\", \"S\", \"S\", \"S\", \"S\", \"S\", \"S…\n$ MD.Tetracyclin                   &lt;dbl&gt; 0.25, 0.25, 0.12, 16.00, 0.12, 0.25, …\n$ TetracyclineSNS                  &lt;chr&gt; \"S\", \"S\", \"S\", \"S\", \"S\", \"S\", \"S\", \"S…\n$ MD.Trimetoprim.Sulfamethoxazole  &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ Sulfamethoxazole.trimethoprimSNS &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ MD.Vancomycin                    &lt;dbl&gt; 0.12, 0.25, 0.25, 0.12, 0.12, 0.25, 0…\n$ VancomycinSNS                    &lt;chr&gt; \"S\", \"S\", \"S\", \"S\", \"S\", \"S\", \"S\", \"S…\n$ date                             &lt;date&gt; 2003-01-15, 2003-01-15, 2003-01-15, …\n$ year                             &lt;dbl&gt; 2003, 2003, 2003, 2003, 2003, 2003, 2…\n$ month                            &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ epiyr                            &lt;dbl&gt; 2002, 2002, 2002, 2002, 2002, 2002, 2…\n$ agec                             &lt;dbl&gt; 4, 5, 5, 5, 5, 5, 5, 4, 5, 4, 4, 5, 5…\n$ vt                               &lt;dbl&gt; 2, 6, 1, 1, 1, 2, 0, 5, 1, 1, 2, 0, 1…\n$ vaxperiod                        &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ zip2                             &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ indic                            &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n\n\n\nSeroType: The serotype of the pathogen.\nepiyr: The epidemiological year of the observation.\ncases: This is a new variable created to represent the count of cases for each serotype and year combination.\n\n\nAggregating the data to count the number of cases for each serotype-year combination.\nEnsuring that all possible serotype-year combinations are represented.\nCreating numeric IDs for SeroType and epiyr.\nCreating a list of data objects required for running the JAGS model, including case counts, numeric IDs, and the number of unique serotypes and years.\n\n\n# Aggregate the data by SeroType and epiyr to get the number of cases \n# for each combination. The function complete() ensures all combinations \n# of SeroType and epiyr are represented in the dataset, even those with \n# zero cases.\nserotype_year_counts &lt;- df %&gt;%\n  count(SeroType, epiyr, name = \"cases\") %&gt;%\n  complete(SeroType, epiyr, fill = list(cases = 0))\n\n# Convert the categorical variables SeroType and epiyr into numeric IDs \n# which are easier to work with in JAGS.\nserotype_year_counts &lt;- serotype_year_counts %&gt;%\n  mutate(sero_id = as.integer(factor(SeroType)),\n         year_id = as.integer(factor(epiyr)))\n\n# Create the data list that will be input into JAGS.\njdat &lt;- list(\n  N = nrow(serotype_year_counts),\n  cases = serotype_year_counts$cases,\n  sero_id = serotype_year_counts$sero_id,\n  year_id = serotype_year_counts$year_id,\n  n_sero = length(unique(serotype_year_counts$sero_id)),\n  n_year = length(unique(serotype_year_counts$year_id))\n)\n\n\nsource(\"Model2.R\")\n\n# mod &lt;- jags.model(textConnection(jcode), data = jdat, n.chains = 2)\n# update(mod, 1000)  # burn-in\n# samp &lt;- coda.samples(mod, variable.names = c(\"mu\", \"beta\"), n.iter = 5000)\n# \n# summary(samp)\n\n\n# par(mar = c(2, 2, 2, 2))  # smaller margins\n# plot(samp)\n# # Density plots\n# densplot(samp, main = \"Posterior Density for Parameters\")\n# \n# posterior_summary &lt;- summary(samp)\n# round(posterior_summary$statistics, 3)  # means, SDs\n# round(posterior_summary$quantiles, 3)   # 2.5%, 50%, 97.5%\n# \n# beta_samples &lt;- as.matrix(samp)[, grep(\"beta\", colnames(as.matrix(samp)))]\n# beta_means &lt;- apply(beta_samples, 2, mean)\n# beta_ci &lt;- apply(beta_samples, 2, quantile, probs = c(0.025, 0.975))\n# \n# df &lt;- data.frame(\n#   param = colnames(beta_samples),\n#   mean = beta_means,\n#   lower = beta_ci[1, ],\n#   upper = beta_ci[2, ]\n# )\n# \n# ggplot(df, aes(x = param, y = mean)) +\n#   geom_point() +\n#   geom_errorbar(aes(ymin = lower, ymax = upper), width = 0.2) +\n#   coord_flip() +\n#   labs(title = \"Posterior Estimates for Beta\", y = \"Effect Size\", x = \"\") +\n#   theme_minimal()\n# \n# # Heatmap\n# # Your original year and serotype info\n# year_seq &lt;- sort(unique(jdat$year_id))  # e.g., 2000:2006\n# sero_seq &lt;- 1:jdat$n_sero\n# mean_year &lt;- (length(year_seq) + 1) / 2\n# \n# # Create a dataframe of all serotype-year combinations\n# pred_grid &lt;- expand.grid(\n#   sero = sero_seq,\n#   year = year_seq\n# )\n\n# # Predict expected log incidence\n# pred_grid$log_lambda &lt;- mu_means[pred_grid$sero] +\n#   beta_means[pred_grid$sero] * (pred_grid$year - mean_year)\n# \n# # Back-transform to incidence\n# pred_grid$lambda &lt;- exp(pred_grid$log_lambda)\n# \n# ggplot(pred_grid, aes(x = year, y = factor(sero), fill = lambda)) +\n#   geom_tile(color = \"white\") +\n#   scale_fill_viridis_c(name = \"Expected\\nCases\", option = \"C\") +\n#   labs(x = \"Year\", y = \"Serotype\", title = \"Expected Cases by Year and Serotype\") +\n#   theme_minimal() +\n#   theme(axis.text.y = element_text(size = 8))\n# \n# ggplot(serotype_year_counts, aes(x = epiyr, y = SeroType, fill = cases)) +\n#   geom_tile(color = \"white\") +\n#   scale_fill_viridis_c(option = \"C\", name = \"Cases\") +\n#   labs(title = \"IPD Cases by Serotype and Year\",\n#        x = \"Year\",\n#        y = \"Serotype\") +\n#   theme_minimal() +\n#   theme(axis.text.x = element_text(angle = 45, hjust = 1))\n# \n# # Model version\n# mean_year &lt;- (max(serotype_year_counts$year_id) + 1) / 2\n# \n# # Create a data frame with expected log lambda for each serotype-year\n# serotype_year_counts$expected_log_lambda &lt;- mu_means[serotype_year_counts$sero_id] + \n#   beta_means[serotype_year_counts$sero_id] * (serotype_year_counts$year_id - mean_year)\n# \n# # Convert to expected cases (lambda)\n# serotype_year_counts$expected_cases &lt;- exp(serotype_year_counts$expected_log_lambda)\n# \n# # Plot\n# ggplot(serotype_year_counts, aes(x = epiyr)) +\n#   geom_point(aes(y = cases), color = \"blue\", alpha = 0.5) +\n#   geom_line(aes(y = expected_cases, group = SeroType), color = \"red\") +\n#   facet_wrap(~ SeroType, scales = \"free_y\") +\n#   labs(y = \"Cases\", x = \"Year\", title = \"Observed (points) vs Expected (lines) Cases by Serotype\") +\n#   theme_minimal() +\n#   theme(axis.text.x = element_text(angle = 45, hjust = 1))"
  }
]